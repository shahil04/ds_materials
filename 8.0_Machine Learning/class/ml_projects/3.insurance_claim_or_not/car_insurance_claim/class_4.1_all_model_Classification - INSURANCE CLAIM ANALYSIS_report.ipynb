{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "numerous-syntax",
   "metadata": {
    "papermill": {
     "duration": 0.050207,
     "end_time": "2021-06-04T19:47:18.066788",
     "exception": false,
     "start_time": "2021-06-04T19:47:18.016581",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<!-- ![image](https://i.ytimg.com/vi/TbEZIZo4PZ8/maxresdefault.jpg) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291ceb92",
   "metadata": {},
   "source": [
    "### Introduction to Classification - INSURANCE CLAIM ANALYSIS\n",
    "Introduction to Logistic Regression\n",
    "\n",
    "Sigmoid Function\n",
    "\n",
    "Confusion Matrix\n",
    "\n",
    "Classification Evaluation Metrics\n",
    "\n",
    "https://www.kaggle.com/code/mohamedbakrey/make-a-prediction-for-insurance-claim-report\n",
    "\n",
    "https://youtu.be/ntBa7YKc9XM?si=1V1RL0wCmxM_gjor  Regression price\n",
    "\n",
    "https://youtu.be/OOLhKLXCJiU?si=dWxvFk82mVO8WxEP DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-blast",
   "metadata": {
    "papermill": {
     "duration": 0.049691,
     "end_time": "2021-06-04T19:47:18.165570",
     "exception": false,
     "start_time": "2021-06-04T19:47:18.115879",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predicting auto and insurance fraud in general:\n",
    "is a contract, represented by a policy, in which an individual or entity receives financial protection or reimbursement against losses from an insurance company. The company pools clients' risks to make payments more affordable for the insured.\n",
    "# Insurance Policy Components\n",
    "When choosing a policy, it is important to understand how insurance works.\n",
    "\n",
    "A firm understanding of these concepts goes a long way in helping you choose the policy that best suits your needs. For instance, whole life insurance may or may not be the right type of life insurance for you. There are three components of any type of insurance (premium, policy limit, and deductible) that are crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-spirituality",
   "metadata": {
    "papermill": {
     "duration": 0.050504,
     "end_time": "2021-06-04T19:47:18.263925",
     "exception": false,
     "start_time": "2021-06-04T19:47:18.213421",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> The goal of this note, Kuho Hwa, is to make a simplified and structured analysis to make an explanation of that dirty process called fraud and lack of it through analysis and the machine learning system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87132b13",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ”¹ **Business Understanding**\n",
    "\n",
    "Insurance companies process thousands of claims annually. A large percentage may be **exaggerated or fraudulent**, leading to financial loss. This dataset represents information about customers, their policies, accidents, and whether the claim was fraudulent.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ **Business Problem**\n",
    "\n",
    "> **The insurance company is experiencing increasing financial loss due to fraudulent and unnecessary claims.**\n",
    "> Detecting fraud manually is slow, expensive, and inaccurate.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ **Business Objective**\n",
    "\n",
    "| Primary Objective                                               | Secondary Objective                                  |\n",
    "| --------------------------------------------------------------- | ---------------------------------------------------- |\n",
    "| Build a **Machine Learning Model to Predict Fraudulent Claims** | Predict expected **claim cost** and detect anomalies |\n",
    "| Reduce false insurance payouts & financial losses               | Prioritize claims for manual investigation           |\n",
    "| Improve claim approval time for genuine customers               | Enhance underwriting & pricing decisions             |\n",
    "\n",
    "---\n",
    "\n",
    "### Expected Outcomes\n",
    "\n",
    "ðŸ“Œ Reduce fraudulent claims by 30â€“60%\n",
    "ðŸ“Œ Save millions in wrongful payments\n",
    "ðŸ“Œ Faster claim settlements for genuine customers\n",
    "ðŸ“Œ Improve risk-based pricing for future policies\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-chance",
   "metadata": {
    "papermill": {
     "duration": 0.049138,
     "end_time": "2021-06-04T19:47:18.365223",
     "exception": false,
     "start_time": "2021-06-04T19:47:18.316085",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-looking",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T19:47:18.469354Z",
     "iopub.status.busy": "2021-06-04T19:47:18.468092Z",
     "iopub.status.idle": "2021-06-04T19:47:19.261102Z",
     "shell.execute_reply": "2021-06-04T19:47:19.261635Z",
     "shell.execute_reply.started": "2021-06-04T19:43:34.590825Z"
    },
    "papermill": {
     "duration": 0.84725,
     "end_time": "2021-06-04T19:47:19.261915",
     "exception": false,
     "start_time": "2021-06-04T19:47:18.414665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-scenario",
   "metadata": {
    "papermill": {
     "duration": 0.049427,
     "end_time": "2021-06-04T19:47:19.361830",
     "exception": false,
     "start_time": "2021-06-04T19:47:19.312403",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Read data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-hartford",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T19:47:19.463871Z",
     "iopub.status.busy": "2021-06-04T19:47:19.463283Z",
     "iopub.status.idle": "2021-06-04T19:47:19.495207Z",
     "shell.execute_reply": "2021-06-04T19:47:19.495711Z",
     "shell.execute_reply.started": "2021-06-04T19:43:35.456881Z"
    },
    "papermill": {
     "duration": 0.086575,
     "end_time": "2021-06-04T19:47:19.495894",
     "exception": false,
     "start_time": "2021-06-04T19:47:19.409319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "db=pd.read_csv('insurance_claims_report.csv')\n",
    "#\n",
    "db.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef1ebfb",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ **Dataset Column Explanation with Business Use**\n",
    "\n",
    "| Column Name                                            | Meaning / Description                                               | Business Use in Insurance Analytics / ML                                                                  |\n",
    "| ------------------------------------------------------ | ------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------- |\n",
    "| **months_as_customer**                                 | Number of months insured with the company                           | Helps evaluate customer loyalty & churn probability; long-term customers are less likely to commit fraud. |\n",
    "| **age**                                                | Age of insured person                                               | Younger drivers may have higher risk profiles â†’ used for risk pricing.                                    |\n",
    "| **policy_number**                                      | Unique ID for policy                                                | Identification â€“ not useful for modeling, usually dropped.                                                |\n",
    "| **policy_bind_date**                                   | Date when the policy was issued                                     | Helps assess policy age; new policies with quick claims may signal fraud.                                 |\n",
    "| **policy_state (OH, IL, IN etc.)**                     | State where policy was registered                                   | Geographic-based risk pricing; accident patterns vary by region.                                          |\n",
    "| **policy_csl (250/500, 100/300 etc.)**                 | Combined single limit â†’ Maximum coverage limits                     | High coverage may attract fraudulent claims for bigger payout.                                            |\n",
    "| **policy_deductable**                                  | Amount customer pays before insurance covers                        | Low deductible may increase claim frequency.                                                              |\n",
    "| **policy_annual_premium**                              | Yearly insurance premium paid                                       | Higher premium = higher risk customer profile.                                                            |\n",
    "| **umbrella_limit**                                     | Additional liability coverage above policy                          | Fraudsters often target high umbrella limits for large payouts.                                           |\n",
    "| **insured_zip**                                        | ZIP code of insured customer                                        | Geography affects accident risk & fraud trends.                                                           |\n",
    "| **insured_sex**                                        | Gender of policy holder                                             | Risk segmentation (male drivers historically slightly riskier).                                           |\n",
    "| **insured_education_level**                            | Educational qualification                                           | Low education may correlate with higher claim probability statistically.                                  |\n",
    "| **insured_occupation**                                 | Job category of customer                                            | High-risk professions (drivers, mechanics) â†’ more exposure â†’ more claims.                                 |\n",
    "| **insured_hobbies**                                    | Hobbies listed (bungie-jumping, board-games)                        | Risk indicator: high-risk hobbies may lead to more accident exposure.                                     |\n",
    "| **insured_relationship**                               | Relationship of policyholder to household (husband, own-child etc.) | Helps profile dependents, household risk level.                                                           |\n",
    "| **capital-gains / capital-loss**                       | Financial gain/loss for the customer                                | Unstable finances may correlate with fraudulent intent.                                                   |\n",
    "| **incident_date**                                      | Date of accident/claim                                              | Helps detect seasonal or suspicious claim timing.                                                         |\n",
    "| **incident_type**                                      | Type of accident (Vehicle Theft, Collision etc.)                    | Key target variable in claim approval logic.                                                              |\n",
    "| **collision_type**                                     | Side/Front/Rear/None                                                | Helps determine accident legitimacy & repair cost.                                                        |\n",
    "| **incident_severity**                                  | Minor/Major/Total Loss                                              | Severity indicates claim value & fraud likelihood.                                                        |\n",
    "| **authorities_contacted**                              | Police / Fire / None                                                | Genuine claims usually have police report â†’ fraud often lacks.                                            |\n",
    "| **incident_state / incident_city / incident_location** | Location of accident                                                | Cross-check for mismatches from policy location â†’ fraud flag.                                             |\n",
    "| **incident_hour_of_the_day**                           | Time accident occurred                                              | Late-night or odd hour accidents often have higher fraud probability.                                     |\n",
    "| **number_of_vehicles_involved**                        | Count of vehicles in accident                                       | Helps validate incident seriousness.                                                                      |\n",
    "| **property_damage**                                    | YES/NO/Unknown                                                      | Claims with missing damage info can be suspicious.                                                        |\n",
    "| **bodily_injuries**                                    | Number of injured persons                                           | Higher injuries â†’ larger payout â†’ fraud cases often exaggerate.                                           |\n",
    "| **witnesses**                                          | Number of eyewitnesses                                              | Low/no witnesses increases fraud probability.                                                             |\n",
    "| **police_report_available**                            | YES/NO/?                                                            | No report = possible fraud scenario.                                                                      |\n",
    "| **total_claim_amount**                                 | Final total compensation                                            | Target variable in claim cost prediction models.                                                          |\n",
    "| **injury_claim / property_claim / vehicle_claim**      | Breakdown of claim amounts                                          | Helps build prediction models for claim estimation.                                                       |\n",
    "| **auto_make / auto_model / auto_year**                 | Car details                                                         | Newer/luxury cars â†’ higher claim payouts; targeted more in fraud.                                         |\n",
    "| **fraud_reported (Y/N)**                               | Fraud label                                                         | Target variable for fraud detection ML models.                                                            |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-edward",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T19:47:19.743572Z",
     "iopub.status.busy": "2021-06-04T19:47:19.742550Z",
     "iopub.status.idle": "2021-06-04T19:47:19.747650Z",
     "shell.execute_reply": "2021-06-04T19:47:19.748144Z",
     "shell.execute_reply.started": "2021-06-04T19:43:35.543852Z"
    },
    "papermill": {
     "duration": 0.058043,
     "end_time": "2021-06-04T19:47:19.748311",
     "exception": false,
     "start_time": "2021-06-04T19:47:19.690268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number of rows and colums\n",
    "db.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-uruguay",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T19:47:19.849910Z",
     "iopub.status.busy": "2021-06-04T19:47:19.849022Z",
     "iopub.status.idle": "2021-06-04T19:47:19.871601Z",
     "shell.execute_reply": "2021-06-04T19:47:19.871076Z",
     "shell.execute_reply.started": "2021-06-04T19:43:35.549408Z"
    },
    "papermill": {
     "duration": 0.074155,
     "end_time": "2021-06-04T19:47:19.871740",
     "exception": false,
     "start_time": "2021-06-04T19:47:19.797585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic details of data\n",
    "db.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-accreditation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T19:47:19.977870Z",
     "iopub.status.busy": "2021-06-04T19:47:19.977172Z",
     "iopub.status.idle": "2021-06-04T19:47:20.037394Z",
     "shell.execute_reply": "2021-06-04T19:47:20.036849Z",
     "shell.execute_reply.started": "2021-06-04T19:43:35.585508Z"
    },
    "papermill": {
     "duration": 0.116792,
     "end_time": "2021-06-04T19:47:20.037539",
     "exception": false,
     "start_time": "2021-06-04T19:47:19.920747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# basic statistical summary\n",
    "db.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-deployment",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T19:47:20.279081Z",
     "iopub.status.busy": "2021-06-04T19:47:20.278213Z",
     "iopub.status.idle": "2021-06-04T19:47:20.282198Z",
     "shell.execute_reply": "2021-06-04T19:47:20.281642Z",
     "shell.execute_reply.started": "2021-06-04T19:43:35.684145Z"
    },
    "papermill": {
     "duration": 0.066284,
     "end_time": "2021-06-04T19:47:20.282326",
     "exception": false,
     "start_time": "2021-06-04T19:47:20.216042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "db.describe(include=\"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-logic",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T19:47:20.408803Z",
     "iopub.status.busy": "2021-06-04T19:47:20.396284Z",
     "iopub.status.idle": "2021-06-04T19:47:20.438709Z",
     "shell.execute_reply": "2021-06-04T19:47:20.438201Z",
     "shell.execute_reply.started": "2021-06-04T19:43:35.699727Z"
    },
    "papermill": {
     "duration": 0.104365,
     "end_time": "2021-06-04T19:47:20.438837",
     "exception": false,
     "start_time": "2021-06-04T19:47:20.334472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set the video\n",
    "pd.set_option('display.max_columns', 500)\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d7bff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all columns name \n",
    "db.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf9ce66",
   "metadata": {},
   "source": [
    "### 2. data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-shame",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T19:47:21.096078Z",
     "iopub.status.busy": "2021-06-04T19:47:21.095460Z",
     "iopub.status.idle": "2021-06-04T19:47:21.141043Z",
     "shell.execute_reply": "2021-06-04T19:47:21.141498Z",
     "shell.execute_reply.started": "2021-06-04T19:43:35.900342Z"
    },
    "papermill": {
     "duration": 0.103572,
     "end_time": "2021-06-04T19:47:21.141660",
     "exception": false,
     "start_time": "2021-06-04T19:47:21.038088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check null values\n",
    "db.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cbfad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before replace value check the datatypes\n",
    "db.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6ddbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check columns\n",
    "db[\"authorities_contacted\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335ab33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is object datatypes so find the mode value\n",
    "db[\"authorities_contacted\"].mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9d9388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or check all distinct values\n",
    "db[\"authorities_contacted\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e286c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now replace the value\n",
    "db[\"authorities_contacted\"] = db[\"authorities_contacted\"].fillna(db[\"authorities_contacted\"].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9797a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again check the null values\n",
    "db.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb9c7d3",
   "metadata": {},
   "source": [
    "- Before apply models , we need to check the data quality and data preprocessing. \n",
    "    - We need to check the data for missing values, outliers, and data types.\n",
    "    - We need to handle missing values by either removing them or imputing them with a suitable\n",
    "    method.\n",
    "    - We need to transform the data into a suitable format for modeling, such as scaling or encoding\n",
    "    categorical variables.\n",
    "    - We need to split the data into training and testing sets to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1cddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03514596",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# change the Label data into numbers --> fraud columns\n",
    "# change yes into 1 and No 0\n",
    "# map\n",
    "db[\"fraud_reported\"] =db[\"fraud_reported\"].map({\"Y\":1,\"N\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c877e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d1f87b",
   "metadata": {},
   "source": [
    "### Change all useful columns in number\n",
    "- for example policy_number is not import so remove it \n",
    "- check all colums then encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068d3113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i  only take numerical columns\n",
    "db.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede19f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db[\"authorities_contacted\"].dtypes !=\"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4c626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i only take numerical columns\n",
    "numerical_col = [] # only store numerical columns name \n",
    "for i in db.columns:\n",
    "    if db[i].dtypes !=\"O\":\n",
    "        print(i)\n",
    "        numerical_col.append(i)\n",
    "numerical_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df215393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only store numerical data into df\n",
    "df= pd.DataFrame()\n",
    "for i in numerical_col:\n",
    "    df[i]= db[i]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc0c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271238d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unwanted columns\n",
    "df = df.drop(['policy_number','insured_zip','capital-gains','capital-loss','auto_year'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe67af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-pepper",
   "metadata": {
    "papermill": {
     "duration": 0.252439,
     "end_time": "2021-06-04T19:49:22.621307",
     "exception": false,
     "start_time": "2021-06-04T19:49:22.368868",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Split the data\n",
    "- Before Create model  (split data into x and y ) feature and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a570999",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07f2822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only numerical columns for training\n",
    "# x-->features ,y-->Label\n",
    "X = df.drop('fraud_reported',axis=1)\n",
    "Y = df['fraud_reported']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2632f460",
   "metadata": {},
   "outputs": [],
   "source": [
    "X\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b839f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into --> train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94662ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66175990",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972758b8",
   "metadata": {},
   "source": [
    "#### Let's apply our classification models one by one:\n",
    "1) Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa80161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97f0bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training Accuracy \n",
    "#  Accuracy on seeen data / training data\n",
    "model.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c8c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing accuracy\n",
    "#  Accuracy on unseeen data / new data\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d7e89",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fe859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983416fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error calcualte\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e125044",
   "metadata": {},
   "source": [
    "#### Evaluation matrics use\n",
    "Model evaluation is the crucial process of assessing an AI/ML model's performance, reliability, and ability to generalize using unseen data and specific metrics (like accuracy, precision, recall, F1-score) to ensure it's effective, not just memorizing training data, and ready for real-world deployment, catching issues like overfitting, underfitting, or bias. \n",
    "\n",
    "1. Accuracy \n",
    "2. confussion matrix\n",
    "3. classifications report\n",
    "4. precision ,recall, f1score\n",
    "5. Roc-Auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05f4522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix, classification_report\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, y_pred), 4))\n",
    "# print('%s' %(\"name\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cb7010",
   "metadata": {},
   "source": [
    "A confusion matrix is a table used to evaluate the performance of a classification model by comparing the predicted outcomes to the actual outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f44f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d2dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore the  ignore\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561b5da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f85da16",
   "metadata": {},
   "source": [
    "![](https://images.prismic.io/encord/edfa849b-03fb-43d2-aba5-1f53a8884e6f_image5.png?auto=compress,format)\n",
    "\n",
    "![](https://www.researchgate.net/publication/346062755/figure/fig5/AS:960496597483542@1606011642491/Confusion-matrix-and-performance-evaluation-metrics.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7d4466",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17d6585",
   "metadata": {},
   "source": [
    "## model Improvements in Machine Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b6128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 72\n",
    "# Diffrents way to improve the model Accuracy \n",
    "# 1. Data-centric improvements\n",
    "    # Data cleaning:\n",
    "    # Feature engineering\n",
    "    # Increase data:\n",
    "    # Add context:  -- relivence\n",
    "\n",
    "# 2. Model-centric improvements\n",
    "    # 1.Choose a robust algorithm\n",
    "    # 2.Hyperparameter tuning\n",
    "    # 3. Cross-validation\n",
    "    # 4. Ensemble learning\n",
    "    # Regularization: only for overfitting\n",
    "\n",
    "# Other strategies\n",
    "    # 1. Reframe the problem: Sometimes, improving a model isn't about the data or algorithm, but about how the problem itself is defined.\n",
    "    # 2. Model monitoring: After deployment, continuously monitor the model's performance in a production environment to identify and address any issues that arise over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf44c68",
   "metadata": {},
   "source": [
    "## 2.Hyperparameter tuning\n",
    "Hyperparameter tuning (or optimization) is the crucial process of finding the best configuration settings (hyperparameters) for a machine learning model before training begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3656bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Tuning Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039c5256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr1 = LogisticRegression()\n",
    "lr1.fit(x_train,y_train)\n",
    "\n",
    "lr1.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6fd273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter change\n",
    "# 1. manually  parameter change\n",
    "lr2 = LogisticRegression(penalty=\"l2\",solver='lbfgs', C=0.5)\n",
    "lr2.fit(x_train,y_train)\n",
    "\n",
    "lr2.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89be55cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter change\n",
    "# 1. manually  parameter change\n",
    "lr2 = LogisticRegression(penalty=\"l2\",solver='saga', C=0.5)\n",
    "lr2.fit(x_train,y_train)\n",
    "\n",
    "lr2.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e6cc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=[0.5,1,10,15,20]\n",
    "for i in list1:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe3e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list1:\n",
    "    lr2 = LogisticRegression(penalty=\"l2\",solver='saga', C=i)\n",
    "    lr2.fit(x_train,y_train)\n",
    "\n",
    "    print(i, lr2.score(x_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc84b73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "C= [0.1,.3,0.5,1,10, 100,100000]\n",
    "p =['l1','l2']\n",
    "solver =['lbfgs','saga','liblinear']\n",
    "for c in C:\n",
    "    for i in p:\n",
    "        lr2 = LogisticRegression(penalty=i,solver='liblinear', C=c)\n",
    "        lr2.fit(x_train,y_train)\n",
    "\n",
    "        print(lr2.score(x_train,y_train),c,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565bee2a",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    " is a technique used to check how well a machine learning model performs on unseen data while preventing overfitting. It works by:\n",
    "\n",
    "- Splitting the dataset into several parts.\n",
    "- Training the model on some parts and testing it on the remaining part.\n",
    "- Repeating this resampling process multiple times by choosing different parts of the dataset.\n",
    "- Averaging the results from each validation step to get the final performance.\n",
    "\n",
    "1. Holdout Validation\n",
    "- from sklearn.model_selection import train_test_split\n",
    "![](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/04/cross-validation-in-machine-learning-how-to-do-it-right-1-1.jpg?resize=1020%2C534&ssl=1)\n",
    "\n",
    "2. LOOCV (Leave One Out Cross Validation)\n",
    "- from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "![](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/04/cross-validation-in-machine-learning-how-to-do-it-right-3.jpg?w=1200&ssl=1)\n",
    "\n",
    "3. Stratified Cross-Validation\n",
    "- from sklearn.model_selection import StratifiedKFold\n",
    "![](https://dataaspirant.com/wp-content/uploads/2020/12/8-Stratified-K-Fold-Cross-Validation-768x516.png)\n",
    "\n",
    "4. K-Fold Cross Validation\n",
    "- from sklearn.model_selection import KFold\n",
    "![](http://media.geeksforgeeks.org/wp-content/uploads/20250927122541290704/222.webp)\n",
    "\n",
    "https://dataaspirant.com/cross-validation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa7751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods \n",
    "# 1. Random search CV\n",
    "# 2. GridSearch CV\n",
    "# 3. Bayesian search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0c34f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Grid search cv\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# random serch cv\n",
    "\n",
    "param = {\n",
    "    'C': [0.1, 1.0, 10.0, 100.0],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear'] # 'liblinear' supports both 'l1' and 'l2' penalties\n",
    "}\n",
    "lr3 =LogisticRegression()\n",
    "\n",
    "grid = GridSearchCV(lr3,param_grid=param,verbose=3 ,cv=2)\n",
    "# cv is the suffle of data to train in same parameter value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc0b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the best params\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb5a9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrun best model\n",
    "grid.best_estimator_\n",
    "model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fae0f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3effba8e",
   "metadata": {},
   "source": [
    "#### 2.Randomsearch cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "02c9d4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# grid = GridSearchCV(lr3,param_grid=param,verbose=3 ,cv=2)\n",
    "random = RandomizedSearchCV(lr3,param_distributions=param,verbose=3 ,cv=10,n_iter=8) \n",
    "# n_iter show how much combinations check in model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ff5c5892",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [800, 700]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[197]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:928\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    925\u001b[39m estimator = \u001b[38;5;28mself\u001b[39m.estimator\n\u001b[32m    926\u001b[39m scorers, refit_metric = \u001b[38;5;28mself\u001b[39m._get_scorers()\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m X, y = \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    929\u001b[39m params = _check_method_params(X, params=params)\n\u001b[32m    931\u001b[39m routed_params = \u001b[38;5;28mself\u001b[39m._get_routed_params_for_fit(params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:532\u001b[39m, in \u001b[36mindexable\u001b[39m\u001b[34m(*iterables)\u001b[39m\n\u001b[32m    502\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[32m    503\u001b[39m \n\u001b[32m    504\u001b[39m \u001b[33;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    528\u001b[39m \u001b[33;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\u001b[39;00m\n\u001b[32m    529\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    531\u001b[39m result = [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:475\u001b[39m, in \u001b[36mcheck_consistent_length\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    473\u001b[39m uniques = np.unique(lengths)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    476\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    477\u001b[39m         % [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[32m    478\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found input variables with inconsistent numbers of samples: [800, 700]"
     ]
    }
   ],
   "source": [
    "random.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f001def",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "3c733a7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This RandomizedSearchCV instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFittedError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[198]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:528\u001b[39m, in \u001b[36mBaseSearchCV.score\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    497\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the score on the given data, if the estimator has been refit.\u001b[39;00m\n\u001b[32m    498\u001b[39m \n\u001b[32m    499\u001b[39m \u001b[33;03mThis uses the score defined by ``scoring`` where provided, and the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    525\u001b[39m \u001b[33;03m    ``best_estimator_.score`` method otherwise.\u001b[39;00m\n\u001b[32m    526\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    527\u001b[39m _check_refit(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    530\u001b[39m _raise_for_params(params, \u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    532\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _routing_enabled():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1757\u001b[39m, in \u001b[36mcheck_is_fitted\u001b[39m\u001b[34m(estimator, attributes, msg, all_or_any)\u001b[39m\n\u001b[32m   1754\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1756\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[32m-> \u001b[39m\u001b[32m1757\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg % {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator).\u001b[34m__name__\u001b[39m})\n",
      "\u001b[31mNotFittedError\u001b[39m: This RandomizedSearchCV instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "random.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a4a43dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700,)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  save the model \n",
    "import pickle\n",
    "filename = 'logistic_model1.pkl'\n",
    "pickle.dump(lr2, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4795ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50b5b65c",
   "metadata": {},
   "source": [
    "## Stream_lit code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load the trained model\n",
    "filename = 'logistic_model.pkl'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "st.title(\"Insurance Claim Fraud Prediction\")\n",
    "\n",
    "st.write(\"\"\"\n",
    "This app predicts whether an insurance claim is fraudulent based on several factors.\n",
    "\"\"\")\n",
    "\n",
    "# Define input fields for numerical features\n",
    "months_as_customer      = st.number_input('total years of customer as a loayality', min_value=0, max_value=500, value=100)\n",
    "months_as_customer = months_as_customer *12\n",
    "\n",
    "age                     = st.number_input('Age', min_value=18, max_value=100, value=30)\n",
    "policy_annual_premium   = st.number_input('Policy Annual Premium', min_value=0.0, max_value=2500.0, value=1000.0)\n",
    "auto_year               = st.number_input('Auto Year', min_value=1980, max_value=2023, value=2010)\n",
    "umbrella_limit          = st.number_input('Umbrella Limit', min_value=-10000000, max_value=10000000, value=0)\n",
    "\n",
    "\n",
    "# Create a button to make a prediction\n",
    "if st.button('Predict Fraud'):\n",
    "    # Create a DataFrame from the input values\n",
    "    input_data = pd.DataFrame({\n",
    "        'months_as_customer': [months_as_customer],\n",
    "        'age': [age],\n",
    "        'policy_annual_premium': [policy_annual_premium],\n",
    "        'auto_year': [auto_year],\n",
    "        'umbrella_limit': [umbrella_limit]\n",
    "      })\n",
    "\n",
    "    # Select only the numerical columns used for prediction based on the original code\n",
    "    numerical_cols_for_prediction = ['months_as_customer', 'age', 'policy_annual_premium', 'auto_year', 'umbrella_limit']\n",
    "    input_data_for_prediction = input_data[numerical_cols_for_prediction]\n",
    "\n",
    "    # Make the prediction\n",
    "    prediction = loaded_model.predict(input_data_for_prediction)\n",
    "\n",
    "    # Display the prediction\n",
    "    if prediction[0] == 'Y':\n",
    "        st.write(\"Prediction: Fraud Reported (Y)\")\n",
    "    else:\n",
    "        st.write(\"Prediction: No Fraud Reported (N)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf0c5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e448437",
   "metadata": {},
   "source": [
    "#### overview of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42ac031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "Accuracy = 0.73\n",
      "=======================\n",
      "K-Nearest Neighbors:\n",
      "Accuracy = 0.68\n",
      "=======================\n",
      "Decision Tree:\n",
      "Accuracy = 0.61\n",
      "=======================\n",
      "Random Forest:\n",
      "Accuracy = 0.70\n",
      "=======================\n",
      "Gradient Boosting:\n",
      "Accuracy = 0.71\n",
      "=======================\n",
      "Support Vector Machine:\n",
      "Accuracy = 0.73\n",
      "=======================\n",
      "Naive Bayes:\n",
      "Accuracy = 0.67\n",
      "=======================\n",
      "Neural Network (MLP):\n",
      "Accuracy = 0.73\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv(\"insurance_claims_report.csv\")\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Sample data creation - REPLACE THIS with your full CSV file loading\n",
    "# data = {\n",
    "#     \"months_as_customer\": [328, 228, 134, 256, 228, 256],\n",
    "#     \"age\": [48, 42, 18, 41, 44, 39],\n",
    "#     \"policy_annual_premium\": [1406.91, 1197.22, 1413.14, 1415.74, 1583.91, 1351.10],\n",
    "#     \"auto_year\": [2004, 2007, 2007, 2014, 2009, 2003],\n",
    "#     \"umbrella_limit\": [0, 5000000, 5000000, 6000000, 6000000, 0],\n",
    "#     \"fraud_reported\": ['Y', 'Y', 'N', 'Y', 'N', 'Y']\n",
    "# }\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# Features and target\n",
    "X = df[[\"months_as_customer\", \"age\", \"policy_annual_premium\", \"auto_year\", \"umbrella_limit\"]]\n",
    "y = df[\"fraud_reported\"]\n",
    "\n",
    "# Encode target variable\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)  # Y -> 1, N -> 0\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# Models dictionary\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=3),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"Support Vector Machine\": SVC(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Neural Network (MLP)\": MLPClassifier(max_iter=500)\n",
    "}\n",
    "# Train & evaluate all models\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, predictions)\n",
    "    print(f\"{name}:\\nAccuracy = {acc:.2f}\")\n",
    "    print(\"=======================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17758f57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 159.334249,
   "end_time": "2021-06-04T19:49:49.572860",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-04T19:47:10.238611",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
