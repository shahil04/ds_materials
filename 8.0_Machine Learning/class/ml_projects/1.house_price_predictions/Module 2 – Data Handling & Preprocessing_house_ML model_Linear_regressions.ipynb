{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2oU7SlOXua1"
   },
   "source": [
    "## Module 2 â€“ Data Handling & Preprocessing_house_ML model_Linear_regressions\n",
    "\n",
    "**Exploratory Data Analysis (EDA)** involves key steps like understanding the problem and data, data collection, data cleaning, descriptive statistics, data visualization, handling missing values and outliers, feature engineering, exploring variable relationships, and finally communicating insights.\n",
    "\n",
    "These steps are iterative, helping to uncover patterns, identify anomalies, and form hypotheses for building better models and making informed decisions.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yhZ6K50FGdN"
   },
   "source": [
    "  ---\n",
    "\n",
    "  ## ðŸ”¹ EDA Steps for Machine Learning\n",
    "\n",
    "  ### 1. **Understand the Problem & Dataset**\n",
    "\n",
    "  * Identify **business objective** or ML task (classification, regression, clustering, etc.)\n",
    "  * Define the Goal: Clarify the business goal or research question the data analysis aims to address.\n",
    "  *Understand Variables: Identify the meaning of each variable, their data types (numerical, categorical), and any known data quality issues or constraints.\n",
    "\n",
    "  * Load dataset (`pandas.read_csv`, SQL, etc.)\n",
    "  * Check dataset shape (`df.shape`)\n",
    "  * Display first few rows (`df.head()`)\n",
    "\n",
    "  ---\n",
    "\n",
    "  ### 2. **Data Collection**\n",
    "  Gather all relevant raw data from appropriate sources, such as databases, CSV files, or APIs.\n",
    "  ---\n",
    "\n",
    "  ### 3. **Data Preparation and Cleaning**\n",
    "  - Data Wrangling: Clean and organize the raw data into a format suitable for analysis.\n",
    "  - Handle Missing Values: Identify and decide how to handle missing or null values, which can be by removal, imputation with the mean or median, or other methods depending on the data's distribution.\n",
    "  - Handle Duplicates and Irregularities: Remove redundant data and address irregularities that can cause noise in the analysis.\n",
    "\n",
    "  ### 3.1. **Data Types & Structure**\n",
    "\n",
    "  * Inspect column types (`df.info()`)\n",
    "  * Separate features into:\n",
    "\n",
    "    * Numerical (continuous/discrete)\n",
    "    * Categorical\n",
    "    * Date/Time\n",
    "    * Text/Unstructured\n",
    "  * Identify target variable.\n",
    "\n",
    "  ---\n",
    "\n",
    "  ### 3.2 **Data Quality Check**\n",
    "\n",
    "  * Missing values (`df.isnull().sum()`)\n",
    "  * Duplicate records (`df.duplicated().sum()`)\n",
    "  * Outliers detection (boxplots, IQR method, z-score)\n",
    "  * Data entry errors (e.g., age = -5, salary = 9999999)\n",
    "\n",
    "  ---\n",
    "  ### 4. **Descriptive Statistics and Visualization**\n",
    "  * Summary Statistics: Calculate descriptive statistics like mean, median, standard deviation, and frequency distributions to get a quick understanding of the dataset's central tendency and variability.\n",
    "  * Visualize Distributions: Use histograms, box plots, and density plots to visualize the distribution of individual variables.\n",
    "  ---\n",
    "\n",
    "  ### 5. **Examine Relationships and Outliers**\n",
    "  * Bivariate and Multivariate Analysis: Explore pairwise relationships between variables using scatter plots or more complex multivariate methods like heatmaps to identify patterns and correlations.\n",
    "  * Outlier Detection: Identify and analyze outliers, which can skew results, and decide whether to remove or retain them based on the context.\n",
    "\n",
    "  ### 5.1 **Univariate Analysis (Single Variable)**\n",
    "\n",
    "  * Numerical features â†’ Histogram, KDE, Boxplot, Summary stats (`df.describe()`)\n",
    "  * Categorical features â†’ Value counts, Bar plot, Pie chart\n",
    "\n",
    "  ---\n",
    "\n",
    "  ### 5.2 **Bivariate Analysis (Feature vs Target)**\n",
    "\n",
    "  * Numerical vs Target:\n",
    "\n",
    "    * Correlation (`df.corr()` + heatmap)\n",
    "    * Scatter plots, boxplots\n",
    "  * Categorical vs Target:\n",
    "\n",
    "    * Grouped bar chart\n",
    "    * Countplot with hue\n",
    "    * Chi-square test (for independence)\n",
    "\n",
    "  ---\n",
    "\n",
    "  ### 5.3. **Multivariate Analysis**\n",
    "\n",
    "  * Correlation matrix for numerical features\n",
    "  * Pairplot/heatmap\n",
    "  * Cross-tabulations\n",
    "  * PCA/Dimensionality reduction (optional for visualization)\n",
    "\n",
    "  ---\n",
    "  ### 6. **Feature Engineering**\n",
    "  Create new features or transform existing ones to better represent the underlying information and improve model performance.\n",
    "\n",
    "  ---\n",
    "  ### 6.1 **Feature Relationships**\n",
    "\n",
    "  * Check multicollinearity (VIF score)\n",
    "  * Identify strong predictors\n",
    "  * Detect redundant features\n",
    "\n",
    "  ---\n",
    "\n",
    "  ### 6.2 **Feature Engineering Needs**\n",
    "\n",
    "  * Encoding categorical variables (OneHot, Label Encoding)\n",
    "  * Feature scaling (Normalization, Standardization)\n",
    "  * Date/time feature extraction (day, month, weekday, etc.)\n",
    "  * Domain-specific feature creation\n",
    "\n",
    "  ---\n",
    "\n",
    "  ### 6.3 **Class Imbalance Check (For Classification)**\n",
    "\n",
    "  * Value counts of target variable\n",
    "  * If imbalance exists â†’ consider SMOTE, undersampling/oversampling, class weights\n",
    "\n",
    "  ---\n",
    "  ### 7. **Communicate Findings**\n",
    "  Present the insights and discoveries from the EDA process to stakeholders.\n",
    "\n",
    "  ---\n",
    "  ### 8. **Iterate**\n",
    "  EDA is an iterative process. You may need to revisit earlier steps to test new hypotheses, address emerging challenges, and refine your understanding of the data to build more robust models\n",
    "\n",
    "  ### 10. **Key Insights & Hypothesis**\n",
    "\n",
    "  * Summarize main findings\n",
    "  * Identify which features are important\n",
    "  * Document anomalies or patterns\n",
    "\n",
    "  ---\n",
    "\n",
    "  âœ… **Outcome of EDA** â†’ Clean dataset + feature insights â†’ Ready for preprocessing & model building.\n",
    "\n",
    "  ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W482l4RlFGUj"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YkrMJ52CXt-6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebH5D2SMbHah"
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Za_6qesN0cCt"
   },
   "outputs": [],
   "source": [
    "# data read\n",
    "path =\"/content/drive/MyDrive/0.self_course_content_data_science/10.machine learning/Module 2 Bengaluru_House_Data.csv\"\n",
    "df = pd.read_csv(path,encoding=\"utf-8\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59bMkmYAaUPW"
   },
   "outputs": [],
   "source": [
    "# - 1. DATA LOAD\n",
    "# - 2. DATA INFO\n",
    "# - 3. DATA CLEANING\n",
    "#   - a. info\n",
    "#   - b. remove unwanted columns\n",
    "#   - c. duplicate\n",
    "#   - d. null check\n",
    "#   - e. clean null values (imputation of missing value )\n",
    "#   - f. save the clean data\n",
    "\n",
    "# - 4. DATA PREPROCESSING\n",
    "    # - a. segregate categorical and numerical columns\n",
    "    # - b. check the data pattern using graphs and show distributions\n",
    "    # - c. data interpolation >> is a process of estimating data in a range, getting unknown values from known values\n",
    "    # - d. data Feature Scalling\n",
    "    # - e. To check outliers >> distplot, boxplot\n",
    "    # - f. data encoding\n",
    "    # - g. class imbalance >> one class has higeher percentage upsampling, downsampling, smote\n",
    "    # - h. Assumptions of Machine Learning for Linear Regression\n",
    "# - MACHINE LEARNING MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1hYvY_NKuIk"
   },
   "source": [
    "#### Hidden Data\n",
    "\n",
    "<!--\n",
    "### Handling Missing Values\n",
    "\n",
    "#drop all the rows\n",
    "df.dropna()\n",
    "#drop a column with missing value\n",
    "df.dropna(axis=1)\n",
    "#imputation of missing value with mean\n",
    "\n",
    "df['B'].fillna(df['B'].mean())\n",
    "df['B'].fillna(df['B'].median())\n",
    "\n",
    "df.drop('deck', axis = 1, inplace=True)\n",
    "#homework>> create a separate column with flag 1 and 0\n",
    "\n",
    "df[\"flag\"]=df.age.apply(lambda x: 1 if pd.isna(x) else 0)\n",
    "\n",
    "\n",
    "# Feature Scalling\n",
    "std_data = []\n",
    "for i in list(df['total_bill']):\n",
    "    zscore = (i-mean)/std\n",
    "    std_data.append(zscore)\n",
    "\n",
    "\n",
    "#scaling is optional because it doesnt change the distribution of data\n",
    "#before scaling\n",
    "sns.distplot(df['total_bill'])\n",
    "\n",
    "#after scaling\n",
    "sns.distplot(std_data)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#Normalization/minmax scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#unit vector\n",
    "from sklearn.preprocessing import normalize\n",
    "#robust scaling >> you have outliers the data\n",
    "#X-median/IQR\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "rs = RobustScaler()\n",
    "\n",
    "\n",
    "#data interpolation >>  is a process of estimating data in a range, getting unknown values from known values\n",
    "1. Linear Interpolation\n",
    "2. cubic interpolation\n",
    "3. Polynomial interpolation\n",
    "\n",
    "#linear interpolation\n",
    "\n",
    "x_new = np.linspace(1, 5, 10)\n",
    "x_new\n",
    "y_interp = np.interp(x_new, x, y)\n",
    "y_interp\n",
    "\n",
    "#cubic interpolation\n",
    "\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([1, 8, 27, 64, 125])\n",
    "plt.scatter(x, y)\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "f = interp1d(x, y, kind = 'cubic')\n",
    "\n",
    "x_new = np.linspace(1, 5, 10)\n",
    "y_interp = f(x_new)\n",
    "\n",
    "\n",
    "#polynomial interpolation\n",
    "\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([1, 4, 9, 1, 25])\n",
    "\n",
    "p = np.polyfit(x, y, 2)\n",
    "\n",
    "x_new = np.linspace(1, 5, 10)\n",
    "y_interp = np.polyval(p, x_new)\n",
    "plt.scatter(x_new, y_interp)\n",
    "\n",
    "===========================================\n",
    "=======================================\n",
    "\n",
    "#data encoding >> converting categorical column to numeric\n",
    "#Nominal/OHE\n",
    "#label and ordinal encoding\n",
    "#target guided ordinal encoding\n",
    "\n",
    "#nominal ohe >> binary vectors for each category\n",
    "#single married, in a relationship\n",
    "#single > [1, 0,a 0]\n",
    "#married > [0, 1, 0]\n",
    "#separated > [0, 0, 1]\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "df = pd.DataFrame({\"qualification\": [\"HS\", \"PG\",\"GR\", \"HS\", \"PhD\", \"HS\", \"PG\"]})\n",
    "encoder = OrdinalEncoder(categories = [[\"HS\", \"GR\", \"PG\", \"PhD\"]])\n",
    "encoder.fit_transform(df[['qualification']])\n",
    "\n",
    "\n",
    "#To check outliers >> distplot, boxplot\n",
    "#dropping the outlier\n",
    "#capping the outlier\n",
    "#replace with mean and median\n",
    "#Scaling and transformation\n",
    "\n",
    "Q1 = df['Salary'].quantile(0.25)\n",
    "Q3 = df['Salary'].quantile(0.75)\n",
    "IQR = Q3-Q1\n",
    "IQR\n",
    "\n",
    "lower_fence = Q1-1.5*IQR\n",
    "upper_fence = Q3+1.5*IQR\n",
    "\n",
    "#dropping the outlier\n",
    "df_filtered = df[(df.Salary >= lower_fence) & (df.Salary <= upper_fence)]\n",
    "\n",
    "#replace the outliers with mean and median\n",
    "df['Salary_mean_imputed'] = np.where((df.Salary >= upper_fence) | (df.Salary <= lower_fence), df['Salary'].mean(), df['Salary'])\n",
    "\n",
    "\n",
    "#capping >> replacing outlier with the nearest values that is not outlier\n",
    "\n",
    "lower_cap = df['Salary'].quantile(0.05) #lower cap as 5th percentile\n",
    "upper_cap = df['Salary'].quantile(0.95)\n",
    "\n",
    "df['Salary_capped'] = np.where(df['Salary'] < lower_cap, lower_cap,\n",
    "                              np.where(df['Salary'] > upper_cap, upper_cap, df['Salary']))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#class imbalance >> one class has higeher percentage\n",
    "#upsampling, downsampling, smote\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(1) #for reproducibility\n",
    "\n",
    "no_samples = 1000\n",
    "class_0_ratio = 0.9\n",
    "no_class_0 = int(no_samples * class_0_ratio)\n",
    "no_class_1 = 100\n",
    "\n",
    "class_0 = {'feature1': np.random.normal(0, 1, no_class_0),\n",
    "'feature2': np.random.normal(0, 1, no_class_0),\n",
    "'target': [0]*no_class_0}\n",
    "\n",
    "\n",
    "class_0 = pd.DataFrame(class_0)\n",
    "\n",
    "class_1 = pd.DataFrame({'feature1': np.random.normal(3, 1, no_class_1),\n",
    "'feature2': np.random.normal(3, 1, no_class_1),\n",
    "'target': [1]*no_class_1})\n",
    "\n",
    "df = pd.concat([class_0, class_1]).reset_index(drop = True)\n",
    "\n",
    "df.target.value_counts(normalize = True)   # show in percentage use normalize true\n",
    "\n",
    "df_minority = df[df.target == 1]\n",
    "df_majority = df[df.target == 0]\n",
    "\n",
    "#oversampling>>upsampling >>increasing the minority to majority no\n",
    "from sklearn.utils import resample\n",
    "\n",
    "df_minority_upsampled = resample(df_minority, replace=True, n_samples = len(df_majority), random_state =1)\n",
    "\n",
    "#downsmapling\n",
    "df_majority_downsampled = resample(df_majority, replace=False, n_samples = len(df_minority), random_state =1)\n",
    "\n",
    "\n",
    "#SMOTE>> synthetic minority oversampling technique\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples = 1000, n_redundant = 0, n_features=2, n_clusters_per_class = 1, weights = [0.90], random_state = 1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(df_final['f1'], df_final['f2'], c = df_final['target'])\n",
    "\n",
    "\n",
    "# !pip install imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(df_final[['f1', 'f2']], df_final['target'])\n",
    "\n",
    "df1 = pd.DataFrame(X, columns = ['f1', 'f2'])\n",
    "df2 = pd.DataFrame(y, columns = ['target'])\n",
    "oversample = pd.concat([df1, df2], axis = 1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(oversample['f1'], oversample['f2'], c = oversample['target'])\n",
    "\n",
    "===========================================================\n",
    "================================================================\n",
    "===============================================================\n",
    "\n",
    "\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WG0khOuxIQ8y"
   },
   "source": [
    "# ðŸ”¹ EDA Steps for Machine Learning\n",
    "\n",
    "### 1. **Data Load**\n",
    "\n",
    "* Import libraries\n",
    "* Load dataset (CSV, SQL, API, etc.)\n",
    "* Preview dataset (`head`, `shape`)\n",
    "---\n",
    "\n",
    "âœ… This outline now has both:\n",
    "\n",
    "* Your **step-by-step workflow (load â†’ clean â†’ preprocess â†’ model assumptions)**\n",
    "* The **structured EDA stages (univariate, bivariate, multivariate, feature engineering, communication)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pM3YS8i40u6Q"
   },
   "source": [
    "## 1. Data Loading and Preprocessing.\n",
    "\n",
    "In these steps, we will load the Bengaluru House Data dataset using pandas and perform an initial exploration to understand its structure and contents.\n",
    "\n",
    "- **Data Import:** The dataset is loaded into a pandas DataFrame named `data`.\n",
    "- **Shape:** The dataset contains 12,530 rows and 7 columns after initial cleaning.\n",
    "- **Columns:**  \n",
    "    - `location`: Area or locality of the property  \n",
    "    - `size`: Number of bedrooms (e.g., \"2 BHK\", \"4 Bedroom\")  \n",
    "    - `total_sqft`: Total area in square feet  \n",
    "    - `bath`: Number of bathrooms  \n",
    "    - `price`: Price of the property (in lakhs)  \n",
    "    - `bhk`: Extracted number of bedrooms as integer  \n",
    "    - `price_per_sqft`: Price per square foot\n",
    "\n",
    "We will also check for missing values, data types, and unique values in key columns to guide further cleaning and preprocessing steps. This foundational understanding helps in identifying potential issues such as outliers, inconsistent data, and the need for encoding categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V58RteqJIjXe"
   },
   "source": [
    "---\n",
    "\n",
    "### 2. **Data Info**\n",
    "\n",
    "* Check dataset structure (`info`)\n",
    "* Identify column data types (numerical, categorical, datetime, text)\n",
    "* Identify target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_bJdHVa10jm"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYeKSSw20kXe"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWRWFvR3Ittu"
   },
   "source": [
    "### 3. **Data Cleaning**\n",
    "\n",
    "* Inspect dataset info again\n",
    "* Remove unwanted/irrelevant columns\n",
    "* Handle duplicates\n",
    "* Null value check\n",
    "* Handle missing values (drop, mean/median/mode imputation, domain-specific methods)\n",
    "* Save cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3skGridjuB4V"
   },
   "outputs": [],
   "source": [
    "# check null\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWo2KPvruQ9N"
   },
   "outputs": [],
   "source": [
    "# 1.check DISTINCT VALUES  in each columns --> categorical values\n",
    "df.nunique()  # -->return unique count number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7AUkG01NclFF"
   },
   "outputs": [],
   "source": [
    "df['area_type'].unique()  # -->return the unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RdCJL2HUcgdR"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ynG3ttCZ2NhZ"
   },
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "  if df[i].dtype == 'object':\n",
    "    print(i, df[i].nunique())\n",
    "    # print(df[i].value_counts())\n",
    "\n",
    "    print('*'*20)\n",
    "\n",
    "# The above loop prints the value counts for each column in the DataFrame,\n",
    "# helping to understand the distribution and frequency of unique values in every column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KxiuNZaYd4Xl"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WftsaTtPd0SB"
   },
   "outputs": [],
   "source": [
    "# drop --> inrelevent columns\n",
    "df.drop(columns=['availability', 'society'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CNlqPE2deeQj"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uh35FSudeu-R"
   },
   "source": [
    "# Remove NUll Values/Replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPG0vgaWaIzH"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2RKhh_AGetrT"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTBfZSCXfH90"
   },
   "outputs": [],
   "source": [
    "# replace the value\n",
    "# locations\n",
    "df[\"location\"].mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VdQalw-PffjT"
   },
   "outputs": [],
   "source": [
    "# replace\n",
    "df[\"location\"].fillna(df[\"location\"].mode()[0],inplace=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uzvmfcn8h_BM"
   },
   "outputs": [],
   "source": [
    "# replace the size with mode\n",
    "df[\"size\"].mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4FGoqE-SiIdg"
   },
   "outputs": [],
   "source": [
    "df[\"size\"].fillna(df[\"size\"].mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ZiveoctiLP6"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2OM6n9YiSe2"
   },
   "outputs": [],
   "source": [
    "df[\"bath\"].mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSfuTfiLffc6"
   },
   "outputs": [],
   "source": [
    "df[\"bath\"].fillna(df[\"bath\"].mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Enp4n_tuatt3"
   },
   "outputs": [],
   "source": [
    "df[\"balcony\"].fillna(df[\"balcony\"].mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DiGbbTaUe9ea"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0y37gViTFPQ"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hj2ZAcvxS6yF"
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TvWZ-81jUHU"
   },
   "source": [
    "# Check DataTypes and correct with cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6DsOVk-4joIM"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOb_C0aXh6jP"
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-AnS5J-TWy8"
   },
   "outputs": [],
   "source": [
    "(df['size'][0][0])\n",
    "# using simple indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bS_U3YOLT1Zh"
   },
   "outputs": [],
   "source": [
    "int(df['size'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wc2v0GMrT7oV"
   },
   "outputs": [],
   "source": [
    "# using split methods\n",
    "df[\"size\"][0].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1qhfVTdnUA-R"
   },
   "outputs": [],
   "source": [
    "df[\"size\"][0].split(\" \")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emLhHkCNkRTq"
   },
   "outputs": [],
   "source": [
    "# size datatype change object into int\n",
    "int(df[\"size\"][0].split(\" \")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9pLjBI3d2Bd4"
   },
   "outputs": [],
   "source": [
    "# # size columns cleaning\n",
    "# df[\"size\"].str.split(\" \")[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5H_AGg_McJ07"
   },
   "outputs": [],
   "source": [
    "def clean_size(x):\n",
    "    return int(x.split(\" \")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8inoKaJLcOXQ"
   },
   "outputs": [],
   "source": [
    "def clean_size(x):\n",
    "  try:\n",
    "    return int(x.split(\" \")[0])\n",
    "  except:\n",
    "    return int(float(x))\n",
    "# if column has null value then use float concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lS-uPQ6Bc_Cy"
   },
   "outputs": [],
   "source": [
    "clean_size(\"2.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5NHenGwd2w7X"
   },
   "outputs": [],
   "source": [
    "df[\"bhk\"] = df[\"size\"].apply(clean_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHyFmZNalfoN"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cz9Us0w1loML"
   },
   "outputs": [],
   "source": [
    "df.drop(columns=[\"size\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sf6k0mH-mq7C"
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpBODhm0lthr"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0V4fwIg2nC8r"
   },
   "outputs": [],
   "source": [
    "# df['total_sqft'].astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zkK83ErolOz"
   },
   "outputs": [],
   "source": [
    "df[\"total_sqft\"].unique()[:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oz1YtcMBnYKq"
   },
   "outputs": [],
   "source": [
    "df[\"total_sqft\"].head(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LzGDPsuZiX4n"
   },
   "outputs": [],
   "source": [
    "df[\"total_sqft\"][30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YElqetX-nkhR"
   },
   "outputs": [],
   "source": [
    "d = df[\"total_sqft\"][30].split(\"-\")\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4EPNO6RQii5o"
   },
   "outputs": [],
   "source": [
    "d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F2EifdoGineU"
   },
   "outputs": [],
   "source": [
    "d[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4XK2p_9iquU"
   },
   "outputs": [],
   "source": [
    "float(d[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MIwHrZYFn-k0"
   },
   "outputs": [],
   "source": [
    "(float(d[0].strip()) +float(d[1].strip()))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Jxshhe37KsX"
   },
   "outputs": [],
   "source": [
    "def convertRange(x):\n",
    "    temp = x.split('-')\n",
    "    if len(temp) == 2:\n",
    "        return (float(temp[0]) + float(temp[1]))/2\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMYMyZkZ7yBP"
   },
   "outputs": [],
   "source": [
    "df[\"total_sqft\"]=df[\"total_sqft\"].apply(convertRange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ik_FNDUi8LZA"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "soDzO4rN870T"
   },
   "outputs": [],
   "source": [
    "df[\"total_sqft\"].fillna(df[\"total_sqft\"].mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLcCkYATnEYO"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSBxoYZDnRpi"
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbPqMoiE1qgy"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MqcrhAuEiFZk"
   },
   "outputs": [],
   "source": [
    "# save the clean file\n",
    "df.to_csv(\"Cleaned_data.csv\",index=False)\n",
    "# df.to_csv(\"Cleaned_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ylCgj4ZJBQP"
   },
   "source": [
    "### 4. **Data Preprocessing**\n",
    "\n",
    "* Segregate categorical & numerical columns\n",
    "* Explore data distribution (graphs, histograms, boxplots, distplots)\n",
    "* Data interpolation (estimate unknown values from known values)\n",
    "* Handle class imbalance (Upsampling, Downsampling, SMOTE, Class weights)\n",
    "* Feature scaling (Normalization, Standardization)\n",
    "* Outlier detection & treatment (boxplot, z-score, IQR method)\n",
    "* Data encoding (Label Encoding, OneHot Encoding)\n",
    "\n",
    "* find the correlations bw variables\n",
    "* Check assumptions of ML models (e.g., Linear Regression â€“ linearity, normality, multicollinearity, homoscedasticity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXNPaOEbiqvz"
   },
   "source": [
    "## Perform EDA for analysis data or understand the data nature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rOjTjD_9b9Ql"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htQC9rwOJZyB"
   },
   "source": [
    "### 5. **Descriptive Statistics & Visualization**\n",
    "\n",
    "* Summary statistics (`describe`)\n",
    "* Numerical feature distribution (histogram, KDE, boxplot)\n",
    "* Categorical feature distribution (countplot, bar chart, pie chart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q2I1-4CyOQxt"
   },
   "outputs": [],
   "source": [
    "# Summary statistics for numerical features\n",
    "print(\"Descriptive Statistics for Numerical Features:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1BG2VoGOWTl"
   },
   "outputs": [],
   "source": [
    "print(df.describe(include=\"O\"))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YlU5r-hLPAue"
   },
   "outputs": [],
   "source": [
    "df.describe(include=\"all\")\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgGZ36cUOakQ"
   },
   "source": [
    "#### CHECK numerical columns and categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s59lclmAOj6d"
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LImnrQYiPiuy"
   },
   "outputs": [],
   "source": [
    "df.columns\n",
    "# show the ALL columns name as list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DqN9WWaPqrn"
   },
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "  if df[i].dtype == 'object':\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtISL-fGPanB"
   },
   "outputs": [],
   "source": [
    "# Visualize distributions of numerical features\n",
    "numerical_cols = df.select_dtypes(include=np.number).columns\n",
    "print(\"\\nVisualizing Distributions of Numerical Features:\")\n",
    "for col in numerical_cols:\n",
    "    if col != 'price_boxcox': # Exclude the transformed price for now if you want to focus on original\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.histplot(df[col], kde=True)\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3z2Cj0XcUSE"
   },
   "outputs": [],
   "source": [
    "# Visualize distributions of categorical features\n",
    "categorical_cols = df.select_dtypes(include='object').columns\n",
    "print(\"\\nVisualizing Distributions of Categorical Features:\")\n",
    "for col in categorical_cols:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(data=df, y=col, order=df[col].value_counts().index[:10]) # Display top 10 for readability\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cCpv8G5N8ys3"
   },
   "source": [
    "### 6. **Examine Relationships**\n",
    "\n",
    "#### 6.1 **Univariate Analysis**\n",
    "\n",
    "* Individual variable distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1na38N8t8z1S"
   },
   "outputs": [],
   "source": [
    "# Visualize distributions of numerical features\n",
    "numerical_cols = df.select_dtypes(include=np.number).columns\n",
    "print(\"\\nVisualizing Distributions of Numerical Features:\")\n",
    "for col in numerical_cols:\n",
    "    if col != 'price_boxcox': # Exclude the transformed price for now if you want to focus on original\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.histplot(df[col], kde=True)\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize distributions of categorical features\n",
    "categorical_cols = df.select_dtypes(include='object').columns\n",
    "print(\"\\nVisualizing Distributions of Categorical Features:\")\n",
    "for col in categorical_cols:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(data=df, y=col, order=df[col].value_counts().index[:10]) # Display top 10 for readability\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8z9HYNFmJiMh"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "#### 6.2 **Bivariate Analysis**\n",
    "\n",
    "* Numerical vs Target (correlation, scatterplot, boxplot)\n",
    "* Categorical vs Target (grouped bar chart, countplot, chi-square test)\n",
    "\n",
    "#### 6.3 **Multivariate Analysis**\n",
    "\n",
    "* Correlation matrix & heatmap\n",
    "* Pairplots\n",
    "* Cross-tabulations\n",
    "* PCA / dimensionality reduction (optional for visualization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aS6eCpQAcV04"
   },
   "outputs": [],
   "source": [
    "# Bivariate Analysis (Numerical vs Target)\n",
    "print(\"\\nBivariate Analysis (Numerical vs Target):\")\n",
    "numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'price' in numerical_cols:\n",
    "    numerical_cols.remove('price') # Exclude target variable from features\n",
    "    if 'price_boxcox' in numerical_cols:\n",
    "        numerical_cols.remove('price_boxcox') # Exclude transformed target as well\n",
    "\n",
    "    for col in numerical_cols:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.scatterplot(data=df, x=col, y='price')\n",
    "        plt.title(f'Scatter plot of {col} vs Price')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Price')\n",
    "        plt.show()\n",
    "\n",
    "# Multivariate Analysis (Correlation Matrix and Heatmap)\n",
    "print(\"\\nMultivariate Analysis (Correlation Matrix):\")\n",
    "numerical_df = df.select_dtypes(include=np.number)\n",
    "correlation_matrix = numerical_df.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhNl2vX_JuPY"
   },
   "source": [
    "### 7. **Feature Engineering**\n",
    "\n",
    "* Create new features\n",
    "* Extract date/time features (day, month, weekday, etc.)\n",
    "* Domain-specific feature creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T7dOAroxcWyX"
   },
   "outputs": [],
   "source": [
    "# Feature Engineering: Create 'price per square foot'\n",
    "df['price_per_sqft'] = df['price'] * 100000 / df['total_sqft']\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JohUbpkVcX7F"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTVK140LJ78I"
   },
   "source": [
    "### 9. **Communicate Findings**\n",
    "\n",
    "* Present visualizations & reports\n",
    "* Share insights with stakeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OF9sXTQJcYrB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eb077055"
   },
   "source": [
    "### Summary of Key EDA Findings\n",
    "\n",
    "Based on the Exploratory Data Analysis performed:\n",
    "\n",
    "* **Data Overview:** We loaded and inspected the Bengaluru House Data, understanding its shape, columns, and initial data types.\n",
    "* **Data Cleaning:**\n",
    "    * Missing values were handled for `location`, `size`, `total_sqft`, `bath`, and `balcony` by imputation with the mode or mean.\n",
    "    * Irrelevant columns (`availability`, `society`) were dropped.\n",
    "    * Data types were corrected, specifically converting `size` to `bhk` (integer) and `total_sqft` to float, handling ranges.\n",
    "    * Outliers were addressed in `total_sqft`, `bath`, `price`, and `bhk`.\n",
    "* **Distribution Analysis:** Visualizations (histograms, countplots) provided insights into the distribution of individual features. We observed the distribution of numerical features and the counts of different categories in `area_type` and `location`.\n",
    "* **Relationship Analysis:**\n",
    "    * Scatter plots showed relationships between numerical features and `price`.\n",
    "    * A correlation matrix highlighted the linear relationships between numerical variables. `total_sqft`, `bath`, and `bhk` show positive correlations with `price`.\n",
    "* **Feature Engineering:** A new feature, `price_per_sqft`, was created, which can be a significant predictor of house prices.\n",
    "* **Skewness Handling:** The `price` column's skewness was addressed using a Box-Cox transformation, resulting in a more normally distributed variable (`price_boxcox`).\n",
    "\n",
    "These findings provide a solid foundation for the next steps in data preprocessing and machine learning model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SzCJ_K229om0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eub8gTy6KB98"
   },
   "source": [
    "### 10. **Iterate & Refine**\n",
    "\n",
    "* Revisit earlier steps if needed\n",
    "* Test new hypotheses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyhbVY5h9rHV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NvJCllHJoSP"
   },
   "source": [
    "### 11. **Key Insights & Hypotheses**\n",
    "\n",
    "* Summarize final findings\n",
    "* Highlight important features\n",
    "* Document anomalies & patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zwIKcruEcaVW"
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Perform ANOVA test to see if there is a significant difference in price between different area types\n",
    "# First, we need to separate the prices by area type\n",
    "area_types = df['area_type'].unique()\n",
    "price_by_area_type = [df['price'][df['area_type'] == area_type] for area_type in area_types]\n",
    "\n",
    "# Perform one-way ANOVA test\n",
    "f_statistic, p_value = stats.f_oneway(*price_by_area_type)\n",
    "\n",
    "print(f\"ANOVA F-statistic: {f_statistic:.2f}\")\n",
    "print(f\"ANOVA P-value: {p_value:.4f}\")\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"\\nConclusion: Reject the null hypothesis. There is a significant difference in price between different area types.\")\n",
    "else:\n",
    "    print(\"\\nConclusion: Fail to reject the null hypothesis. There is no significant difference in price between different area types.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nyP5WPlV-MC_"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Create a contingency table of 'area_type' and 'balcony'\n",
    "contingency_table = pd.crosstab(df['area_type'], df['balcony'])\n",
    "\n",
    "# Perform the chi-square test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(f\"Chi-square statistic: {chi2:.2f}\")\n",
    "print(f\"P-value: {p:.4f}\")\n",
    "print(f\"Degrees of freedom: {dof}\")\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "if p < alpha:\n",
    "    print(\"\\nConclusion: Reject the null hypothesis. There is a significant association between area type and number of balconies.\")\n",
    "else:\n",
    "    print(\"\\nConclusion: Fail to reject the null hypothesis. There is no significant association between area type and number of balconies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtbHKe-I6qNE"
   },
   "source": [
    "## Prepare the data for  machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6I9u2gKecbaI"
   },
   "outputs": [],
   "source": [
    "# EDA-Info,shape\n",
    "# Handling Missing Values\n",
    "# Handling Outliers\n",
    "\n",
    "# Handling Skewness\n",
    "\n",
    "# Data Encoding\n",
    "\n",
    "# Feature Scaling -Normalization and Standardization\n",
    "\n",
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bgoKwTRpcQH4"
   },
   "outputs": [],
   "source": [
    "# load the clean data\n",
    "import pandas as pd\n",
    "# df = pd.read_csv(\"clean_data_for_ml_eda.csv\")\n",
    "df= pd.read_csv(\"https://media.githubusercontent.com/media/shahil04/ds_materials/refs/heads/main/8.0_Machine%20Learning/class/ml_projects/1.house_price_predictions/clean_data_for_ml_eda.csv\")\n",
    "df.head()\n",
    "\n",
    "df1 = df.copy()  # for  comparison data after Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Remove unwanted columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TpLtpCuU9FYy"
   },
   "outputs": [],
   "source": [
    "# df.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "df.drop(columns=[\"availability\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fiux84C4eZdc"
   },
   "source": [
    "# 1. Handling Outliers\n",
    "\n",
    "An outlier is an observation that lies an abnormal distance from other values in a random sample from a population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwhyKyjzem1R"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gP44F7f7dLuS"
   },
   "outputs": [],
   "source": [
    "# check outliers\n",
    "# df['total_sqft']\n",
    "plt.boxplot(df['total_sqft'])\n",
    "plt.show()\n",
    "\n",
    "sns.boxenplot(df['total_sqft'])\n",
    "plt.show()\n",
    "sns.violinplot(df['total_sqft'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fg9VAwOoeITH"
   },
   "outputs": [],
   "source": [
    "# Q. show the details of sales above 30k sqrtfeet\n",
    "df[df[\"total_sqft\"]>30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZlsUQrVDgRgL"
   },
   "outputs": [],
   "source": [
    "df = df[df[\"total_sqft\"]<30000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Vl1vZHPgZIr"
   },
   "outputs": [],
   "source": [
    "df[df[\"total_sqft\"]>30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcEpl23vgh0w"
   },
   "outputs": [],
   "source": [
    "# bath\tbalcony\tprice\tbhk\n",
    "l = ['bath',\t'balcony',\t'price',\t'bhk']\n",
    "for i in l:\n",
    "  print(i)\n",
    "  sns.boxenplot(df[i])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JX9pV-obldBO"
   },
   "outputs": [],
   "source": [
    "# bath 20,pric 3000, bhk 20 conditions for outliers\n",
    "df = df[df[\"bath\"]<20]\n",
    "df = df[df[\"price\"]<3000]\n",
    "df = df[df[\"bhk\"]<20]\n",
    "\n",
    "# df[(df[\"bath\"]>20) ! (df[\"price\"]>30000) ! (df[\"bhk\"]>20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KEvb_hw0mK_Y"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4m3Lm4smr3y"
   },
   "outputs": [],
   "source": [
    "# check outliers for categorical values\n",
    "df['area_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7QxeCnnp46R"
   },
   "outputs": [],
   "source": [
    "df['location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BcBO1XTTm7cN"
   },
   "outputs": [],
   "source": [
    "(df['location'].value_counts().reset_index()[\"count\"]==1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_CVNwwqpKQu"
   },
   "source": [
    "# 2. Handling Skewness\n",
    "\n",
    "Skewness is a statistical measure that describes the asymmetry of a probability distribution, indicating whether the data is stretched more toward the left or right of the mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PtPL7cGkqMki"
   },
   "outputs": [],
   "source": [
    "# check the distributions\n",
    "# histogram only for numerical columns -->specialy from continous data\n",
    "for i in df.columns:\n",
    "  if df[i].dtype != 'object':\n",
    "    lab = i+\" Skewness: \"+str(df[i].skew())\n",
    "    # print(lab)\n",
    "    # print(df[i].skew())\n",
    "    sns.histplot(df[i],kde=True)\n",
    "\n",
    "    plt.legend([lab])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFEk7gCDsyqz"
   },
   "outputs": [],
   "source": [
    "# skewness handle\n",
    "df['price'].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BxqMPjNvxJ-K"
   },
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/skewness-be-gone-transformative-tricks-for-data-scientists/\n",
    "# 1. sqrt\n",
    "# 2. square\n",
    "# 3. log\n",
    "# 4. box-cox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5doue9Hvhl_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.sqrt(df['price']).skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ImW7HZMHv6Ph"
   },
   "outputs": [],
   "source": [
    "np.square(df['price']).skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvikxJEOv-ru"
   },
   "outputs": [],
   "source": [
    "np.log(df['price']).skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66EO3Uczvp9P"
   },
   "outputs": [],
   "source": [
    "sns.histplot(np.log(df['price']),kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PvpkgsVxOYX"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "\n",
    "df2=pd.DataFrame()\n",
    "# Apply Box-Cox transformation to the 'price' column\n",
    "# df['total_sqft_box_cox'], lambda_price = boxcox(df['total_sqft'])\n",
    "df2['total_sqft_box_cox'], lambda_price = boxcox(df['total_sqft'])\n",
    "# Display the skewness of the original and transformed price\n",
    "print(f\"Original Price Skewness: {df['total_sqft'].skew()}\")\n",
    "print(f\"Box-Cox Transformed Price Skewness: {df2['total_sqft_box_cox'].skew()}\")\n",
    "\n",
    "# Visualize the distribution of the transformed price\n",
    "sns.histplot(df2['total_sqft_box_cox'], kde=True)\n",
    "plt.title('Distribution of Box-Cox Transformed Price')\n",
    "plt.xlabel('Box-Cox Transformed Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18l2_igLxO6d"
   },
   "source": [
    "# 3. Feature Scaling -Normalization and Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKw_H4jqvTI7"
   },
   "source": [
    "## Scaling\n",
    "- change the range of value in fixed range\n",
    "- Normalization (min-max scaler) -->range (0-1)/(-1 to 1)\n",
    "- Standarization (z-score) --> mean=0, std=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1Lip59Koun8"
   },
   "outputs": [],
   "source": [
    "# x-xmin/xmax-xmin\n",
    "li = [1,2,3,4]\n",
    "for x in li:\n",
    "  new_x = (x-min(li))/(max(li)-min(li))\n",
    "\n",
    "  print(new_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2i5GnxPDvbHx"
   },
   "outputs": [],
   "source": [
    "# !pip install scikit-learn\n",
    "# scaling\n",
    "from sklearn.preprocessing import  MinMaxScaler,StandardScaler, RobustScaler\n",
    "\n",
    "# MinMaxScaler --> range(0-1) ,it not handle outliers\n",
    "# StandardScaler -->mean 0, std =1 --> std normal distributions\n",
    "#  RobustScaler --> it handle the outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object\n",
    "min_max = MinMaxScaler()\n",
    "\n",
    "min_max.fit(df[[\"total_sqft\"]])  # it get the min and max value\n",
    "\n",
    "min_max.transform(df[[\"total_sqft\"]])  # it apply the formula in data and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UBiCNGomvdRc"
   },
   "outputs": [],
   "source": [
    "# change the range of total_sqft into 0 to 1\n",
    "min_max.fit_transform(df[[\"total_sqft\"]])  # find min max and transform data\n",
    "\n",
    "scaled_data = min_max.fit_transform(df[[\"total_sqft\"]])\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtEAb_cHyFKq"
   },
   "outputs": [],
   "source": [
    "# reassign\n",
    "df[\"total_sqft\"] = scaled_data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tf5bbzJnzyt"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# change the range of where mean=0 and std =1\n",
    "scaled_data = scaler.fit_transform(df[[\"total_sqft\"]])\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPGeNucnxCOq"
   },
   "outputs": [],
   "source": [
    "# robust\n",
    "scaler = RobustScaler()\n",
    "# Why use robust scaling\n",
    "# Handles outliers: It is robust to the presence of outliers, which can distort other scaling methods that rely on the mean and standard deviation.\n",
    "# Less sensitive to skewed data: It is suitable for datasets with non-normal distributions, which are common in real-world data.\n",
    "# Improves model performance:\n",
    "# X_scaled=(X-X_median)/IQR)\n",
    "scaled_data = scaler.fit_transform(df[[\"total_sqft\"]])\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzDWPAvTyoFK"
   },
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtP-SlJScGYJ"
   },
   "source": [
    "## 4.Encoding\n",
    "\n",
    "Encoding in machine learning is the process of converting categorical data(like text or non-numeric labels) into a numerical format that machine learning algorithms can understand and process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dcD5BWIZkCwZ"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqCZYOTz092-"
   },
   "source": [
    "## encoding\n",
    "* types\n",
    "  1. nominal/Label encoding --> only name in data (ranking is not important)\n",
    "  2. ordinal encodnig --> (ranking is important)\n",
    "  3. one-hot encoding/ df_dummies -->( all are equal)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T5K1MDIV7NsZ"
   },
   "outputs": [],
   "source": [
    "# text column\n",
    "df[\"area_type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M5i6HQDg7WjB"
   },
   "outputs": [],
   "source": [
    "df[\"area_type\"].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3sirdL7V0x4J"
   },
   "outputs": [],
   "source": [
    "# sk-learn\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder , OneHotEncoder\n",
    "\n",
    "label= LabelEncoder()  #change the categorical data  into number without order\n",
    "\n",
    "l = label.fit_transform(df[\"area_type\"])\n",
    "\n",
    "print(l[:50]) # print 1st 50 data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-J-ek4GNmh-8"
   },
   "outputs": [],
   "source": [
    "label.classes_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVPz2q7WrE-d"
   },
   "outputs": [],
   "source": [
    "df[\"area_type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9q9QM3w6Ao53"
   },
   "outputs": [],
   "source": [
    "# Ordinal for apply the order in distinct values\n",
    "\n",
    "ord = OrdinalEncoder(categories=[[\"Super built-up  Area\",\"Built-up  Area\",\"Plot  Area\",\"Carpet  Area\"]])\n",
    "\n",
    "o = ord.fit_transform(df[[\"area_type\"]])\n",
    "print(o[:50])\n",
    "\n",
    "# o = ord.fit_transform(df[\"area_type\"].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOtmTx8q99bW"
   },
   "outputs": [],
   "source": [
    "ord.categories_  # show the order of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dokqmR8xA4_W"
   },
   "outputs": [],
   "source": [
    "# show the data with actual data\n",
    "for i in zip(df[\"area_type\"],o):\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/1*ggtP4a5YaRx6l09KQaYOnw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9fw9ukxBMIa"
   },
   "outputs": [],
   "source": [
    "# One-hot encoding is a process that converts categorical data into a numerical format that machine learning algorithms can process. It creates new binary (0 or 1) columns for each unique category in a feature, with a \"1\" indicating the presence of that category and \"0\" indicating its absence for a given row\n",
    "# Binary number --> 0,1 if value then 1 and rest of then equal 0\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "o =ohe.fit_transform(df[[\"area_type\"]]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_3L1fl0n-pk"
   },
   "outputs": [],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z32v26qPBnuF"
   },
   "outputs": [],
   "source": [
    "ohe.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BYbsC0zdBt1N"
   },
   "outputs": [],
   "source": [
    "area_ohe = pd.DataFrame(o,columns=ohe.get_feature_names_out())\n",
    "area_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.reset_index(drop=True)\n",
    "print(\"\\nDataFrame after reset_index(drop=True):\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add data into existing df and area_ohe into df1\n",
    "\n",
    "df = pd.concat([df,area_ohe],axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('area_type', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label= LabelEncoder()  #change the categorical data  into number without order\n",
    "\n",
    "# l = label.fit_transform(df[\"location\"])\n",
    "df[\"location\"] = label.fit_transform(df[\"location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# afer preprocessing data save for ml \n",
    "df.to_csv(\"data_preprocess_for_ml.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create simple Linear Model model \n",
    "\n",
    "Y =mx + c\n",
    "\n",
    "* use data: \n",
    "    - X = df['total_sqft']\n",
    "    - y = df[df['price']]\n",
    "\n",
    "create a model to find price(y) of house using only total sqft. data as x,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from simple data\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data one x and one y p\n",
    "X = df1[['total_sqft']]\n",
    "y = df1['price']\n",
    "X.shape\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(X, y)\n",
    "plt.plot([0,20000], [0, 3000] ,'r')\n",
    "plt.plot([0,15000], [0, 3000] ,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr.intercept_)\n",
    "print(lr.coef_)\n",
    "\n",
    "# y= 0.104*X + (-48.5)  # model \n",
    "\n",
    "# y= mx + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Get the intercept and coefficient\n",
    "intercept = lr.intercept_\n",
    "coefficient = lr.coef_[0] # For simple linear regression, coef_ is an array\n",
    "\n",
    "# 4. Generate points for the regression line\n",
    "# You need a range of x-values to plot the line\n",
    "x_line = np.linspace(X.min(),X.max(), 100).reshape(-1, 1)\n",
    "\n",
    "y_line = coefficient * x_line + intercept\n",
    "\n",
    "# 5. Plot the original data points and the regression line\n",
    "plt.scatter(X, y, color='blue', label='Data Points')\n",
    "plt.plot(x_line, y_line, color='red', label=f'Regression Line: y = {coefficient:.2f}x + {intercept:.2f}')\n",
    "plt.xlabel('Independent Variable (X)')\n",
    "plt.ylabel('Dependent Variable (y)')\n",
    "plt.title('Linear Regression Line')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(df1[['total_sqft','bhk','bath']], df1['price'])\n",
    "lr.score(df1[['total_sqft','bhk','bath']], df1['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oAFDURvNBuo2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to we know which columns important or which not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUH-ifZZ7A9Y"
   },
   "outputs": [],
   "source": [
    "# load the clean data\n",
    "import pandas as pd\n",
    "# df = pd.read_csv(\"clean_data_for_ml_eda.csv\")\n",
    "df1= pd.read_csv(\"https://media.githubusercontent.com/media/shahil04/ds_materials/refs/heads/main/8.0_Machine%20Learning/class/ml_projects/1.house_price_predictions/clean_data_for_ml_eda.csv\")\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asDWdSbc7B_1"
   },
   "source": [
    "### 5. **Feature Relationships**\n",
    "\n",
    "* Check multicollinearity (VIF)\n",
    "* Identify strong predictors\n",
    "* Remove redundant features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multicollinearity is a situation in a multiple regression model where two or more independent variables are highly correlated, meaning they have a linear relationship and provide redundant information. This makes it difficult to determine the independent effect of each variable on the dependent variable, potentially leading to unreliable model results and wider confidence intervals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. VIF\n",
    "2. RFE \n",
    "\n",
    " VIF (Variance Inflation Factor) is a common metric for detecting multicollinearity in a regression model.\n",
    "\n",
    "Recursive Feature Elimination (RFE) is a feature-selection method that helps you automatically remove variables causing multicollinearity or low predictive value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "# Separate features (X) from the target variable (y)\n",
    "\n",
    "\n",
    "numerical_cols_original = df1.select_dtypes(include=np.number).columns.tolist()\n",
    "# Create a DataFrame with just the numerical features from the original df\n",
    "numerical_features_df = df1[numerical_cols_original]\n",
    "numerical_features_df = numerical_features_df.drop('price',axis=1)\n",
    "numerical_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle potential infinite VIF if there's perfect multicollinearity (e.g., constant column)\n",
    "# Add a small constant to avoid division by zero if a column has zero variance\n",
    "numerical_features_df = numerical_features_df + 1e-9\n",
    "\n",
    "# Calculate VIF for each numerical feature and store in dataframe\n",
    "vif_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIF (Variance Inflation Factor) is a common metric for detecting multicollinearity in a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_data[\"feature\"] = numerical_features_df.columns\n",
    "\n",
    "# Ensure the data type is float for variance_inflation_factor\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(numerical_features_df.values.astype(float), i)\n",
    "                   for i in range(numerical_features_df.shape[1])]\n",
    "\n",
    "print(\"Variance Inflation Factor (VIF) for Numerical Features:\")\n",
    "display(vif_data.sort_values(by=\"VIF\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Standard VIF Guidelines**\n",
    "\n",
    "| VIF Value | Interpretation                              | Action                                                                             |\n",
    "| --------- | ------------------------------------------- | ---------------------------------------------------------------------------------- |\n",
    "| **1â€“5**   | Moderate correlation but usually acceptable | Often *no action* needed                                                           |\n",
    "| **> 5**   | Potentially concerning multicollinearity    | Investigate further (correlations, model design)                                   |\n",
    "| **> 10**  | Serious multicollinearity problem           | Typically warrants correction (remove or combine predictors, regularization, etc.) |\n",
    "\n",
    "### **More Nuanced View (Used in Practice)**\n",
    "\n",
    "* **VIF > 2.5** may already be problematic in **small samples** or **noisy data**.\n",
    "* **VIF > 4** is sometimes used as a cutoff in **econometrics**.\n",
    "* **VIF > 10** is the **classical rule of thumb** from older regression diagnostics (e.g., Kutner, Nachtsheim & Neter).\n",
    "\n",
    "### **Why VIF Thresholds Vary**\n",
    "\n",
    "Threshold choice depends on:\n",
    "\n",
    "* **Model purpose** (prediction vs. inference)\n",
    "* **Sample size**\n",
    "* **Acceptable variance inflation**\n",
    "* **Domain norms** (e.g., social sciences vs. engineering)\n",
    "\n",
    "### **Quick Practical Advice**\n",
    "\n",
    "* **If inference (p-values, coefficients) matters:** aim for **VIF < 5**, ideally **< 3**.\n",
    "* **If only prediction matters:** higher VIFs are more tolerableâ€”regularized models (Lasso/Ridge) can handle collinearity without removing variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **suggest:**\n",
    "\n",
    "1. **Check correlation matrix** between bhk, bath, total_sqft.\n",
    "2. **Try dropping one of (bhk, bath)** and compare model metrics.\n",
    "\n",
    "\n",
    "<!-- Hereâ€™s a clear interpretation of your VIF values and what you should do next:\n",
    "\n",
    "---\n",
    "\n",
    "## **Interpretation of Your VIF Table**\n",
    "\n",
    "| Feature        | VIF       | Interpretation                   |\n",
    "| -------------- | --------- | -------------------------------- |\n",
    "| **bhk**        | **26.50** | Extremely high multicollinearity |\n",
    "| **bath**       | **26.37** | Extremely high multicollinearity |\n",
    "| **balcony**    | 3.68      | Acceptable / moderate            |\n",
    "| **total_sqft** | 2.96      | Acceptable                       |\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "You have **serious multicollinearity** between **bhk** and **bath**.\n",
    "\n",
    "A VIF above **10** is considered severe; both of these are >26, which means the variables are highly redundant.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why is bhk and bath highly collinear?**\n",
    "\n",
    "In real estate datasets, this is common:\n",
    "\n",
    "* More **BHK â†’ more bathrooms**\n",
    "* Bathrooms scale with apartment size & number of rooms\n",
    "* Thus they tend to be strongly correlated\n",
    "\n",
    "Your model is probably struggling to distinguish their individual effects.\n",
    "\n",
    "---\n",
    "\n",
    "## **What you should do**\n",
    "\n",
    "### **Option 1 (Best for interpretability): Remove one of them**\n",
    "\n",
    "Check which one is more predictive individually:\n",
    "\n",
    "* Compare model performance after dropping **bhk**\n",
    "* Compare performance after dropping **bath**\n",
    "\n",
    "Keep the one that improves validation score.\n",
    "\n",
    "### **Option 2: Combine them into a single engineered feature**\n",
    "\n",
    "Examples:\n",
    "\n",
    "* **bath_per_bhk = bath / bhk**\n",
    "* **rooms_ratio = bhk / total_sqft**\n",
    "* **utility_score = bhk + bath**\n",
    "\n",
    "This often reduces multicollinearity while keeping signal.\n",
    "\n",
    "### **Option 3: Use regularization (Ridge or Lasso)**\n",
    "\n",
    "If you use:\n",
    "\n",
    "* **Lasso** â†’ May drop one variable automatically\n",
    "* **Ridge** â†’ Keeps both but reduces instability\n",
    "\n",
    "This is good if prediction (not coefficient interpretability) is your goal.\n",
    "\n",
    "### **Option 4: Centering variables (mean subtraction)**\n",
    "\n",
    "This helps numerically but **does not actually fix** collinearityâ€”just makes coefficients more stable.\n",
    "\n",
    "---\n",
    "\n",
    "## **Recommended next step**\n",
    "\n",
    "I suggest:\n",
    "\n",
    "1. **Check correlation matrix** between bhk, bath, total_sqft.\n",
    "2. **Try dropping one of (bhk, bath)** and compare model metrics.\n",
    "\n",
    "If you want, send me your correlation matrix or your model goal (prediction vs. interpretability), and Iâ€™ll tell you *exactly* which option is best for your dataset. -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZCQDZCmunxA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_9lqXJd__y23"
   },
   "outputs": [],
   "source": [
    "# Identify features with high VIF to remove\n",
    "# Based on the calculated VIF values, both 'bath' (VIF: 26.96) and 'bhk' (VIF: 26.31) have high multicollinearity. This is likely because the number of bathrooms and the number of bedrooms are often strongly related to each other and possibly to the total square footage of the property.\n",
    "\n",
    "# Having high multicollinearity between features can affect the interpretation and stability of some linear models. You might consider:\n",
    "\n",
    "# Removing one of the highly correlated features: For example, you could remove either 'bath' or 'bhk'.\n",
    "# Using techniques less sensitive to multicollinearity: Models like Ridge or Lasso regression include regularization that can handle multicollinearity.\n",
    "# Combining features: Create a new feature that represents the relationship between 'bath' and 'bhk'.\n",
    "# Would you like me to help you explore any of these options, or would you prefer to proceed with the current features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFE \n",
    "Recursive Feature Elimination (RFE) is a feature selection technique that works by iteratively training a model, ranking its features by importance, and removing the least important ones until a desired number of features is reached\n",
    "\n",
    "\n",
    "https://www.machinelearningmastery.com/rfe-feature-selection-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = df[['total_sqft', 'bath', 'bhk', 'balcony']]\n",
    "y = df['price']\n",
    "\n",
    "model = LinearRegression()\n",
    "rfe = RFE(model, n_features_to_select=3)\n",
    "rfe = rfe.fit(X, y)\n",
    "\n",
    "print(\"Selected Features:\", X.columns[rfe.support_].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using RFECV (Automatically finds optimal number of features)\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "rfecv = RFECV(estimator=LinearRegression(), cv=5)\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(\"Optimal number of features:\", rfecv.n_features_)\n",
    "print(\"Selected features:\", X.columns[rfecv.support_].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df3a444b"
   },
   "source": [
    "## 6. Correlations\n",
    "\n",
    "Based on the correlation matrix:\n",
    "\n",
    "*   **Positive Correlations with Price:** `total_sqft`, `bath`, and `bhk` show positive correlations with the target variable `price`. This is expected, as larger properties with more bathrooms and bedrooms typically have higher prices.\n",
    "*   **Strong Correlation between Bath and BHK:** There is a strong positive correlation between `bath` and `bhk`, which was also highlighted by the VIF analysis.\n",
    "*   **Price per Square Foot:** The engineered feature `price_per_sqft` also shows a positive correlation with price, although its relationship with the original numerical features might need further investigation.\n",
    "*   **Other Correlations:** Review the heatmap in the previous output for specific correlation values between all numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKXIbP9w7EjN"
   },
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X7uVRgZa0SAj"
   },
   "outputs": [],
   "source": [
    "# create a bi-variant using price and total_sqft\n",
    "plt.scatter(df1[\"price\"],df1[\"total_sqft\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate Analysis (Correlation Matrix and Heatmap)\n",
    "print(\"\\nMultivariate Analysis (Correlation Matrix):\")\n",
    "numerical_df = df1.select_dtypes(include=np.number)\n",
    "correlation_matrix = numerical_df.corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPff814c7AsQ"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Data /Featrue Selection for Machine Learning Models\n",
    "# then save the data\n",
    "# Preprocess the data \n",
    "# the use the data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx3eqEI-yiWR"
   },
   "outputs": [],
   "source": [
    "# # prepared_data for ml save\n",
    "# df.to_csv(\"prepared_ml_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode â†’ Scale â†’ Feature Select â†’ Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.feature_selection import RFE\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# pipe = Pipeline([\n",
    "#     ('encode', OneHotEncoder(handle_unknown='ignore')),\n",
    "#     ('scale', StandardScaler(with_mean=False)),   # if sparse matrix after OHE\n",
    "#     ('select', RFE(LinearRegression(), n_features_to_select=5)),\n",
    "#     ('model', LinearRegression())\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TajX2Hqez_8c"
   },
   "source": [
    "## Model Create |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_Ds6AgY0_ze"
   },
   "outputs": [],
   "source": [
    "# import\n",
    "df = pd.read_csv(r\"clean_data_for_ml_eda.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9X0bVkRH0_mE"
   },
   "outputs": [],
   "source": [
    "## Model Building\n",
    "# 1. select the numerical columns\n",
    "# 2. seprate the x and y\n",
    "# 2.1 split the data into training testing\n",
    "# 3. load the algorithms and create the object of algoritms\n",
    "# 4. train the model --> find the pattern value from data\n",
    "# 5. test then model/predict\n",
    "# 6. Evaluate/accuracy the model\n",
    "\n",
    "# 7. fine tuning the model(for improve accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pliU0tmx6fRI"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "szW6yUtu5LC9"
   },
   "outputs": [],
   "source": [
    "# 1. select the numerical columns\n",
    "df = df.drop(columns=[\"area_type\",\"location\",'availability'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7tYcHMqy6jez"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mza8pfWh6p67"
   },
   "outputs": [],
   "source": [
    "# 2. seprate the feature/input (x) and label/target (y) \n",
    "X = df.drop(columns=[\"price\"])\n",
    "y = df[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uVU4n1nY6vwr"
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7OfCipa7cEQ"
   },
   "outputs": [],
   "source": [
    "# 2. split the data into training testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state =0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train\n",
    "y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZOMyIu-895yk"
   },
   "outputs": [],
   "source": [
    "# 3. load the algorithms and create the object of algoritms\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gg-A51KI_WcJ"
   },
   "outputs": [],
   "source": [
    "# 4. train the model --> find the pattern value from data\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jSs-McG_cQy"
   },
   "outputs": [],
   "source": [
    "# coffecient, intercept\n",
    "print(lr.coef_)\n",
    "print(lr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ya80s3z6CG6b"
   },
   "outputs": [],
   "source": [
    "# check the trainning accuracy\n",
    "lr.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4b3lrANCXmo"
   },
   "outputs": [],
   "source": [
    "# check the testing accuracy\n",
    "lr.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pd66nb5qCweH"
   },
   "outputs": [],
   "source": [
    "# 5. test then model/predict\n",
    "y_pred = lr.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# errors CALCULATE\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "print(\"MAE:\",mean_absolute_error(y_test,y_pred))\n",
    "print(\"MSE:\",mean_squared_error(y_test,y_pred))\n",
    "print(\"RMSE:\",np.sqrt(mean_squared_error(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GFsTCpngDFoC"
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZWRIDBvElPT"
   },
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nBQP01ySEXxZ"
   },
   "outputs": [],
   "source": [
    "# 1000,2,1,2 -->price\n",
    "sqrt = int(input(\"Enter total_sqft: \"))\n",
    "bath = int(input(\"Enter number of bathrooms: \"))\n",
    "bhk = int(input(\"Enter number of bedrooms (BHK): \"))\n",
    "balcony = int(input(\"Enter number of balcony: \"))\n",
    "\n",
    "lr.predict([[sqrt,bath,bhk,balcony]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XJ2nxl3zFRU3"
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "import pickle\n",
    "pickle.dump(lr,open(\"lrmodel.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n8CETg7TFmH1"
   },
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = pickle.load(open(\"lrmodel.pkl\",\"rb\"))\n",
    "\n",
    "sqrt = int(input(\"Enter total_sqft: \"))\n",
    "bath = int(input(\"Enter number of bathrooms: \"))\n",
    "bhk = int(input(\"Enter number of bedrooms (BHK): \"))\n",
    "balcony = int(input(\"Enter number of balcony: \"))\n",
    "\n",
    "model.predict([[sqrt,bath,bhk,balcony]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wSCVYl-5_RrB"
   },
   "outputs": [],
   "source": [
    "# 6. Evaluate/accuracy the model\n",
    "# 7. fine tuning the model(for improve accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnnmdeSzcUVI"
   },
   "source": [
    "==================================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKca6ZXRv3We"
   },
   "source": [
    "# 2nd way for same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VFHWVc9-5T_Q"
   },
   "outputs": [],
   "source": [
    "# After clean data\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"https://media.githubusercontent.com/media/shahil04/ds_materials/refs/heads/main/8.0_Machine%20Learning/class/ml_projects/1.house_price_predictions/clean_data_for_ml_eda.csv\")\n",
    "\n",
    "df = df.drop('availability',axis=1)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(r'C:\\Users\\hp\\Documents\\ds_materials\\8.0_Machine Learning\\class\\ml_projects\\1.house_price_predictions\\data_preprocess_for_ml.csv')\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X = df1.drop(columns=[\"price\"])\n",
    "y = df1[\"price\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()\n",
    "location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000,2,1,2 -->price\n",
    "location = input(\"enter your locations\")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "la = LabelEncoder()\n",
    "la.transform \n",
    "location = l\n",
    "sqrt = int(input(\"Enter total_sqft: \"))\n",
    "bath = int(input(\"Enter number of bathrooms: \"))\n",
    "bhk = int(input(\"Enter number of bedrooms (BHK): \"))\n",
    "balcony = int(input(\"Enter number of balcony: \"))\n",
    "\n",
    "lr.predict([[sqrt,bath,bhk,balcony]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2J15eJSC-EgO"
   },
   "outputs": [],
   "source": [
    "# 2nd way\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import ColumnTransformer # get each column and apply indivisualy changes\n",
    "from sklearn.pipeline import make_pipeline  # create a pipeline \n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "df = pd.read_csv(\"https://media.githubusercontent.com/media/shahil04/ds_materials/refs/heads/main/8.0_Machine%20Learning/class/ml_projects/1.house_price_predictions/clean_data_for_ml_eda.csv\")\n",
    "df = df.drop('availability',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TpVi_Ue18-IP"
   },
   "outputs": [],
   "source": [
    "# task 1 transform applying\n",
    "# columns transform\n",
    "columns_trans = ColumnTransformer(\n",
    "    [('onehot_location', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), ['location']),\n",
    "     ('onehot_area_type', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), [\"area_type\"]),\n",
    "     ('scaler', StandardScaler(), [\"total_sqft\", \"bath\"]),\n",
    "     ],\n",
    "    remainder='passthrough')\n",
    "# task 2 model apply\n",
    "# model\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2c64Dves_-SU"
   },
   "outputs": [],
   "source": [
    "#pipeline\n",
    "pipe = make_pipeline(columns_trans,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r3rECBuFBj5J"
   },
   "outputs": [],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "woq0GdvEAhKy"
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=[\"price\"])\n",
    "y = df[\"price\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2cbJ9nsAjMb"
   },
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_W-HAxETd2w"
   },
   "outputs": [],
   "source": [
    "pipe.score(X_train,y_train)\n",
    "# trainig accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_train, pipe.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZ1MjCMZBBU8"
   },
   "outputs": [],
   "source": [
    "# predicted y Predictions \n",
    "y_pred = pipe.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tM2OcG-Tugu"
   },
   "outputs": [],
   "source": [
    "# R2 score  --> calculate the variance ratio \n",
    "from sklearn.metrics import r2_score\n",
    "# for new data  \n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2TAkC1MgsqX"
   },
   "outputs": [],
   "source": [
    "# save mode\n",
    "import pickle\n",
    "pickle.dump(pipe,open(\"model2.pkl\",\"wb\"))\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJP3jvrCSk3W"
   },
   "source": [
    "# Performance Matrix\n",
    "Measuring Performance metrics-Lost and Cost Function (MAE,MSE,RMSE,R2 Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ByS9DK4gSr3o"
   },
   "outputs": [],
   "source": [
    "# cost functions --> calculate erros\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error,root_mean_squared_error\n",
    "\n",
    "print(\"MAE:\",mean_absolute_error(y_test,y_pred))\n",
    "print(\"MSE:\",mean_squared_error(y_test,y_pred))\n",
    "print(\"RMSE:\",root_mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IxvrGk-ch3w"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIuPUx3yciUt"
   },
   "source": [
    "# Regression Performance check using r2_score ,and Adjusted r2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HyvSJ2l-DvJs"
   },
   "outputs": [],
   "source": [
    "# Regression Performance check using r2_score ,and Adjusted r2 score\n",
    "from sklearn.metrics import r2_score\n",
    "r_squared = r2_score(y_test,y_pred)\n",
    "r_squared\n",
    "\n",
    "# https://benjaminobi.medium.com/what-really-is-r2-score-in-linear-regression-20cafdf5b87c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m23aGQaIWF-B"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wddRRO1qV4s4"
   },
   "outputs": [],
   "source": [
    "n_samples = df.shape[0]\n",
    "n_features = df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_BuJfU_xE_0i"
   },
   "outputs": [],
   "source": [
    "adjusted_r2 = 1 - (1 - r_squared) * (n_samples - 1) / (n_samples - n_features - 1)\n",
    "adjusted_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRn9iqUyEvFD"
   },
   "outputs": [],
   "source": [
    "# save the movel for future use\n",
    "import pickle\n",
    "pickle.dump(pipe,open(\"model.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "pipe = pickle.load(open(\"model.pkl\",\"rb\"))\n",
    "\n",
    "# take input from user\n",
    "location = input(\"Enter location: \")\n",
    "area_type = input(\"Enter area type: \")\n",
    "total_sqft = float(input(\"Enter total square feet: \"))\n",
    "bath = float(input(\"Enter number of bathrooms: \"))\n",
    "bhk = int(input(\"Enter number of bedrooms (BHK): \"))\n",
    "balcony = int(input(\"Enter number of balcony: \"))\n",
    "\n",
    "# Create a DataFrame from user input\n",
    "user_input = pd.DataFrame([[location, area_type, total_sqft, bath, bhk,balcony]],\n",
    "                          columns=['location', 'area_type', 'total_sqft', 'bath', 'bhk',\"balcony\"])\n",
    "\n",
    "# Predict the price\n",
    "predicted_price = pipe.predict(user_input)\n",
    "\n",
    "print(f\"The predicted price is: {predicted_price[0]:.2f} Lakhs\")\n",
    "# Electronic City Phase II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POWgAszs9dBI"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6H_C4tdsgErK"
   },
   "outputs": [],
   "source": [
    "locations = df[\"location\"].unique()\n",
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "kNIXeqRHCG85"
   },
   "outputs": [],
   "source": [
    "locations = df[\"location\"].unique()\n",
    "locations\n",
    "# save the data for future use\n",
    "pickle.dump(locations, open(\"locations.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "BHE4azOvCo7d"
   },
   "outputs": [],
   "source": [
    "# carpet area\n",
    "carpet_area = df[\"area_type\"].unique()\n",
    "carpet_area\n",
    "# save the data for future use\n",
    "pickle.dump(carpet_area, open(\"area_type.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84iGvD3PbfTI"
   },
   "outputs": [],
   "source": [
    "# Save the Streamlit app code to app.py\n",
    "# with Dropdown meanu use\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# ----------------------\n",
    "# Load Model & Dropdown Data\n",
    "# ----------------------\n",
    "pipe = pickle.load(open(\"model.pkl\", \"rb\"))\n",
    "locations = pickle.load(open(\"locations.pkl\", \"rb\"))\n",
    "area_types = pickle.load(open(\"area_type.pkl\", \"rb\"))\n",
    "\n",
    "# ----------------------\n",
    "# Streamlit App\n",
    "# ----------------------\n",
    "st.set_page_config(page_title=\"House Price Prediction\", layout=\"centered\")\n",
    "st.title(\"ðŸ  House Price Prediction App\")\n",
    "st.write(\"Enter property details below to get an estimated price (in Lakhs).\")\n",
    "\n",
    "# User inputs\n",
    "location = st.selectbox(\"Select Location\", locations)\n",
    "area_type = st.selectbox(\"Select Area Type\", area_types)\n",
    "total_sqft = st.number_input(\"Enter Total Square Feet\", min_value=100.0, step=10.0)\n",
    "bath = st.number_input(\"Enter Number of Bathrooms\", min_value=1.0, step=1.0)\n",
    "bhk = st.number_input(\"Enter Number of Bedrooms (BHK)\", min_value=1, step=1)\n",
    "balcony = st.number_input(\"Enter number of Balcony: \", min_value=0)\n",
    "\n",
    "\n",
    "# Prediction button\n",
    "if st.button(\"Predict Price\"):\n",
    "    # Create DataFrame from inputs\n",
    "    user_input = pd.DataFrame([[location, area_type, total_sqft, bath, bhk,balcony]],\n",
    "                          columns=['location', 'area_type', 'total_sqft', 'bath', 'bhk','balcony'])\n",
    "\n",
    "    # Make prediction\n",
    "    predicted_price = pipe.predict(user_input)[0]\n",
    "\n",
    "    st.success(f\"ðŸ’° Estimated Price: **{predicted_price:.2f} Lakhs**\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lvjl5EsRH9OA"
   },
   "outputs": [],
   "source": [
    "# steps\n",
    "# 1. take all pkl files in one folder after training\n",
    "# 2. create aap.py --> insert all code\n",
    "# 3. create requirements.txt --> write all library which is needed to run project\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cjjDT3sOlHs"
   },
   "outputs": [],
   "source": [
    "# Assumtions\n",
    "data = pd.read_csv(\"/content/Bengaluru_House_Data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XSHrGokpOyye"
   },
   "outputs": [],
   "source": [
    "# Regression models --> liner---> LinearRegression, ridge lasso, SGDRegressor ,\n",
    "#       non-linear -->(Decision tree) DecisionTreeRegressor, RandomForestRegressor,\n",
    "#       ensembles models --> GradientBoostingRegressor , support_vector_regression\n",
    "\n",
    "# classifications ---> Logistic regression(linear)\n",
    "# (non-linear) SVM(SVC), KNN,naive bayes, Decision tree(classifier),RandomForestClassifer,\n",
    "#       ensembles models --> all\n",
    "\n",
    "\n",
    "# Deeplearning --> non-linear -Neural Network (only change in architecture for diffrent work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LLI3uB5jc1k-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U1rcgvR2c1ai"
   },
   "outputs": [],
   "source": [
    "r = Ridge(.2)\n",
    "p2 =make_pipeline(columns_trans,r)\n",
    "p2.fit(X_train,y_train)\n",
    "y_pred = p2.predict(X_test)\n",
    "# cost functions --> calculate erros\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error,root_mean_squared_log_error\n",
    "print(\"MAE:\",mean_absolute_error(y_test,y_pred))\n",
    "print(\"MSE:\",mean_squared_error(y_test,y_pred))\n",
    "print(\"RMSE:\",np.sqrt(mean_squared_error(y_test,y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZdqvjAuic5gF"
   },
   "outputs": [],
   "source": [
    "r_squared = r2_score(y_test,y_pred)\n",
    "r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBLhmiE-dB8o"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNzZNTHshiJ3ze5reHbVmWM",
   "mount_file_id": "1CoQVZpPJI6bxFE3ABJxba40dXNDvFNKS",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
