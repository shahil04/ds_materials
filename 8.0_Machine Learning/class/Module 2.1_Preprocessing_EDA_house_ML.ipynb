{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2oU7SlOXua1"
   },
   "source": [
    "## Module 2 â€“ Data Handling & Preprocessing_house_ML model_Linear_regressions\n",
    "\n",
    "**Exploratory Data Analysis (EDA)** involves key steps like understanding the problem and data, data collection, data cleaning, descriptive statistics, data visualization, handling missing values and outliers, feature engineering, exploring variable relationships, and finally communicating insights.\n",
    "\n",
    "These steps are iterative, helping to uncover patterns, identify anomalies, and form hypotheses for building better models and making informed decisions.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yhZ6K50FGdN"
   },
   "source": [
    "  ---\n",
    "\n",
    "  ## ðŸ”¹ EDA Steps for Machine Learning\n",
    "\n",
    "  ### 1. **Understand the Problem & Dataset**\n",
    "\n",
    "  * Identify **business objective** or ML task (classification, regression, clustering, etc.)\n",
    "  * Define the Goal: Clarify the business goal or research question the data analysis aims to address.\n",
    "  *Understand Variables: Identify the meaning of each variable, their data types (numerical, categorical), and any known data quality issues or constraints.\n",
    "\n",
    "  * Load dataset (`pandas.read_csv`, SQL, etc.)\n",
    "  * Check dataset shape (`df.shape`)\n",
    "  * Display first few rows (`df.head()`)\n",
    "\n",
    "  ---\n",
    "\n",
    "  ### 2. **Data Collection**\n",
    "  \n",
    "  - Gather all relevant raw data from appropriate sources, such as databases, CSV files, or APIs.\n",
    "  ---\n",
    "\n",
    "  ### 3. **Data Preparation and Cleaning**\n",
    "  - Data Wrangling: Clean and organize the raw data into a format suitable for analysis.\n",
    "  - Handle Missing Values: Identify and decide how to handle missing or null values, which can be by removal, imputation with the mean or median, or other methods depending on the data's distribution.\n",
    "  - Handle Duplicates and Irregularities: Remove redundant data and address irregularities that can cause noise in the analysis.\n",
    "\n",
    "  ### 3.1. **Data Types & Structure**\n",
    "\n",
    "  * Inspect column types (`df.info()`)\n",
    "  * Separate features into:\n",
    "\n",
    "    * Numerical (continuous/discrete)\n",
    "    * Categorical\n",
    "    * Date/Time\n",
    "    * Text/Unstructured\n",
    "  * Identify target variable.\n",
    "\n",
    "  ---\n",
    "\n",
    "  ### 3.2 **Data Quality Check**\n",
    "\n",
    "  * Missing values (`df.isnull().sum()`)\n",
    "  * Duplicate records (`df.duplicated().sum()`)\n",
    "  * Outliers detection (boxplots, IQR method, z-score)\n",
    "  * Data entry errors (e.g., age = -5, salary = 9999999)\n",
    "\n",
    "  ---\n",
    "  ### 4. **Descriptive Statistics and Visualization**\n",
    "  * Summary Statistics: Calculate descriptive statistics like mean, median, standard deviation, and frequency distributions to get a quick understanding of the dataset's central tendency and variability.\n",
    "  * Visualize Distributions: Use histograms, box plots, and density plots to visualize the distribution of individual variables.\n",
    "  ---\n",
    "\n",
    "  ### 5. **Examine Relationships and Outliers**\n",
    "  * Bivariate and Multivariate Analysis: Explore pairwise relationships between variables using scatter plots or more complex multivariate methods like heatmaps to identify patterns and correlations.\n",
    "  * Outlier Detection: Identify and analyze outliers, which can skew results, and decide whether to remove or retain them based on the context.\n",
    "\n",
    "  ### 5.1 **Univariate Analysis (Single Variable)**\n",
    "\n",
    "  * Numerical features â†’ Histogram, KDE, Boxplot, Summary stats (`df.describe()`)\n",
    "  * Categorical features â†’ Value counts, Bar plot, Pie chart\n",
    "\n",
    "  ---\n",
    "\n",
    "  ### 5.2 **Bivariate Analysis (Feature vs Target)**\n",
    "\n",
    "  * Numerical vs Target:\n",
    "\n",
    "    * Correlation (`df.corr()` + heatmap)\n",
    "    * Scatter plots, boxplots\n",
    "  * Categorical vs Target:\n",
    "\n",
    "    * Grouped bar chart\n",
    "    * Countplot with hue\n",
    "    * Chi-square test (for independence)\n",
    "\n",
    "  ---\n",
    "\n",
    "  ### 5.3. **Multivariate Analysis**\n",
    "\n",
    "  * Correlation matrix for numerical features\n",
    "  * Pairplot/heatmap\n",
    "  * Cross-tabulations\n",
    "  * PCA/Dimensionality reduction (optional for visualization)\n",
    "\n",
    "  ---\n",
    "  ### 6. **Feature Engineering**\n",
    "  Create new features or transform existing ones to better represent the underlying information and improve model performance.\n",
    "\n",
    "  ---\n",
    "  ### 6.1 **Feature Relationships**\n",
    "\n",
    "  * Check multicollinearity (VIF score)\n",
    "  * Identify strong predictors\n",
    "  * Detect redundant features\n",
    "\n",
    "  ---\n",
    "\n",
    "  ### 6.2 **Feature Engineering Needs**\n",
    "\n",
    "  * Encoding categorical variables (OneHot, Label Encoding)\n",
    "  * Feature scaling (Normalization, Standardization)\n",
    "  * Date/time feature extraction (day, month, weekday, etc.)\n",
    "  * Domain-specific feature creation\n",
    "\n",
    "  ---\n",
    "\n",
    "  ### 6.3 **Class Imbalance Check (For Classification)**\n",
    "\n",
    "  * Value counts of target variable\n",
    "  * If imbalance exists â†’ consider SMOTE, undersampling/oversampling, class weights\n",
    "\n",
    "  ---\n",
    "  ### 7. **Communicate Findings**\n",
    "  Present the insights and discoveries from the EDA process to stakeholders.\n",
    "\n",
    "  ---\n",
    "  ### 8. **Iterate**\n",
    "  EDA is an iterative process. You may need to revisit earlier steps to test new hypotheses, address emerging challenges, and refine your understanding of the data to build more robust models\n",
    "\n",
    "  ### 10. **Key Insights & Hypothesis**\n",
    "\n",
    "  * Summarize main findings\n",
    "  * Identify which features are important\n",
    "  * Document anomalies or patterns\n",
    "\n",
    "  ---\n",
    "\n",
    "  âœ… **Outcome of EDA** â†’ Clean dataset + feature insights â†’ Ready for preprocessing & model building.\n",
    "\n",
    "  ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W482l4RlFGUj"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas numpy matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebH5D2SMbHah"
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59bMkmYAaUPW"
   },
   "outputs": [],
   "source": [
    "# - 1. DATA LOAD\n",
    "# - 2. DATA INFO\n",
    "# - 3. DATA CLEANING\n",
    "#   - a. info\n",
    "#   - b. remove unwanted columns\n",
    "#   - c. duplicate\n",
    "#   - d. null check\n",
    "#   - e. clean null values (imputation of missing value )\n",
    "#   - f. save the clean data\n",
    "\n",
    "# - 4. DATA PREPROCESSING\n",
    "    # - a. segregate categorical and numerical columns\n",
    "    # - b. check the data pattern using graphs and show distributions\n",
    "    # - c. data interpolation >> is a process of estimating data in a range, getting unknown values from known values\n",
    "    # - d. data Feature Scalling\n",
    "    # - e. To check outliers >> distplot, boxplot\n",
    "    # - f. data encoding\n",
    "    # - g. class imbalance >> one class has higeher percentage upsampling, downsampling, smote\n",
    "    # - h. Assumptions of Machine Learning for Linear Regression\n",
    "# - MACHINE LEARNING MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WG0khOuxIQ8y"
   },
   "source": [
    "# ðŸ”¹ EDA Steps for Machine Learning\n",
    "\n",
    "### 1. **Data Load**\n",
    "\n",
    "* Import libraries\n",
    "* Load dataset (CSV, SQL, API, etc.)\n",
    "* Preview dataset (`head`, `shape`)\n",
    "---\n",
    "\n",
    "âœ… This outline now has both:\n",
    "\n",
    "* Your **step-by-step workflow (load â†’ clean â†’ preprocess â†’ model assumptions)**\n",
    "* The **structured EDA stages (univariate, bivariate, multivariate, feature engineering, communication)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pM3YS8i40u6Q"
   },
   "source": [
    "## 1. Data Loading and Preprocessing.\n",
    "\n",
    "In these steps, we will load the Bengaluru House Data dataset using pandas and perform an initial exploration to understand its structure and contents.\n",
    "\n",
    "- **Data Import:** The dataset is loaded into a pandas DataFrame named `data`.\n",
    "- **Shape:** The dataset contains 12,530 rows and 7 columns after initial cleaning.\n",
    "- **Columns:**  \n",
    "    - `location`: Area or locality of the property  \n",
    "    - `size`: Number of bedrooms (e.g., \"2 BHK\", \"4 Bedroom\")  \n",
    "    - `total_sqft`: Total area in square feet  \n",
    "    - `bath`: Number of bathrooms  \n",
    "    - `price`: Price of the property (in lakhs)  \n",
    "    - `bhk`: Extracted number of bedrooms as integer  \n",
    "    - `price_per_sqft`: Price per square foot\n",
    "\n",
    "We will also check for missing values, data types, and unique values in key columns to guide further cleaning and preprocessing steps. This foundational understanding helps in identifying potential issues such as outliers, inconsistent data, and the need for encoding categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V58RteqJIjXe"
   },
   "source": [
    "---\n",
    "\n",
    "### 2. **Data Info**\n",
    "\n",
    "* Check dataset structure (`info`)\n",
    "* Identify column data types (numerical, categorical, datetime, text)\n",
    "* Identify target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ylCgj4ZJBQP"
   },
   "source": [
    "### 4. **Data Preprocessing**\n",
    "\n",
    "* Segregate categorical & numerical columns\n",
    "* Explore data distribution (graphs, histograms, boxplots, distplots)\n",
    "* Data interpolation (estimate unknown values from known values)\n",
    "* Handle class imbalance (Upsampling, Downsampling, SMOTE, Class weights)\n",
    "* Feature scaling (Normalization, Standardization)\n",
    "* Outlier detection & treatment (boxplot, z-score, IQR method)\n",
    "* Data encoding (Label Encoding, OneHot Encoding)\n",
    "\n",
    "* find the correlations bw variables\n",
    "* Check assumptions of ML models (e.g., Linear Regression â€“ linearity, normality, multicollinearity, homoscedasticity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Cleaned_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXNPaOEbiqvz"
   },
   "source": [
    "## Perform EDA for analysis data or understand the data nature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htQC9rwOJZyB"
   },
   "source": [
    "### 2. **Descriptive Statistics & Visualization**\n",
    "ðŸ”¹ STEP 2: Statistical Summary\n",
    "\n",
    "* Summary statistics (`describe`)\n",
    "* Numerical feature distribution (histogram, KDE, boxplot)\n",
    "* Categorical feature distribution (countplot, bar chart, pie chart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q2I1-4CyOQxt"
   },
   "outputs": [],
   "source": [
    "# describe use for Summarize statistics for numerical features\n",
    "print(\"Descriptive Statistics for Numerical Features:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1BG2VoGOWTl"
   },
   "outputs": [],
   "source": [
    "df.describe(include=\"O\")\n",
    "# for oject datatype columns    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YlU5r-hLPAue"
   },
   "outputs": [],
   "source": [
    "df.describe(include=\"all\")\n",
    "# both numerical and categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ”¹ STEP 3: Missing Values Analysis already done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgGZ36cUOakQ"
   },
   "source": [
    "#### CHECK numerical columns and categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s59lclmAOj6d"
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LImnrQYiPiuy"
   },
   "outputs": [],
   "source": [
    "df.columns\n",
    "# show the ALL columns name as list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DqN9WWaPqrn"
   },
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "  if df[i].dtype != 'object':\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols =[]\n",
    "catgorical_cols =[]\n",
    "for i in df.columns:\n",
    "    if df[i].dtype == 'object':\n",
    "        # print(i)\n",
    "        catgorical_cols.append(i)\n",
    "    else:\n",
    "        numerical_cols.append(i) \n",
    "\n",
    "print(\"Numerical Columns:\", numerical_cols)\n",
    "print(\"Categorical Columns:\", catgorical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”¹ STEP 4: Understand Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”¹ STEP 5: Relationship Analysis **Examine Relationships**\n",
    "\n",
    "#### 6.1 **Univariate Analysis**\n",
    "\n",
    "* Individual variable distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [3,4,2,3,1,2,3,4,5,6,5,2,2]\n",
    "plt.hist(d, bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install matplotlib seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(df['bhk'], bins=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the  count of  eack bhk\n",
    "df['bhk'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtISL-fGPanB"
   },
   "outputs": [],
   "source": [
    "# Visualize distributions of numerical features\n",
    "plt.figure(figsize=(8, 4))\n",
    "print(\"Visualizing Distributions of Numerical Features:\")\n",
    "for col in numerical_cols: \n",
    "    sns.histplot(df[col], kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency/count')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['location'].value_counts().index[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3z2Cj0XcUSE"
   },
   "outputs": [],
   "source": [
    "# Visualize distributions of categorical features\n",
    "print(\"\\nVisualizing Distributions of Categorical Features:\")\n",
    "for col in catgorical_cols:\n",
    "    sns.countplot(data=df, y=col, order=df[col].value_counts().index[:10]) # Display top 10 for readability\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8z9HYNFmJiMh"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "#### 6.2 **Bivariate Analysis**\n",
    "\n",
    "* Numerical vs Target (correlation, scatterplot, boxplot)\n",
    "* Categorical vs Target (grouped bar chart, countplot, chi-square test)\n",
    "\n",
    "#### 6.3 **Multivariate Analysis**\n",
    "\n",
    "* Correlation matrix & heatmap\n",
    "* Pairplots\n",
    "* Cross-tabulations\n",
    "* PCA / dimensionality reduction (optional for visualization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Price vs Sqft\n",
    "# 2.Price vs BHK\n",
    "# 3.Correlation\n",
    "\n",
    "# sns.scatterplot(x='total_sqft', y='price', data=df)\n",
    "# sns.boxplot(x='bhk', y='price', data=df)\n",
    "# df.corr(numeric_only=True)\n",
    "\n",
    "# Expect:\n",
    "# Strong correlation: total_sqft, bath, bhk\n",
    "# Weak: balcony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aS6eCpQAcV04"
   },
   "outputs": [],
   "source": [
    "# Bivariate Analysis (Numerical vs Target)\n",
    "print(\"\\nBivariate Analysis (Numerical vs Target):\")\n",
    "plt.figure(figsize=(8, 4))\n",
    "for col in numerical_cols:\n",
    "    sns.scatterplot(data=df, x=col, y='price')\n",
    "    plt.title(f'Scatter plot of {col} vs Price')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Price')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numerical_cols].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate Analysis (Correlation Matrix and Heatmap)\n",
    "print(\"\\nMultivariate Analysis (Correlation Matrix):\")\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhNl2vX_JuPY"
   },
   "source": [
    "### 7. **Feature Engineering**\n",
    "\n",
    "* Create new features\n",
    "* Extract date/time features (day, month, weekday, etc.)\n",
    "* Domain-specific feature creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 550 sqft 20 lakh\n",
    "# what is the 1 sqrt feet price on above data ?\n",
    "round(20*100000/550,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T7dOAroxcWyX"
   },
   "outputs": [],
   "source": [
    "# Feature Engineering: Create 'price per square foot'\n",
    "df['price_per_sqft'] = df['price'] * 100000 / df['total_sqft']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTVK140LJ78I"
   },
   "source": [
    "### 9. **Communicate Findings**\n",
    "\n",
    "* Present visualizations & reports\n",
    "* Share insights with stakeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OF9sXTQJcYrB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eb077055"
   },
   "source": [
    "### Summary of Key EDA Findings\n",
    "\n",
    "Based on the Exploratory Data Analysis performed:\n",
    "\n",
    "* **Data Overview:** We loaded and inspected the Bengaluru House Data, understanding its shape, columns, and initial data types.\n",
    "* **Data Cleaning:**\n",
    "    * Missing values were handled for `location`, `size`, `total_sqft`, `bath`, and `balcony` by imputation with the mode or mean.\n",
    "    * Irrelevant columns (`availability`, `society`) were dropped.\n",
    "    * Data types were corrected, specifically converting `size` to `bhk` (integer) and `total_sqft` to float, handling ranges.\n",
    "    * Outliers were addressed in `total_sqft`, `bath`, `price`, and `bhk`.\n",
    "* **Distribution Analysis:** Visualizations (histograms, countplots) provided insights into the distribution of individual features. We observed the distribution of numerical features and the counts of different categories in `area_type` and `location`.\n",
    "* **Relationship Analysis:**\n",
    "    * Scatter plots showed relationships between numerical features and `price`.\n",
    "    * A correlation matrix highlighted the linear relationships between numerical variables. `total_sqft`, `bath`, and `bhk` show positive correlations with `price`.\n",
    "* **Feature Engineering:** A new feature, `price_per_sqft`, was created, which can be a significant predictor of house prices.\n",
    "* **Skewness Handling:** The `price` column's skewness was addressed using a Box-Cox transformation, resulting in a more normally distributed variable (`price_boxcox`).\n",
    "\n",
    "These findings provide a solid foundation for the next steps in data preprocessing and machine learning model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SzCJ_K229om0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eub8gTy6KB98"
   },
   "source": [
    "### 10. **Iterate & Refine**\n",
    "\n",
    "* Revisit earlier steps if needed\n",
    "* Test new hypotheses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyhbVY5h9rHV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NvJCllHJoSP"
   },
   "source": [
    "### 11. **Key Insights & Hypotheses** (OPTIONAL)\n",
    "\n",
    "* Summarize final findings\n",
    "* Highlight important features\n",
    "* Document anomalies & patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- code in comment for Hypothesis\n",
    "<!-- import scipy.stats as stats\n",
    "\n",
    "# Perform ANOVA test to see if there is a significant difference in price between different area types\n",
    "# First, we need to separate the prices by area type\n",
    "area_types = df['area_type'].unique()\n",
    "price_by_area_type = [df['price'][df['area_type'] == area_type] for area_type in area_types]\n",
    "\n",
    "# Perform one-way ANOVA test\n",
    "f_statistic, p_value = stats.f_oneway(*price_by_area_type)\n",
    "\n",
    "print(f\"ANOVA F-statistic: {f_statistic:.2f}\")\n",
    "print(f\"ANOVA P-value: {p_value:.4f}\")\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"\\nConclusion: Reject the null hypothesis. There is a significant difference in price between different area types.\")\n",
    "else:\n",
    "    print(\"\\nConclusion: Fail to reject the null hypothesis. There is no significant difference in price between different area types.\")\n",
    "\n",
    "\n",
    " ------\n",
    "\n",
    "\n",
    " from scipy.stats import chi2_contingency\n",
    "\n",
    "# Create a contingency table of 'area_type' and 'balcony'\n",
    "contingency_table = pd.crosstab(df['area_type'], df['balcony'])\n",
    "\n",
    "# Perform the chi-square test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(f\"Chi-square statistic: {chi2:.2f}\")\n",
    "print(f\"P-value: {p:.4f}\")\n",
    "print(f\"Degrees of freedom: {dof}\")\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "if p < alpha:\n",
    "    print(\"\\nConclusion: Reject the null hypothesis. There is a significant association between area type and number of balconies.\")\n",
    "else:\n",
    "    print(\"\\nConclusion: Fail to reject the null hypothesis. There is no significant association between area type and number of balconies.\") -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtbHKe-I6qNE"
   },
   "source": [
    "## Prepare the data for  machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6I9u2gKecbaI"
   },
   "outputs": [],
   "source": [
    "# Already done \n",
    "# EDA-Info,shape\n",
    "# Handling Missing Values\n",
    "# Feature Engineering\n",
    "# ========\n",
    "# 1. Handling Outliers\n",
    "# 2. Handling Skewness\n",
    "# 3. Data Encoding\n",
    "# 4. Feature Scaling -Normalization and Standardization\n",
    "\n",
    "# final --> model training \n",
    "    # 1. data split\n",
    "    # 2. model fit\n",
    "    # 3. model evauate/Accuracy check\n",
    "    # 4. predictions\n",
    "    # 5. Fine Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bgoKwTRpcQH4"
   },
   "outputs": [],
   "source": [
    "# load the clean data\n",
    "df = pd.read_csv(\"Cleaned_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fiux84C4eZdc"
   },
   "source": [
    "### 1.Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwhyKyjzem1R"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gP44F7f7dLuS"
   },
   "outputs": [],
   "source": [
    "# 1.1 check outliers\n",
    "# df['total_sqft']\n",
    "plt.boxplot(df['total_sqft'])\n",
    "plt.show()\n",
    "\n",
    "sns.boxenplot(df['total_sqft'])\n",
    "plt.show()\n",
    "sns.violinplot(df['total_sqft'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fg9VAwOoeITH"
   },
   "outputs": [],
   "source": [
    "# Q. show the details of sales above 30k sqrtfeet\n",
    "df[df[\"total_sqft\"]>30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZlsUQrVDgRgL"
   },
   "outputs": [],
   "source": [
    "df = df[df[\"total_sqft\"]<30000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Vl1vZHPgZIr"
   },
   "outputs": [],
   "source": [
    "df[df[\"total_sqft\"]>30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcEpl23vgh0w"
   },
   "outputs": [],
   "source": [
    "# bath\tbalcony\tprice\tbhk\n",
    "l = ['bath',\t'balcony',\t'price',\t'bhk']\n",
    "for i in l:\n",
    "  print(i)\n",
    "  sns.boxenplot(df[i])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JX9pV-obldBO"
   },
   "outputs": [],
   "source": [
    "# bath 20,pric 3000, bhk 20 conditions for outliers\n",
    "df = df[df[\"bath\"]<20]\n",
    "df = df[df[\"price\"]<3000]\n",
    "df = df[df[\"bhk\"]<20]\n",
    "\n",
    "# df[(df[\"bath\"]>20) ! (df[\"price\"]>30000) ! (df[\"bhk\"]>20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KEvb_hw0mK_Y"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4m3Lm4smr3y"
   },
   "outputs": [],
   "source": [
    "# check outliers for categorical values\n",
    "df['area_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7QxeCnnp46R"
   },
   "outputs": [],
   "source": [
    "df['location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BcBO1XTTm7cN"
   },
   "outputs": [],
   "source": [
    "(df['location'].value_counts().reset_index()[\"count\"]==1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_CVNwwqpKQu"
   },
   "source": [
    "### 2.Handling Skewness\n",
    "\n",
    "Skewness is a statistical measure that describes the asymmetry of a probability distribution, indicating whether the data is stretched more toward the left or right of the mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PtPL7cGkqMki"
   },
   "outputs": [],
   "source": [
    "# check the distributions\n",
    "# histogram only for numerical columns -->specialy from continous data\n",
    "for i in df.columns:\n",
    "  if df[i].dtype != 'object':\n",
    "    lab = i+\" Skewness: \"+str(df[i].skew())\n",
    "    # print(lab)\n",
    "    # print(df[i].skew())\n",
    "    sns.histplot(df[i],kde=True)\n",
    "\n",
    "    plt.legend([lab])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFEk7gCDsyqz"
   },
   "outputs": [],
   "source": [
    "# skewness handle\n",
    "df['price'].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5doue9Hvhl_"
   },
   "outputs": [],
   "source": [
    "np.sqrt(df['price']).skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ImW7HZMHv6Ph"
   },
   "outputs": [],
   "source": [
    "np.square(df['price']).skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvikxJEOv-ru"
   },
   "outputs": [],
   "source": [
    "np.log(df['price']).skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### # skewness handle use Transformations methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. square\n",
    "# 2. sqrt\n",
    "# 3. log\n",
    "# 4. box cox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66EO3Uczvp9P"
   },
   "outputs": [],
   "source": [
    "sns.histplot(np.log(df['price']),kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BxqMPjNvxJ-K"
   },
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/skewness-be-gone-transformative-tricks-for-data-scientists/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PvpkgsVxOYX"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "\n",
    "# Apply Box-Cox transformation to the 'price' column\n",
    "df['price_boxcox'], lambda_price = boxcox(df['price'])\n",
    "\n",
    "# Display the skewness of the original and transformed price\n",
    "print(f\"Original Price Skewness: {df['price'].skew()}\")\n",
    "print(f\"Box-Cox Transformed Price Skewness: {df['price_boxcox'].skew()}\")\n",
    "\n",
    "# Visualize the distribution of the transformed price\n",
    "sns.histplot(df['price_boxcox'], kde=True)\n",
    "plt.title('Distribution of Box-Cox Transformed Price')\n",
    "plt.xlabel('Box-Cox Transformed Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18l2_igLxO6d"
   },
   "source": [
    "# Feature Scaling -Normalization and Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKw_H4jqvTI7"
   },
   "source": [
    "## Scaling\n",
    "- change the range of value in fixed range\n",
    "- Normalization (min-max scaler) -->range (0-1)/(-1 to 1)\n",
    "- Standarization (z-score) --> mean=0, std=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1Lip59Koun8"
   },
   "outputs": [],
   "source": [
    "# x-xmin/xmax-xmin\n",
    "li = [1,2,3,4]\n",
    "for x in li:\n",
    "  new_x = (x-min(li))/(max(li)-min(li))\n",
    "\n",
    "  print(new_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2i5GnxPDvbHx"
   },
   "outputs": [],
   "source": [
    "# !pip install scikit-learn\n",
    "# scaling\n",
    "from sklearn.preprocessing import  MinMaxScaler,StandardScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOdNcBNaw3wW"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMuekkdurIas"
   },
   "outputs": [],
   "source": [
    "# obj = classname()\n",
    "# obj.methods(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UBiCNGomvdRc"
   },
   "outputs": [],
   "source": [
    "min_max = MinMaxScaler()\n",
    "# change the range of total_sqft into 0 to 1\n",
    "min_max.fit_transform(df[[\"total_sqft\"]])\n",
    "\n",
    "scaled_data = min_max.fit_transform(df[[\"total_sqft\"]])\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtEAb_cHyFKq"
   },
   "outputs": [],
   "source": [
    "# reassign\n",
    "df[\"total_sqft\"] = scaled_data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tf5bbzJnzyt"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# change the range of where mean=0 and std =1\n",
    "scaled_data = scaler.fit_transform(df[[\"total_sqft\"]])\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPGeNucnxCOq"
   },
   "outputs": [],
   "source": [
    "# robust\n",
    "scaler = RobustScaler()\n",
    "# Why use robust scaling\n",
    "# Handles outliers: It is robust to the presence of outliers, which can distort other scaling methods that rely on the mean and standard deviation.\n",
    "# Less sensitive to skewed data: It is suitable for datasets with non-normal distributions, which are common in real-world data.\n",
    "# Improves model performance:\n",
    "# X_scaled=(X-X_median)/IQR)\n",
    "scaled_data = scaler.fit_transform(df[[\"total_sqft\"]])\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzDWPAvTyoFK"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtP-SlJScGYJ"
   },
   "source": [
    "## Encoding\n",
    "\n",
    "Encoding in machine learning is the process of converting categorical data(like text or non-numeric labels) into a numerical format that machine learning algorithms can understand and process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dcD5BWIZkCwZ"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqCZYOTz092-"
   },
   "source": [
    "## encoding\n",
    "* types\n",
    "  1. nominal/Label encoding --> only name in data (ranking is not important)\n",
    "  2. ordinal encodnig --> (ranking is important)\n",
    "  3. one-hot encoding/ df_dummies -->( all are equal)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T5K1MDIV7NsZ"
   },
   "outputs": [],
   "source": [
    "df[\"area_type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M5i6HQDg7WjB"
   },
   "outputs": [],
   "source": [
    "df[\"area_type\"].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3sirdL7V0x4J"
   },
   "outputs": [],
   "source": [
    "# sk-learn\n",
    "# preprocessing\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder , OneHotEncoder\n",
    "\n",
    "label= LabelEncoder()\n",
    "\n",
    "l = label.fit_transform(df[\"area_type\"])\n",
    "\n",
    "print(l[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-J-ek4GNmh-8"
   },
   "outputs": [],
   "source": [
    "label.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GDVorarG9ZzM"
   },
   "outputs": [],
   "source": [
    "df[\"area_type\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVPz2q7WrE-d"
   },
   "outputs": [],
   "source": [
    "df[\"area_type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9q9QM3w6Ao53"
   },
   "outputs": [],
   "source": [
    "# Ordinal for apply the order in distinct values\n",
    "\n",
    "ord = OrdinalEncoder(categories=[[\"Super built-up  Area\",\"Built-up  Area\",\"Plot  Area\",\"Carpet  Area\"]])\n",
    "\n",
    "o = ord.fit_transform(df[[\"area_type\"]])\n",
    "print(o[:50])\n",
    "\n",
    "# o = ord.fit_transform(df[\"area_type\"].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOtmTx8q99bW"
   },
   "outputs": [],
   "source": [
    "ord.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dokqmR8xA4_W"
   },
   "outputs": [],
   "source": [
    "# show the data with actual data\n",
    "for i in zip(df[\"area_type\"],o):\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9fw9ukxBMIa"
   },
   "outputs": [],
   "source": [
    "# Binary number --> 0,1 if value then 1 and rest of then equal 0\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "o =ohe.fit_transform(df[[\"area_type\"]]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_3L1fl0n-pk"
   },
   "outputs": [],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z32v26qPBnuF"
   },
   "outputs": [],
   "source": [
    "ohe.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BYbsC0zdBt1N"
   },
   "outputs": [],
   "source": [
    "area_ohe = pd.DataFrame(o,columns=ohe.get_feature_names_out())\n",
    "area_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "db-kZ4gMCD7A"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oAFDURvNBuo2"
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"data_preprocess_for_ml.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ku70aJo2CQ6z"
   },
   "outputs": [],
   "source": [
    "# add data into existing df and area_ohe into df1\n",
    "df1 = pd.concat([df,area_ohe],axis=1,)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxaE_6bI3xrm"
   },
   "outputs": [],
   "source": [
    "# same do with locations -->for OHE encoding\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "l =ohe.fit_transform(df[[\"location\"]]).toarray()\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ArTHZyh4eXG"
   },
   "outputs": [],
   "source": [
    "location = pd.DataFrame(l,columns=ohe.get_feature_names_out())\n",
    "location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7f6FPIeuIlK"
   },
   "outputs": [],
   "source": [
    "df1 = pd.concat([df1,location],axis=1)  # also apend location ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Zk4Z9CpuLqw"
   },
   "outputs": [],
   "source": [
    "df1.drop(columns=[\"area_type\",\"location\"],inplace=True) # drop the string/object value columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E5bfA1SI5B4-"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kIPtqZqx47bI"
   },
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUH-ifZZ7A9Y"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asDWdSbc7B_1"
   },
   "source": [
    "### **Feature Relationships**\n",
    "\n",
    "* Check multicollinearity (VIF)\n",
    "* Identify strong predictors\n",
    "* Remove redundant features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KvSwVfQq_Y2b"
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Separate features (X) from the target variable (y)\n",
    "# Exclude the original 'price' and the Box-Cox transformed 'price_boxcox' if they exist in df\n",
    "numerical_cols_original = df.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'price' in numerical_cols_original:\n",
    "    numerical_cols_original.remove('price')\n",
    "if 'price_boxcox' in numerical_cols_original:\n",
    "     numerical_cols_original.remove('price_boxcox') # Exclude transformed price\n",
    "\n",
    "# Create a DataFrame with just the numerical features from the original df\n",
    "numerical_features_df = df[numerical_cols_original]\n",
    "\n",
    "# Handle potential infinite VIF if there's perfect multicollinearity (e.g., constant column)\n",
    "# Add a small constant to avoid division by zero if a column has zero variance\n",
    "numerical_features_df = numerical_features_df + 1e-9\n",
    "\n",
    "# Calculate VIF for each numerical feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = numerical_features_df.columns\n",
    "# Ensure the data type is float for variance_inflation_factor\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(numerical_features_df.values.astype(float), i)\n",
    "                   for i in range(numerical_features_df.shape[1])]\n",
    "\n",
    "print(\"Variance Inflation Factor (VIF) for Numerical Features:\")\n",
    "display(vif_data.sort_values(by=\"VIF\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZCQDZCmunxA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_9lqXJd__y23"
   },
   "outputs": [],
   "source": [
    "# Identify features with high VIF to remove\n",
    "# Based on the calculated VIF values, both 'bath' (VIF: 26.96) and 'bhk' (VIF: 26.31) have high multicollinearity. This is likely because the number of bathrooms and the number of bedrooms are often strongly related to each other and possibly to the total square footage of the property.\n",
    "\n",
    "# Having high multicollinearity between features can affect the interpretation and stability of some linear models. You might consider:\n",
    "\n",
    "# Removing one of the highly correlated features: For example, you could remove either 'bath' or 'bhk'.\n",
    "# Using techniques less sensitive to multicollinearity: Models like Ridge or Lasso regression include regularization that can handle multicollinearity.\n",
    "# Combining features: Create a new feature that represents the relationship between 'bath' and 'bhk'.\n",
    "# Would you like me to help you explore any of these options, or would you prefer to proceed with the current features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df3a444b"
   },
   "source": [
    "## Correlations\n",
    "\n",
    "Based on the correlation matrix:\n",
    "\n",
    "*   **Positive Correlations with Price:** `total_sqft`, `bath`, and `bhk` show positive correlations with the target variable `price`. This is expected, as larger properties with more bathrooms and bedrooms typically have higher prices.\n",
    "*   **Strong Correlation between Bath and BHK:** There is a strong positive correlation between `bath` and `bhk`, which was also highlighted by the VIF analysis.\n",
    "*   **Price per Square Foot:** The engineered feature `price_per_sqft` also shows a positive correlation with price, although its relationship with the original numerical features might need further investigation.\n",
    "*   **Other Correlations:** Review the heatmap in the previous output for specific correlation values between all numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKXIbP9w7EjN"
   },
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X7uVRgZa0SAj"
   },
   "outputs": [],
   "source": [
    "# create a bi-variant using price and total_sqft\n",
    "plt.scatter(df[\"price\"],df[\"total_sqft\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPff814c7AsQ"
   },
   "outputs": [],
   "source": [
    "# Multivariate Analysis (Correlation Matrix and Heatmap)\n",
    "print(\"\\nMultivariate Analysis (Correlation Matrix):\")\n",
    "numerical_df = df.select_dtypes(include=np.number)\n",
    "correlation_matrix = numerical_df.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx3eqEI-yiWR"
   },
   "outputs": [],
   "source": [
    "# prepared_data for ml save\n",
    "df.to_csv(\"prepared_ml_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TajX2Hqez_8c"
   },
   "source": [
    "## Model Create |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_Ds6AgY0_ze"
   },
   "outputs": [],
   "source": [
    "# import\n",
    "df = pd.read_csv(r\"/content/Cleaned_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9X0bVkRH0_mE"
   },
   "outputs": [],
   "source": [
    "## Model Building\n",
    "# 1. select the numerical columns\n",
    "# 2. seprate the x and y\n",
    "# 2.1 split the data into training testing\n",
    "# 3. load the algorithms and create the object of algoritms\n",
    "# 4. train the model --> find the pattern value from data\n",
    "# 5. test then model/predict\n",
    "# 6. Evaluate/accuracy the model\n",
    "\n",
    "# 7. fine tuning the model(for improve accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pliU0tmx6fRI"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "szW6yUtu5LC9"
   },
   "outputs": [],
   "source": [
    "# 1. select the numerical columns\n",
    "df = df.drop(columns=[\"area_type\",\"location\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7tYcHMqy6jez"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mza8pfWh6p67"
   },
   "outputs": [],
   "source": [
    "# 2. seprate the x and y\n",
    "X = df.drop(columns=[\"price\"])\n",
    "y = df[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uVU4n1nY6vwr"
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7OfCipa7cEQ"
   },
   "outputs": [],
   "source": [
    "# 2. split the data into training testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state =0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZOMyIu-895yk"
   },
   "outputs": [],
   "source": [
    "# 3. load the algorithms and create the object of algoritms\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gg-A51KI_WcJ"
   },
   "outputs": [],
   "source": [
    "# 4. train the model --> find the pattern value from data\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jSs-McG_cQy"
   },
   "outputs": [],
   "source": [
    "# coffecient, intercept\n",
    "print(lr.coef_)\n",
    "print(lr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ya80s3z6CG6b"
   },
   "outputs": [],
   "source": [
    "# check the trainning accuracy\n",
    "lr.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4b3lrANCXmo"
   },
   "outputs": [],
   "source": [
    "# check the testing accuracy\n",
    "lr.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pd66nb5qCweH"
   },
   "outputs": [],
   "source": [
    "# 5. test then model/predict\n",
    "y_pred = lr.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GFsTCpngDFoC"
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZWRIDBvElPT"
   },
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nBQP01ySEXxZ"
   },
   "outputs": [],
   "source": [
    "# 1000,2,1,2 -->price\n",
    "sqrt = int(input(\"Enter total_sqft: \"))\n",
    "bath = int(input(\"Enter number of bathrooms: \"))\n",
    "bhk = int(input(\"Enter number of bedrooms (BHK): \"))\n",
    "balcony = int(input(\"Enter number of balcony: \"))\n",
    "\n",
    "lr.predict([[sqrt,bath,bhk,balcony]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XJ2nxl3zFRU3"
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "import pickle\n",
    "pickle.dump(lr,open(\"lrmodel.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n8CETg7TFmH1"
   },
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = pickle.load(open(\"lrmodel.pkl\",\"rb\"))\n",
    "sqrt = int(input(\"Enter total_sqft: \"))\n",
    "bath = int(input(\"Enter number of bathrooms: \"))\n",
    "bhk = int(input(\"Enter number of bedrooms (BHK): \"))\n",
    "balcony = int(input(\"Enter number of balcony: \"))\n",
    "\n",
    "model.predict([[sqrt,bath,bhk,balcony]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wSCVYl-5_RrB"
   },
   "outputs": [],
   "source": [
    "# 6. Evaluate/accuracy the model\n",
    "# 7. fine tuning the model(for improve accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FOsFpqKLAvOU"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqgnOesZSNVg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnnmdeSzcUVI"
   },
   "source": [
    "==================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JTDbuUB2JlqC"
   },
   "outputs": [],
   "source": [
    "lr =LinearRegression()\n",
    "# Train the model\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRUp6cYr9Xfc"
   },
   "outputs": [],
   "source": [
    "# test\n",
    "y_pred = lr.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fx2R5-RA2UW8"
   },
   "outputs": [],
   "source": [
    "# errors CALCULATE\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "print(\"MAE:\",mean_absolute_error(y_test,y_pred))\n",
    "print(\"MSE:\",mean_squared_error(y_test,y_pred))\n",
    "print(\"RMSE:\",np.sqrt(mean_squared_error(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9wmFtzr3R08"
   },
   "outputs": [],
   "source": [
    "# R2score\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKca6ZXRv3We"
   },
   "source": [
    "# 2nd way for same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VFHWVc9-5T_Q"
   },
   "outputs": [],
   "source": [
    "# After clean data\n",
    "df = pd.read_csv(\"/content/Cleaned_data.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2J15eJSC-EgO"
   },
   "outputs": [],
   "source": [
    "# 2nd way\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TpVi_Ue18-IP"
   },
   "outputs": [],
   "source": [
    "# task 1 transform applying\n",
    "# columns transform\n",
    "columns_trans = ColumnTransformer(\n",
    "    [('onehot_location', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), ['location']),\n",
    "     ('onehot_area_type', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), [\"area_type\"]),\n",
    "     ('scaler', StandardScaler(), [\"total_sqft\", \"bath\"]),\n",
    "\n",
    "     ],\n",
    "    remainder='passthrough')\n",
    "# task 2 model apply\n",
    "# model\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2c64Dves_-SU"
   },
   "outputs": [],
   "source": [
    "#pipeline\n",
    "pipe = make_pipeline(columns_trans,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r3rECBuFBj5J"
   },
   "outputs": [],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "woq0GdvEAhKy"
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=[\"price\"])\n",
    "y = df[\"price\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2cbJ9nsAjMb"
   },
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_W-HAxETd2w"
   },
   "outputs": [],
   "source": [
    "pipe.score(X_train,y_train)\n",
    "# trainig accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZ1MjCMZBBU8"
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = pipe.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tM2OcG-Tugu"
   },
   "outputs": [],
   "source": [
    "# R2 score\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2TAkC1MgsqX"
   },
   "outputs": [],
   "source": [
    "# save mode\n",
    "import pickle\n",
    "pickle.dump(pipe,open(\"model2.pkl\",\"wb\"))\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJP3jvrCSk3W"
   },
   "source": [
    "# Performance Matrix\n",
    "Measuring Performance metrics-Lost and Cost Function (MAE,MSE,RMSE,R2 Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ByS9DK4gSr3o"
   },
   "outputs": [],
   "source": [
    "# cost functions --> calculate erros\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error,root_mean_squared_error\n",
    "\n",
    "print(\"MAE:\",mean_absolute_error(y_test,y_pred))\n",
    "print(\"MSE:\",mean_squared_error(y_test,y_pred))\n",
    "print(\"RMSE:\",root_mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IxvrGk-ch3w"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIuPUx3yciUt"
   },
   "source": [
    "# Regression Performance check using r2_score ,and Adjusted r2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HyvSJ2l-DvJs"
   },
   "outputs": [],
   "source": [
    "# Regression Performance check using r2_score ,and Adjusted r2 score\n",
    "from sklearn.metrics import r2_score\n",
    "r_squared = r2_score(y_test,y_pred)\n",
    "r_squared\n",
    "\n",
    "# https://benjaminobi.medium.com/what-really-is-r2-score-in-linear-regression-20cafdf5b87c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m23aGQaIWF-B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wddRRO1qV4s4"
   },
   "outputs": [],
   "source": [
    "n_samples = df.shape[0]\n",
    "n_features = df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_BuJfU_xE_0i"
   },
   "outputs": [],
   "source": [
    "adjusted_r2 = 1 - (1 - r_squared) * (n_samples - 1) / (n_samples - n_features - 1)\n",
    "adjusted_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRn9iqUyEvFD"
   },
   "outputs": [],
   "source": [
    "# save the movel for future use\n",
    "import pickle\n",
    "pickle.dump(pipe,open(\"model.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POWgAszs9dBI"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6H_C4tdsgErK"
   },
   "outputs": [],
   "source": [
    "locations = df[\"location\"].unique()\n",
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kNIXeqRHCG85"
   },
   "outputs": [],
   "source": [
    "locations = df[\"location\"].unique()\n",
    "locations\n",
    "# save the data for future use\n",
    "pickle.dump(locations, open(\"locations.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BHE4azOvCo7d"
   },
   "outputs": [],
   "source": [
    "# carpet area\n",
    "carpet_area = df[\"area_type\"].unique()\n",
    "carpet_area\n",
    "# save the data for future use\n",
    "pickle.dump(carpet_area, open(\"area_type.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKklO-x2E2ZX"
   },
   "outputs": [],
   "source": [
    "# load the model\n",
    "pipe = pickle.load(open(\"model.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IheFab96AxiP"
   },
   "outputs": [],
   "source": [
    "# load the model\n",
    "pipe = pickle.load(open(\"model.pkl\",\"rb\"))\n",
    "\n",
    "# take input from user\n",
    "location = input(\"Enter location: \")\n",
    "area_type = input(\"Enter area type: \")\n",
    "total_sqft = float(input(\"Enter total square feet: \"))\n",
    "bath = float(input(\"Enter number of bathrooms: \"))\n",
    "bhk = int(input(\"Enter number of bedrooms (BHK): \"))\n",
    "balcony = int(input(\"Enter number of balcony: \"))\n",
    "\n",
    "# Create a DataFrame from user input\n",
    "user_input = pd.DataFrame([[location, area_type, total_sqft, bath, bhk,balcony]],\n",
    "                          columns=['location', 'area_type', 'total_sqft', 'bath', 'bhk',\"balcony\"])\n",
    "\n",
    "# Predict the price\n",
    "predicted_price = pipe.predict(user_input)\n",
    "\n",
    "print(f\"The predicted price is: {predicted_price[0]:.2f} Lakhs\")\n",
    "# Electronic City Phase II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8tC-D1iUgupd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84iGvD3PbfTI"
   },
   "outputs": [],
   "source": [
    "# Save the Streamlit app code to app.py\n",
    "# with Dropdown meanu use\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# ----------------------\n",
    "# Load Model & Dropdown Data\n",
    "# ----------------------\n",
    "pipe = pickle.load(open(\"model.pkl\", \"rb\"))\n",
    "locations = pickle.load(open(\"locations.pkl\", \"rb\"))\n",
    "area_types = pickle.load(open(\"area_type.pkl\", \"rb\"))\n",
    "\n",
    "# ----------------------\n",
    "# Streamlit App\n",
    "# ----------------------\n",
    "st.set_page_config(page_title=\"House Price Prediction\", layout=\"centered\")\n",
    "st.title(\"ðŸ  House Price Prediction App\")\n",
    "st.write(\"Enter property details below to get an estimated price (in Lakhs).\")\n",
    "\n",
    "# User inputs\n",
    "location = st.selectbox(\"Select Location\", locations)\n",
    "area_type = st.selectbox(\"Select Area Type\", area_types)\n",
    "total_sqft = st.number_input(\"Enter Total Square Feet\", min_value=100.0, step=10.0)\n",
    "bath = st.number_input(\"Enter Number of Bathrooms\", min_value=1.0, step=1.0)\n",
    "bhk = st.number_input(\"Enter Number of Bedrooms (BHK)\", min_value=1, step=1)\n",
    "balcony = st.number_input(\"Enter number of Balcony: \", min_value=0)\n",
    "\n",
    "\n",
    "# Prediction button\n",
    "if st.button(\"Predict Price\"):\n",
    "    # Create DataFrame from inputs\n",
    "    user_input = pd.DataFrame([[location, area_type, total_sqft, bath, bhk,balcony]],\n",
    "                          columns=['location', 'area_type', 'total_sqft', 'bath', 'bhk','balcony'])\n",
    "\n",
    "    # Make prediction\n",
    "    predicted_price = pipe.predict(user_input)[0]\n",
    "\n",
    "    st.success(f\"ðŸ’° Estimated Price: **{predicted_price:.2f} Lakhs**\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lvjl5EsRH9OA"
   },
   "outputs": [],
   "source": [
    "# steps\n",
    "# 1. take all pkl files in one folder after training\n",
    "# 2. create aap.py --> insert all code\n",
    "# 3. create requirements.txt --> write all library which is needed to run project\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cjjDT3sOlHs"
   },
   "outputs": [],
   "source": [
    "# Assumtions\n",
    "data = pd.read_csv(\"/content/Bengaluru_House_Data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XSHrGokpOyye"
   },
   "outputs": [],
   "source": [
    "# Regression models --> liner---> LinearRegression, ridge lasso, SGDRegressor ,\n",
    "#       non-linear -->(Decision tree) DecisionTreeRegressor, RandomForestRegressor,\n",
    "#       ensembles models --> GradientBoostingRegressor , support_vector_regression\n",
    "\n",
    "# classifications ---> Logistic regression(linear)\n",
    "# (non-linear) SVM(SVC), KNN,naive bayes, Decision tree(classifier),RandomForestClassifer,\n",
    "#       ensembles models --> all\n",
    "\n",
    "\n",
    "# Deeplearning --> non-linear -Neural Network (only change in architecture for diffrent work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LLI3uB5jc1k-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U1rcgvR2c1ai"
   },
   "outputs": [],
   "source": [
    "r = Ridge(.2)\n",
    "p2 =make_pipeline(columns_trans,r)\n",
    "p2.fit(X_train,y_train)\n",
    "y_pred = p2.predict(X_test)\n",
    "# cost functions --> calculate erros\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error,root_mean_squared_log_error\n",
    "print(\"MAE:\",mean_absolute_error(y_test,y_pred))\n",
    "print(\"MSE:\",mean_squared_error(y_test,y_pred))\n",
    "print(\"RMSE:\",np.sqrt(mean_squared_error(y_test,y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZdqvjAuic5gF"
   },
   "outputs": [],
   "source": [
    "r_squared = r2_score(y_test,y_pred)\n",
    "r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBLhmiE-dB8o"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNzZNTHshiJ3ze5reHbVmWM",
   "mount_file_id": "1CoQVZpPJI6bxFE3ABJxba40dXNDvFNKS",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
