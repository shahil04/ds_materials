{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd01148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b324e54d",
   "metadata": {},
   "source": [
    "No problem üòä ‚Äî Bayesian Optimization can seem confusing at first, but I can explain it **step by step** in simple terms, using **Logistic Regression** as an example.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† What Is Bayesian Optimization?\n",
    "\n",
    "Bayesian Optimization is a **smart way to find the best hyperparameters** (like `C` in Logistic Regression) when training a model.\n",
    "It‚Äôs **smarter and faster** than trying many random or grid combinations.\n",
    "\n",
    "---\n",
    "\n",
    "### üí° The basic idea\n",
    "\n",
    "Imagine you have a black box (your model) that takes an input (a hyperparameter, like `C`)\n",
    "and gives you an output (how well the model performs ‚Äî e.g., accuracy).\n",
    "\n",
    "You want to find **which input (C)** gives the **best accuracy**.\n",
    "\n",
    "But testing every possible value would take forever!\n",
    "\n",
    "So instead of testing everything blindly (like grid search), **Bayesian Optimization**:\n",
    "\n",
    "1. **Starts with a few random tests** to see how the model performs.\n",
    "2. **Builds a model** (called a *surrogate model*, usually a Gaussian Process) that tries to predict how good other parameter values might be.\n",
    "3. **Chooses the next best value to test**, based on both:\n",
    "\n",
    "   * How good the model thinks it might be (exploitation),\n",
    "   * How uncertain the model is (exploration).\n",
    "4. Repeats steps 2‚Äì3, learning as it goes.\n",
    "5. Finally, picks the best parameters found.\n",
    "\n",
    "It‚Äôs like a **scientist doing experiments intelligently**, not randomly.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è In Logistic Regression\n",
    "\n",
    "Let‚Äôs say we want to find the best **`C`** (regularization strength).\n",
    "\n",
    "* Small `C` = more regularization (simpler model)\n",
    "* Big `C` = less regularization (can overfit)\n",
    "\n",
    "We tell Bayesian Optimization:\n",
    "\n",
    "> \"Try different `C` values between `1e-6` and `1e3`,\n",
    "> and tell me which gives the best cross-validation accuracy.\"\n",
    "\n",
    "The optimizer starts by testing random `C`s, builds a model of how accuracy depends on `C`,\n",
    "and then tests new `C`s that it thinks might be better.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Visual Example\n",
    "\n",
    "| Iteration | C Tried | Accuracy | Optimizer learns that... |\n",
    "| --------- | ------- | -------- | ------------------------ |\n",
    "| 1         | 0.01    | 0.88     | Low C gives okay result  |\n",
    "| 2         | 10      | 0.93     | Higher C seems better    |\n",
    "| 3         | 100     | 0.92     | Too high might not help  |\n",
    "| 4         | 2.5     | 0.94     | Good!                    |\n",
    "| ...       | ...     | ...      | Keeps refining           |\n",
    "\n",
    "So instead of checking hundreds of `C` values, it finds the best one in maybe **20 tries**.\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Why use it?\n",
    "\n",
    "‚úÖ Finds good parameters **faster** than Grid or Random Search\n",
    "‚úÖ Works even when the search space is **continuous or large**\n",
    "‚úÖ Learns as it goes (doesn‚Äôt waste time testing bad regions)\n",
    "\n",
    "---\n",
    "\n",
    "### üß© In Code (simplified)\n",
    "\n",
    "```python\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "def evaluate_model(C):\n",
    "    model = LogisticRegression(C=C, solver='lbfgs', max_iter=10000)\n",
    "    score = cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()\n",
    "    return score\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=evaluate_model,   # function to optimize\n",
    "    pbounds={'C': (1e-6, 1e3)},  # parameter range\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "optimizer.maximize(init_points=5, n_iter=25)\n",
    "\n",
    "print(\"Best parameters:\", optimizer.max)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to show a **visual animation / graph** that explains how Bayesian Optimization searches for the best point step by step (with intuition, not code)?\n",
    "https://github.com/bayesian-optimization/BayesianOptimization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16428e3f",
   "metadata": {},
   "source": [
    "Here‚Äôs a step-by-step guide on how to perform **Bayesian Optimization** for tuning **Logistic Regression** hyperparameters in Python.\n",
    "\n",
    "We‚Äôll use the popular library **`scikit-learn`** for logistic regression and **`scikit-optimize` (skopt)** or **`bayes_opt`** for Bayesian optimization.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Option 1: Using `scikit-optimize` (`skopt`)\n",
    "\n",
    "### Installation\n",
    "\n",
    "```bash\n",
    "pip install scikit-learn scikit-optimize\n",
    "```\n",
    "\n",
    "### Example Code\n",
    "\n",
    "```python\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define model\n",
    "log_reg = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Define parameter search space\n",
    "param_space = {\n",
    "    'C': (1e-6, 1e+3, 'log-uniform'),   # Inverse of regularization strength\n",
    "    'penalty': ['l2'],                  # or ['l1', 'l2'] if using solver='liblinear'\n",
    "    'solver': ['lbfgs']                 # Solver choice depends on penalty\n",
    "}\n",
    "\n",
    "# Bayesian optimization\n",
    "opt = BayesSearchCV(\n",
    "    estimator=log_reg,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=30,               # Number of parameter settings to sample\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit optimizer\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Best parameters:\", opt.best_params_)\n",
    "print(\"Best CV score:\", opt.best_score_)\n",
    "print(\"Test accuracy:\", opt.score(X_test, y_test))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Option 2: Using `bayes_opt` library\n",
    "\n",
    "### Installation\n",
    "\n",
    "```bash\n",
    "pip install bayesian-optimization scikit-learn\n",
    "```\n",
    "\n",
    "### Example Code\n",
    "\n",
    "```python\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Objective function to maximize\n",
    "def cv_logreg(C):\n",
    "    model = LogisticRegression(C=C, solver='lbfgs', max_iter=10000)\n",
    "    return cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()\n",
    "\n",
    "# Define Bayesian optimizer\n",
    "optimizer = BayesianOptimization(\n",
    "    f=cv_logreg,\n",
    "    pbounds={'C': (1e-6, 1e+3)},   # search over C\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "optimizer.maximize(init_points=5, n_iter=25)\n",
    "\n",
    "print(\"Best parameters:\", optimizer.max)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Notes\n",
    "\n",
    "* `C` controls the regularization strength (`smaller C = stronger regularization`).\n",
    "* You can also optimize over:\n",
    "\n",
    "  * `penalty` (`'l1'`, `'l2'`)\n",
    "  * `solver` (`'liblinear'`, `'lbfgs'`, `'saga'`)\n",
    "  * `class_weight`\n",
    "* Be sure to use the right solver for the chosen penalty (e.g., `l1` works with `'liblinear'` or `'saga'`).\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to show how to **visualize the optimization process** (e.g., convergence plots)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d43ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Perfect üëç ‚Äî let‚Äôs extend the Bayesian Optimization example for **Logistic Regression** to include **visualization of the optimization process** so you can see how performance evolves across iterations.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Using `bayes_opt` with Visualization\n",
    "\n",
    "### Full Example\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Load data\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Define objective function\n",
    "def cv_logreg(C):\n",
    "    # Define model\n",
    "    model = LogisticRegression(C=C, solver='lbfgs', max_iter=10000)\n",
    "    # Return mean CV accuracy\n",
    "    return cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()\n",
    "\n",
    "# Define search bounds\n",
    "pbounds = {'C': (1e-6, 1e3)}\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = BayesianOptimization(\n",
    "    f=cv_logreg,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "optimizer.maximize(init_points=5, n_iter=25)\n",
    "\n",
    "# Show best result\n",
    "print(\"\\n‚úÖ Best parameters found:\")\n",
    "print(optimizer.max)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Plotting Convergence\n",
    "\n",
    "```python\n",
    "# Extract iteration data\n",
    "iters = [res[\"iteration\"] for res in optimizer.res]\n",
    "values = [res[\"target\"] for res in optimizer.res]\n",
    "\n",
    "# Cumulative best values so far\n",
    "best_values = np.maximum.accumulate(values)\n",
    "\n",
    "# Plot optimization progress\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(iters, values, 'o-', label='Observed Accuracy', color='skyblue')\n",
    "plt.plot(iters, best_values, 'r--', label='Best Accuracy So Far', linewidth=2)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"CV Accuracy\")\n",
    "plt.title(\"Bayesian Optimization Progress (Logistic Regression)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Optional: Plot Parameter Space Exploration\n",
    "\n",
    "You can also visualize how the optimizer explores the `C` parameter:\n",
    "\n",
    "```python\n",
    "# Extract parameters and corresponding accuracies\n",
    "C_vals = [res[\"params\"][\"C\"] for res in optimizer.res]\n",
    "scores = [res[\"target\"] for res in optimizer.res]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(C_vals, scores, c=scores, cmap='viridis', s=60)\n",
    "plt.xscale('log')\n",
    "plt.colorbar(label='CV Accuracy')\n",
    "plt.xlabel(\"C (log scale)\")\n",
    "plt.ylabel(\"CV Accuracy\")\n",
    "plt.title(\"Exploration of Parameter Space\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Result:**\n",
    "You‚Äôll get two plots:\n",
    "\n",
    "1. **Optimization progress** ‚Äî showing how accuracy improves across iterations.\n",
    "2. **Parameter space** ‚Äî showing how different `C` values perform.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to show a version that tunes **both `C` and `penalty`** together (using categorical parameters)?\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
