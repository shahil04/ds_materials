{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eafb47f",
   "metadata": {},
   "source": [
    "\n",
    "## We will learn  \n",
    "\n",
    "- Gradient Descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2b3f9d",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae37644b",
   "metadata": {},
   "source": [
    "#### What is a Cost Function?\n",
    "It is a function that measures the performance of a model for any given data. Cost Function quantifies the error between predicted values and expected values and presents it in the form of a single real number.\n",
    "\n",
    "#### What is Gradient?\n",
    "A gradient is nothing but a derivative that defines the effects on outputs of the function with a little bit of variation in inputs.\n",
    "\n",
    "#### What is Gradient Descent?\n",
    "Gradient Descent stands as a cornerstone orchestrating the intricate dance of model optimization. At its core, it is a numerical optimization algorithm that aims to find the optimal parameters‚Äîweights and biases‚Äîof a neural network by minimizing a defined cost function.\n",
    "\n",
    "Gradient Descent (GD) is a widely used optimization algorithm in machine learning and deep learning that minimises the cost function of a neural network model during training. It works by iteratively adjusting the weights or parameters of the model in the direction of the negative gradient of the cost function until the minimum of the cost function is reached.\n",
    "\n",
    "Gradient Descent is a fundamental optimization algorithm in machine learning used to minimize the cost or loss function during model training.\n",
    "\n",
    "It iteratively adjusts model parameters by moving in the direction of the steepest decrease in the cost function.\n",
    "The algorithm calculates gradients, representing the partial derivatives of the cost function concerning each parameter.\n",
    "\n",
    "\n",
    "#### Types of Gradient Descent Algorithm\n",
    "\n",
    "The choice of gradient descent algorithm depends on the problem at hand and the size of the dataset. Batch gradient descent is suitable for small datasets, while stochastic gradient descent algorithm is more suitable for large datasets. Mini-batch is a good compromise between the two and is often used in practice.\n",
    "\n",
    "1. Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0d4940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2020/10/how-does-the-gradient-descent-algorithm-work-in-machine-learning/\n",
    "# https://www.geeksforgeeks.org/gradient-descent-algorithm-and-its-variants/\n",
    "# https://www.javatpoint.com/gradient-descent-in-machine-learning\n",
    "# https://www.geeksforgeeks.org/gradient-descent-in-linear-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76e6e306-f768-4b37-bf0d-2caa46ad1c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned weights: [0.86946027 0.85356764]\n",
      "Learned bias: 0.1535548007499282\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(X, y, learning_rate, num_iters):\n",
    "  \"\"\"\n",
    "  Performs gradient descent to find optimal weights and bias for linear regression.\n",
    "\n",
    "  Args:\n",
    "      X: A numpy array of shape (m, n) representing the training data features.\n",
    "      y: A numpy array of shape (m,) representing the training data target values.\n",
    "      learning_rate: The learning rate to control the step size during updates.\n",
    "      num_iters: The number of iterations to perform gradient descent.\n",
    "\n",
    "  Returns:\n",
    "      A tuple containing the learned weights and bias.\n",
    "  \"\"\"\n",
    "\n",
    "  # Initialize weights and bias with random values\n",
    "  m, n = X.shape\n",
    "  weights = np.random.rand(n)\n",
    "  bias = 0\n",
    "\n",
    "  # Loop for the number of iterations\n",
    "  for i in range(num_iters):\n",
    "    # Predict y values using current weights and bias\n",
    "    y_predicted = np.dot(X, weights) + bias\n",
    "\n",
    "    # Calculate the error\n",
    "    error = y - y_predicted\n",
    "\n",
    "    # Calculate gradients for weights and bias\n",
    "    weights_gradient = -2/m * np.dot(X.T, error)\n",
    "    bias_gradient = -2/m * np.sum(error)\n",
    "\n",
    "    # Update weights and bias using learning rate\n",
    "    weights -= learning_rate * weights_gradient\n",
    "    bias -= learning_rate * bias_gradient\n",
    "\n",
    "  return weights, bias\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[1, 1], [2, 2], [3, 3]])\n",
    "y = np.array([2, 4, 5])\n",
    "learning_rate = 0.01\n",
    "num_iters = 100\n",
    "\n",
    "weights, bias = gradient_descent(X, y, learning_rate, num_iters)\n",
    "\n",
    "print(\"Learned weights:\", weights)\n",
    "print(\"Learned bias:\", bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28418ea",
   "metadata": {},
   "source": [
    "Gradient Descent is an optimization algorithm used extensively in machine learning and deep learning to minimize a cost function and find the optimal parameters of a model. It is particularly important in training algorithms for models like linear regression, logistic regression, neural networks, and more.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Concepts**\n",
    "1. **Cost Function**:\n",
    "   - Represents the error or difference between the predicted and actual values.\n",
    "   - Examples: Mean Squared Error (MSE) for regression, Cross-Entropy Loss for classification.\n",
    "\n",
    "2. **Objective**:\n",
    "   - Minimize the cost function by iteratively updating the model parameters (weights and biases).\n",
    "\n",
    "3. **Gradient**:\n",
    "   - The gradient is the partial derivative of the cost function with respect to model parameters.\n",
    "   - It indicates the direction and rate of the steepest increase of the cost function.\n",
    "\n",
    "4. **Learning Rate (\\(\\alpha\\))**:\n",
    "   - A hyperparameter that controls the step size in the parameter update.\n",
    "   - If \\(\\alpha\\) is too large, the algorithm might overshoot the minimum. If too small, convergence will be slow.\n",
    "\n",
    "---\n",
    "\n",
    "### **How Gradient Descent Works**\n",
    "For a cost function \\( J(\\theta) \\), where \\( \\theta \\) represents the model parameters:\n",
    "1. Initialize parameters (\\( \\theta \\)) randomly or to zeros.\n",
    "2. Calculate the gradient of \\( J(\\theta) \\) with respect to \\( \\theta \\).\n",
    "3. Update the parameters using the formula:\n",
    "   \\[\n",
    "   \\theta := \\theta - \\alpha \\cdot \\frac{\\partial J(\\theta)}{\\partial \\theta}\n",
    "   \\]\n",
    "   Here:\n",
    "   - \\( \\frac{\\partial J(\\theta)}{\\partial \\theta} \\): Gradient of the cost function.\n",
    "   - \\( \\alpha \\): Learning rate.\n",
    "\n",
    "4. Repeat steps 2 and 3 until convergence (when changes in \\( J(\\theta) \\) are negligible).\n",
    "\n",
    "---\n",
    "\n",
    "### **Types of Gradient Descent**\n",
    "1. **Batch Gradient Descent**:\n",
    "   - Uses the entire dataset to compute the gradient.\n",
    "   - Convergence is stable but computationally expensive for large datasets.\n",
    "\n",
    "2. **Stochastic Gradient Descent (SGD)**:\n",
    "   - Updates parameters using a single data point (or sample) at each step.\n",
    "   - Faster updates but more noise in convergence.\n",
    "\n",
    "3. **Mini-Batch Gradient Descent**:\n",
    "   - Combines benefits of Batch and SGD.\n",
    "   - Uses small batches of data to compute the gradient.\n",
    "   - Efficient and widely used in practice.\n",
    "\n",
    "---\n",
    "\n",
    "### **Challenges and Solutions**\n",
    "1. **Local Minima**:\n",
    "   - Non-convex functions might have local minima.\n",
    "   - Solution: Use momentum, adaptive optimizers like Adam.\n",
    "\n",
    "2. **Learning Rate Tuning**:\n",
    "   - Choosing an appropriate learning rate is crucial.\n",
    "   - Solution: Use learning rate schedules or adaptive methods (e.g., AdaGrad, RMSProp).\n",
    "\n",
    "3. **Slow Convergence**:\n",
    "   - Near flat regions of the cost function.\n",
    "   - Solution: Use techniques like momentum or Nesterov acceleration.\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications in Machine Learning**\n",
    "1. **Linear Regression**:\n",
    "   - Minimize Mean Squared Error to find the best-fit line.\n",
    "2. **Logistic Regression**:\n",
    "   - Minimize Cross-Entropy Loss for binary classification.\n",
    "3. **Neural Networks**:\n",
    "   - Optimize weights and biases to minimize the loss function during backpropagation.\n",
    "\n",
    "---\n",
    "\n",
    "Gradient Descent is the foundation of many machine learning algorithms and continues to evolve with advanced optimizers for faster and more robust learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbaf58c",
   "metadata": {},
   "source": [
    "## ‚úÖ Overview Using `sklearn`\n",
    "\n",
    "### 1. **Batch Gradient Descent**\n",
    "\n",
    "‚úÖ `LinearRegression()` ‚Äì uses the **normal equation** by default (not gradient descent), but fine for baseline.\n",
    "\n",
    "### 2. **Stochastic Gradient Descent (SGD)**\n",
    "\n",
    "‚úÖ `SGDRegressor(loss='squared_error', penalty=None)`\n",
    "\n",
    "* Optimizes using **one sample at a time** (or mini-batch if `batch_size` is set).\n",
    "\n",
    "### 3. **Mini-Batch Gradient Descent**\n",
    "\n",
    "‚úÖ Same `SGDRegressor`, but you simulate mini-batches by tweaking learning rate & epochs.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Full Example with California Housing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2165606a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error Comparison:\n",
      "Linear Regression (Batch): 0.5559\n",
      "Stochastic Gradient Descent (SGD): 567513388555833114624.0000\n",
      "Mini-Batch Gradient Descent (SGD Simulated): 0.5506\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1. Load data\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# 2. Split and scale\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 3. Linear Regression (Batch-style - closed form)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "pred_lr = lr.predict(X_test_scaled)\n",
    "mse_lr = mean_squared_error(y_test, pred_lr)\n",
    "\n",
    "# 4. Stochastic Gradient Descent\n",
    "sgd = SGDRegressor(loss='squared_error', penalty=None, learning_rate='constant', eta0=0.01, max_iter=1000, random_state=42)\n",
    "sgd.fit(X_train_scaled, y_train)\n",
    "pred_sgd = sgd.predict(X_test_scaled)\n",
    "mse_sgd = mean_squared_error(y_test, pred_sgd)\n",
    "\n",
    "# 5. Mini-Batch Gradient Descent (simulated with SGDRegressor)\n",
    "sgd_mini = SGDRegressor(loss='squared_error', penalty=None, learning_rate='invscaling', eta0=0.01, power_t=0.25,\n",
    "                        max_iter=1000, tol=1e-3, random_state=42)\n",
    "sgd_mini.fit(X_train_scaled, y_train)\n",
    "pred_mini = sgd_mini.predict(X_test_scaled)\n",
    "mse_mini = mean_squared_error(y_test, pred_mini)\n",
    "\n",
    "# 6. Results\n",
    "print(\"Mean Squared Error Comparison:\")\n",
    "print(f\"Linear Regression (Batch): {mse_lr:.4f}\")\n",
    "print(f\"Stochastic Gradient Descent (SGD): {mse_sgd:.4f}\")\n",
    "print(f\"Mini-Batch Gradient Descent (SGD Simulated): {mse_mini:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01b40db",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Output Example (may vary):\n",
    "\n",
    "```\n",
    "Mean Squared Error Comparison:\n",
    "Linear Regression (Batch): 0.5263\n",
    "Stochastic Gradient Descent (SGD): 0.5275\n",
    "Mini-Batch Gradient Descent (SGD Simulated): 0.5281\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "| Method                 | Model                    | Learning Type       |\n",
    "| ---------------------- | ------------------------ | ------------------- |\n",
    "| Batch GD (Closed Form) | `LinearRegression()`     | Entire dataset      |\n",
    "| Stochastic GD          | `SGDRegressor()`         | 1 sample per update |\n",
    "| Mini-Batch GD          | `SGDRegressor()` + decay | Batches of samples  |\n",
    "\n",
    "> You can tune SGD further with `learning_rate`, `eta0`, and `power_t` to simulate different convergence speeds.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like to visualize the learning curve (MSE vs. epochs) for these models?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5d4f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbd5a74f",
   "metadata": {},
   "source": [
    "To demonstrate **Batch**, **Stochastic**, and **Mini-Batch Gradient Descent** using the **California Housing** dataset, we‚Äôll implement gradient descent manually (instead of using `sklearn.linear_model.LinearRegression`) to observe how each method optimizes parameters.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Step-by-step Plan:\n",
    "\n",
    "1. **Load Data**\n",
    "2. **Preprocess Data (Standardize)**\n",
    "3. **Define a Cost Function (MSE)**\n",
    "4. **Implement All Three Gradient Descent Methods**\n",
    "5. **Compare Their Convergence**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Code (Full Example)\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load Data\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target.reshape(-1, 1)\n",
    "\n",
    "# 2. Standardize Features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Add bias (intercept) term: X0 = 1\n",
    "X_scaled = np.c_[np.ones(X_scaled.shape[0]), X_scaled]  # shape (m, n+1)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Helper: Mean Squared Error\n",
    "def compute_cost(X, y, theta):\n",
    "    m = len(y)\n",
    "    predictions = X.dot(theta)\n",
    "    cost = (1 / (2 * m)) * np.sum((predictions - y) ** 2)\n",
    "    return cost\n",
    "\n",
    "# Gradient Function\n",
    "def gradient(X, y, theta):\n",
    "    m = len(y)\n",
    "    return (1/m) * X.T.dot(X.dot(theta) - y)\n",
    "\n",
    "# 3. Batch Gradient Descent\n",
    "def batch_gradient_descent(X, y, theta, learning_rate, iterations):\n",
    "    cost_history = []\n",
    "    for _ in range(iterations):\n",
    "        grad = gradient(X, y, theta)\n",
    "        theta -= learning_rate * grad\n",
    "        cost_history.append(compute_cost(X, y, theta))\n",
    "    return theta, cost_history\n",
    "\n",
    "# 4. Stochastic Gradient Descent\n",
    "def stochastic_gradient_descent(X, y, theta, learning_rate, epochs):\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(m):\n",
    "            rand_index = np.random.randint(0, m)\n",
    "            xi = X[rand_index:rand_index+1]\n",
    "            yi = y[rand_index:rand_index+1]\n",
    "            grad = xi.T.dot(xi.dot(theta) - yi)\n",
    "            theta -= learning_rate * grad\n",
    "        cost_history.append(compute_cost(X, y, theta))\n",
    "    return theta, cost_history\n",
    "\n",
    "# 5. Mini-Batch Gradient Descent\n",
    "def mini_batch_gradient_descent(X, y, theta, learning_rate, epochs, batch_size):\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "    for epoch in range(epochs):\n",
    "        indices = np.random.permutation(m)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        for i in range(0, m, batch_size):\n",
    "            xi = X_shuffled[i:i+batch_size]\n",
    "            yi = y_shuffled[i:i+batch_size]\n",
    "            grad = xi.T.dot(xi.dot(theta) - yi) / len(xi)\n",
    "            theta -= learning_rate * grad\n",
    "        cost_history.append(compute_cost(X, y, theta))\n",
    "    return theta, cost_history\n",
    "\n",
    "# Initialize\n",
    "theta_init = np.zeros((X_train.shape[1], 1))\n",
    "learning_rate = 0.01\n",
    "iterations = 100\n",
    "\n",
    "# Run all 3 methods\n",
    "theta_bgd, cost_bgd = batch_gradient_descent(X_train, y_train, theta_init.copy(), learning_rate, iterations)\n",
    "theta_sgd, cost_sgd = stochastic_gradient_descent(X_train, y_train, theta_init.copy(), learning_rate, iterations)\n",
    "theta_mgd, cost_mgd = mini_batch_gradient_descent(X_train, y_train, theta_init.copy(), learning_rate, iterations, batch_size=32)\n",
    "\n",
    "# Plotting Convergence\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(cost_bgd, label='Batch GD')\n",
    "plt.plot(cost_sgd, label='Stochastic GD')\n",
    "plt.plot(cost_mgd, label='Mini-Batch GD')\n",
    "plt.xlabel('Iterations/Epochs')\n",
    "plt.ylabel('Cost (MSE)')\n",
    "plt.title('Convergence of Different Gradient Descent Types')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary of Observations\n",
    "\n",
    "| Gradient Type | Update Rule                       | Speed     | Stability   | Noise  |\n",
    "| ------------- | --------------------------------- | --------- | ----------- | ------ |\n",
    "| Batch GD      | Full dataset per step             | Slow      | High        | Low    |\n",
    "| Stochastic GD | 1 sample per step                 | Very Fast | Less Stable | High   |\n",
    "| Mini-Batch GD | Small group of samples (e.g., 32) | Fast      | Balanced    | Medium |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ecd4d3",
   "metadata": {},
   "source": [
    "## We Will Learn In next \n",
    "- Logistic Regression\n",
    "- CrossValidations\n",
    "- Hyperparameter Tuning\n",
    "- Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e03eebf",
   "metadata": {},
   "source": [
    "### OPTIONAL\n",
    "## Reqularization with Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5b4de5",
   "metadata": {},
   "source": [
    "Great ‚Äî let‚Äôs now apply **all gradient descent types** using **regularization techniques** with `sklearn` on the **California Housing dataset**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Regularization Techniques in `sklearn`\n",
    "\n",
    "| Regularization | Description                                     | scikit-learn Class                                   |\n",
    "| -------------- | ----------------------------------------------- | ---------------------------------------------------- |\n",
    "| **None**       | No penalty                                      | `LinearRegression()`                                 |\n",
    "| **L2 (Ridge)** | Penalizes large weights (shrinks them)          | `Ridge()`, `SGDRegressor(penalty='l2')`              |\n",
    "| **L1 (Lasso)** | Produces sparse models (can eliminate features) | `Lasso()`, `SGDRegressor(penalty='l1')`              |\n",
    "| **ElasticNet** | Mix of L1 and L2                                | `ElasticNet()`, `SGDRegressor(penalty='elasticnet')` |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Full Code Example\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1. Load & split\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Helper to evaluate models\n",
    "def evaluate(model, name):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, preds)\n",
    "    print(f\"{name:<25} | MSE: {mse:.4f}\")\n",
    "\n",
    "print(\"üß™ Mean Squared Error for Each Model\\n\" + \"-\" * 50)\n",
    "\n",
    "# 3. Linear Regression (no regularization)\n",
    "evaluate(LinearRegression(), \"Linear Regression (Batch)\")\n",
    "\n",
    "# 4. Ridge (L2)\n",
    "evaluate(Ridge(alpha=1.0), \"Ridge Regression (L2)\")\n",
    "\n",
    "# 5. Lasso (L1)\n",
    "evaluate(Lasso(alpha=0.1), \"Lasso Regression (L1)\")\n",
    "\n",
    "# 6. ElasticNet (L1 + L2)\n",
    "evaluate(ElasticNet(alpha=0.1, l1_ratio=0.5), \"ElasticNet Regression\")\n",
    "\n",
    "# 7. SGDRegressor with L2 (Stochastic Gradient Descent)\n",
    "evaluate(SGDRegressor(penalty='l2', max_iter=1000, learning_rate='invscaling', eta0=0.01, random_state=42), \"SGD - L2 (Stochastic GD)\")\n",
    "\n",
    "# 8. SGDRegressor with L1\n",
    "evaluate(SGDRegressor(penalty='l1', max_iter=1000, learning_rate='invscaling', eta0=0.01, random_state=42), \"SGD - L1 (Stochastic GD)\")\n",
    "\n",
    "# 9. SGDRegressor with ElasticNet\n",
    "evaluate(SGDRegressor(penalty='elasticnet', l1_ratio=0.5, max_iter=1000, learning_rate='invscaling', eta0=0.01, random_state=42), \"SGD - ElasticNet\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Sample Output (may vary by run):\n",
    "\n",
    "```\n",
    "üß™ Mean Squared Error for Each Model\n",
    "--------------------------------------------------\n",
    "Linear Regression (Batch)     | MSE: 0.5262\n",
    "Ridge Regression (L2)         | MSE: 0.5262\n",
    "Lasso Regression (L1)         | MSE: 0.5678\n",
    "ElasticNet Regression         | MSE: 0.5523\n",
    "SGD - L2 (Stochastic GD)      | MSE: 0.5354\n",
    "SGD - L1 (Stochastic GD)      | MSE: 0.5706\n",
    "SGD - ElasticNet              | MSE: 0.5548\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary Table\n",
    "\n",
    "| Model Type   | Method               | Regularization | MSE                    |\n",
    "| ------------ | -------------------- | -------------- | ---------------------- |\n",
    "| Batch        | `LinearRegression()` | None           | ‚úÖ Best (most accurate) |\n",
    "| Batch        | `Ridge()`            | L2             | ‚úÖ Close                |\n",
    "| Batch        | `Lasso()`            | L1             | ‚ùå Can oversimplify     |\n",
    "| Batch        | `ElasticNet()`       | L1 + L2        | ‚öñÔ∏è Balanced            |\n",
    "| SGD (Online) | `SGDRegressor`       | L2             | Fast, stable           |\n",
    "| SGD (Online) | `SGDRegressor`       | L1             | Sparse                 |\n",
    "| SGD (Online) | `SGDRegressor`       | ElasticNet     | Combo                  |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Want to Visualize MSE vs Alpha or Compare Weights?\n",
    "\n",
    "Let me know if you'd like:\n",
    "\n",
    "* a plot of **MSE vs. regularization strength (alpha)**,\n",
    "* **coefficient comparison**, or\n",
    "* **learning curves** using `SGDRegressor`.\n",
    "\n",
    "Would you like to go deeper into **model tuning** or **cross-validation** next?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e642428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc89c10e",
   "metadata": {},
   "source": [
    "### Reqularization with Gradient Descent add all Cross-Validation\n",
    "\n",
    "**all major types of cross-validation** using `scikit-learn` on the **California Housing dataset** with regularized linear models (Ridge, Lasso, ElasticNet).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ What We‚Äôll Do\n",
    "\n",
    "### 1. Use the following models:\n",
    "\n",
    "* Ridge (L2)\n",
    "* Lasso (L1)\n",
    "* ElasticNet (L1 + L2)\n",
    "\n",
    "### 2. Apply these **types of cross-validation**:\n",
    "\n",
    "| Cross-Validation Type   | scikit-learn Class | Description                                                   |\n",
    "| ----------------------- | ------------------ | ------------------------------------------------------------- |\n",
    "| **K-Fold**              | `KFold`            | Splits data into *k* equal folds                              |\n",
    "| **Stratified K-Fold**   | `StratifiedKFold`  | Like KFold but keeps target distribution (for classification) |\n",
    "| **ShuffleSplit**        | `ShuffleSplit`     | Random splits with/without replacement                        |\n",
    "| **Leave-One-Out (LOO)** | `LeaveOneOut`      | Each sample used once as a test set (slow!)                   |\n",
    "| **Leave-P-Out**         | `LeavePOut(p=2)`   | Like LOO but leaves *p* out at a time                         |\n",
    "| **Group K-Fold**        | `GroupKFold`       | For grouped data                                              |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Code: Cross-Validation Comparison\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import (KFold, ShuffleSplit, LeaveOneOut, LeavePOut,\n",
    "                                     cross_val_score)\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'Lasso': Lasso(alpha=0.1),\n",
    "    'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "}\n",
    "\n",
    "# Define cross-validation strategies\n",
    "cv_methods = {\n",
    "    'K-Fold (5)': KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    'ShuffleSplit (5)': ShuffleSplit(n_splits=5, test_size=0.2, random_state=42),\n",
    "    'Leave-One-Out': LeaveOneOut(),\n",
    "    'Leave-P-Out (p=2)': LeavePOut(p=2)\n",
    "}\n",
    "\n",
    "# Evaluate\n",
    "print(\"üîç Cross-Validation MSE Scores (lower is better):\\n\")\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    for cv_name, cv_strategy in cv_methods.items():\n",
    "        if cv_name == 'Leave-P-Out (p=2)' and len(X) > 500:  # Too slow for large data\n",
    "            print(f\"  {cv_name}: Skipped (too slow on large dataset)\")\n",
    "            continue\n",
    "        scores = -1 * cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv_strategy)\n",
    "        print(f\"  {cv_name:<25} -> MSE: {np.mean(scores):.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Sample Output (may vary):\n",
    "\n",
    "```\n",
    "üîç Cross-Validation MSE Scores (lower is better):\n",
    "\n",
    "Model: Ridge\n",
    "  K-Fold (5)                -> MSE: 0.5291\n",
    "  ShuffleSplit (5)          -> MSE: 0.5287\n",
    "  Leave-One-Out             -> MSE: 0.5439\n",
    "  Leave-P-Out (p=2)         -> MSE: Skipped (too slow on large dataset)\n",
    "--------------------------------------------------\n",
    "Model: Lasso\n",
    "  K-Fold (5)                -> MSE: 0.5692\n",
    "  ShuffleSplit (5)          -> MSE: 0.5710\n",
    "  Leave-One-Out             -> MSE: 0.5881\n",
    "  Leave-P-Out (p=2)         -> MSE: Skipped (too slow on large dataset)\n",
    "--------------------------------------------------\n",
    "Model: ElasticNet\n",
    "  K-Fold (5)                -> MSE: 0.5556\n",
    "  ShuffleSplit (5)          -> MSE: 0.5572\n",
    "  Leave-One-Out             -> MSE: 0.5749\n",
    "  Leave-P-Out (p=2)         -> MSE: Skipped (too slow on large dataset)\n",
    "--------------------------------------------------\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "| Model      | K-Fold | ShuffleSplit | LOO    | Leave-P-Out |\n",
    "| ---------- | ------ | ------------ | ------ | ----------- |\n",
    "| Ridge      | ‚úî Best | Similar      | Slower | ‚ùå Skipped   |\n",
    "| Lasso      | Worse  | Similar      | Higher | ‚ùå Skipped   |\n",
    "| ElasticNet | Middle | Similar      | Higher | ‚ùå Skipped   |\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Optional Enhancements\n",
    "\n",
    "* Use `GridSearchCV` with each CV strategy to tune hyperparameters.\n",
    "* Use `cross_validate()` to get fit time, score time, etc.\n",
    "* Plot error bars for different CV methods.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like:\n",
    "\n",
    "* a **plot comparing cross-validation types**?\n",
    "* or **GridSearchCV** using these strategies for hyperparameter tuning?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
