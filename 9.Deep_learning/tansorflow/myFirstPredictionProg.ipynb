{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f0a814d",
   "metadata": {},
   "source": [
    "# Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae81a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "D = np.array([[1,2,3],     # dataset\n",
    "              [1,0,2],\n",
    "              [0,1,4],\n",
    "              [2,1,4]])\n",
    "\n",
    "# Initialize weight matrix\n",
    "np.random.seed(1)\n",
    "w = 2*np.random.random((3,1))-1\n",
    "print(\"Weight Matrix : \")\n",
    "print(w)\n",
    "\n",
    "#Forward Pass\n",
    "for iteration in range(1):\n",
    "    iLayer = D    \n",
    "    oPer = np.dot(iLayer,w)         # Perceptron\n",
    "    oLayer = 1/(1+np.exp(-oPer))      # Sigmoid\n",
    "    \n",
    "print(\"Input :\")\n",
    "print(D)\n",
    "print(\"Output: \")\n",
    "print(oLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9ab654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "D = np.array([[1,2,3],\n",
    "              [1,0,2],\n",
    "              [0,1,4],\n",
    "              [2,1,4]])\n",
    "\n",
    "# Initialize Weight Matrices\n",
    "np.random.seed(1)\n",
    "W = np.random.random((3,4))\n",
    "print(\"W weight matrix:\")\n",
    "print(W)\n",
    "\n",
    "print(\"V weight matrix:\")\n",
    "V = np.random.random((4,2))\n",
    "print(V)\n",
    "\n",
    "for iteration in range(1):\n",
    "    iLayer = D\n",
    "    \n",
    "    hP = np.dot(iLayer,W)       # Hidden Layer\n",
    "    hLayer = 1/(1+np.exp(-hP))\n",
    "    \n",
    "    oP = np.dot(hLayer,V)       # Output Layer\n",
    "    oLayer = 1/(1+np.exp(-oP))\n",
    "    \n",
    "print(\"Input :\")\n",
    "print(training)\n",
    "print(\"Predicted Output: \")\n",
    "print(oLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204faf8f",
   "metadata": {},
   "source": [
    "# Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f424f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "D = np.array([[4,500,6],\n",
    "              [4,550,5.5],\n",
    "              [2,200,3.5],\n",
    "              [2,250,4]])\n",
    "label = np.array([[1,1,0,0]]).T\n",
    "\n",
    "np.random.seed(1)\n",
    "w = np.random.random((3,1))\n",
    "\n",
    "for iteration in range(10):\n",
    "    iLayer = D\n",
    "    p = np.dot(iLayer,w)       # Perceptron\n",
    "    print(p)\n",
    "    oLayer = 1/(1+np.exp(-p))  # Sigmoid(x)\n",
    "        \n",
    "    MSE = 2*np.square(np.subtract(oLayer,label)).mean() # Mean Square Error\n",
    "    print(MSE)\n",
    "    \n",
    "    der = p * (1-p) # dirivatives of sigmoid \n",
    "    grad = np.dot(iLayer.T, der *MSE)\n",
    "    \n",
    "    w += 0.00000000001*grad\n",
    "    print(w)\n",
    "    \n",
    "print(oLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2076f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "D = np.array([[4,500,6],\n",
    "              [4,550,5.5],\n",
    "              [2,200,3.5],\n",
    "              [2,250,4]])\n",
    "label = np.array([[1,1,0,0]]).T\n",
    "\n",
    "np.random.seed(1)\n",
    "w = np.random.random((3,4))\n",
    "v = np.random.random((4,1))\n",
    "\n",
    "for iteration in range(10):\n",
    "    iLayer = D\n",
    "    hP = np.dot(iLayer,w)       # Perceptron  \n",
    "    hLayer = 1/(1+np.exp(-hP))  # Sigmoid(x)\n",
    "    \n",
    "    oP = np.dot(hLayer,v)       # Perceptron  \n",
    "    oLayer = 1/(1+np.exp(-oP))  # Sigmoid(x)\n",
    "    \n",
    "    MSE = 2*np.square(np.subtract(oLayer,label)).mean() # Mean Square Error\n",
    "    print(MSE)\n",
    "    \n",
    "    oDer = oP * (1-oP) # dirivatives of sigmoid \n",
    "    vGrad = np.dot(oLayer.T, oDer *MSE)\n",
    "    v += 0.00000001*vGrad\n",
    "    print(v)\n",
    "    \n",
    "    hDer = hP * (1-hP) # dirivatives of sigmoid \n",
    "    wGrad = np.dot(iLayer.T, hDer *v*oDer*MSE)\n",
    "    w += 0.00000001*wGrad\n",
    "    print(w)\n",
    "    \n",
    "print(oLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d316b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = np.array([[1,2,3]])\n",
    "for iteration in range(1):\n",
    "    iLayer = testing\n",
    "    p = np.dot(iLayer,w)       # Perceptron  \n",
    "    oLayer = 1/(1+np.exp(-p))  # Sigmoid(x)\n",
    "        \n",
    "print(oLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f5b0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "D = np.array([[4,500,6],\n",
    "              [4,550,5.5],\n",
    "              [2,200,3.5],\n",
    "              [2,250,4]])\n",
    "label = np.array([[1,1,0,0]]).T\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(4, input_shape=(3,),activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # Configure the Model\n",
    "model.fit(training, output, epochs=10, batch_size=250, verbose=1, validation_split=0.2) # Train the model for fix number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dcd0a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d36d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_D = tf.constant([[4,500,6],\n",
    "              [4,550,5.5],\n",
    "              [2,200,3.5],\n",
    "              [2,250,4]]).numpy()\n",
    "test_label = tf.constant([[1,1,0,0]]).numpy().T\n",
    "\n",
    "test_results = model.evaluate(test_, test_label, verbose=1)  # Evaluate the model\n",
    "print(test_results)\n",
    "print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c4f51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "D = np.array([[4,500,6],\n",
    "              [4,550,5.5],\n",
    "              [2,200,3.5],\n",
    "              [2,250,4]])\n",
    "label = np.array([[1,1,0,0]]).T\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(2, input_shape=(3,),activation='tanh'))   \n",
    "#model.add(Dense(2,activation='tanh'))    \n",
    "model.add(Dense(1,  activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(training, output, epochs=10, batch_size=250, validation_data=(D,label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36d2d71",
   "metadata": {},
   "source": [
    "# Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fb231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input,Dense\n",
    "\n",
    "D = np.array([[4,500,6],\n",
    "              [4,550,5.5],\n",
    "              [2,200,3.5],\n",
    "              [2,250,4]])\n",
    "label = np.array([[1,1,0,0]]).T\n",
    "\n",
    "## Creating the layers\n",
    "input_layer = Input(shape=(3,))\n",
    "layer_1 = Dense(4, activation=\"relu\")(input_layer)\n",
    "layer_2 = Dense(4, activation=\"relu\")(layer_1)\n",
    "o_layer = Dense(4, activation=\"relu\")(layer_2)\n",
    "\n",
    "##Defining the model by specifying the input and output layers\n",
    "model = Model(inputs=input_layer, outputs=o_layer)\n",
    "model.summary()\n",
    "\n",
    "## defining the optimiser and loss function\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "## training the model\n",
    "model.fit(D, label,epochs=2, batch_size=128,validation_data=(D,label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d6ee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input,Dense\n",
    "\n",
    "D = np.array([[4,500,6],\n",
    "              [4,550,5.5],\n",
    "              [2,200,3.5],\n",
    "              [2,250,4]])\n",
    "label = np.array([[1,1,0,0]]).T\n",
    "\n",
    "## Creating the layers\n",
    "input_layer = Input(shape=(3,))\n",
    "layer_1 = Dense(4, activation=\"relu\")(input_layer)\n",
    "layer_2 = Dense(4, activation=\"relu\")(layer_1)\n",
    "layer_3 = Dense(4, activation=\"relu\")(layer_2)\n",
    "o1_layer= Dense(1, activation=\"linear\")(layer_2)\n",
    "o2_layer= Dense(1, activation=\"linear\")(layer_3)\n",
    "\n",
    "##Defining the model by specifying the input and output layers\n",
    "model = Model(inputs=input_layer, outputs=[o1_layer,o2_layer])\n",
    "model.summary()\n",
    "\n",
    "## defining the optimiser and loss function\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "## training the model\n",
    "model.fit(D, label,epochs=2, batch_size=128,validation_data=(D,label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df1fd83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6bff98ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_57 (Dense)            (None, 30)                120       \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 20)                620       \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 2)                 42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 782\n",
      "Trainable params: 782\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 152ms/step - loss: 0.2548 - accuracy: 0.5000 - val_loss: 0.2523 - val_accuracy: 0.5000\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2522 - accuracy: 0.5000 - val_loss: 0.2518 - val_accuracy: 0.5000\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2517 - accuracy: 0.5000 - val_loss: 0.2512 - val_accuracy: 0.5000\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2519 - accuracy: 0.5000 - val_loss: 0.2510 - val_accuracy: 0.5000\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2510 - accuracy: 0.5000 - val_loss: 0.2506 - val_accuracy: 0.5000\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2505 - accuracy: 0.5000 - val_loss: 0.2498 - val_accuracy: 0.5000\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2496 - accuracy: 0.5000 - val_loss: 0.2484 - val_accuracy: 0.5000\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2482 - accuracy: 0.5000 - val_loss: 0.2469 - val_accuracy: 0.5000\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2467 - accuracy: 0.5000 - val_loss: 0.2460 - val_accuracy: 0.5000\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2459 - accuracy: 0.5000 - val_loss: 0.2456 - val_accuracy: 0.5000\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2456 - accuracy: 0.5000 - val_loss: 0.2454 - val_accuracy: 0.5000\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2453 - accuracy: 0.5000 - val_loss: 0.2452 - val_accuracy: 0.5000\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2451 - accuracy: 0.5000 - val_loss: 0.2450 - val_accuracy: 0.5000\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2449 - accuracy: 0.5000 - val_loss: 0.2447 - val_accuracy: 0.5000\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2447 - accuracy: 0.5000 - val_loss: 0.2444 - val_accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2445 - accuracy: 1.0000 - val_loss: 0.2441 - val_accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2439 - accuracy: 1.0000 - val_loss: 0.2439 - val_accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2438 - accuracy: 0.8333 - val_loss: 0.2437 - val_accuracy: 0.5000\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2435 - accuracy: 0.5000 - val_loss: 0.2435 - val_accuracy: 0.5000\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2435 - accuracy: 0.5000 - val_loss: 0.2434 - val_accuracy: 0.5000\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2434 - accuracy: 0.5000 - val_loss: 0.2432 - val_accuracy: 0.5000\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2431 - accuracy: 0.5000 - val_loss: 0.2430 - val_accuracy: 0.5000\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2430 - accuracy: 0.5000 - val_loss: 0.2428 - val_accuracy: 0.5000\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2431 - accuracy: 0.5000 - val_loss: 0.2425 - val_accuracy: 0.5000\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2426 - accuracy: 0.5000 - val_loss: 0.2422 - val_accuracy: 0.5000\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2421 - accuracy: 0.5000 - val_loss: 0.2420 - val_accuracy: 0.5000\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2420 - accuracy: 0.5000 - val_loss: 0.2421 - val_accuracy: 0.5000\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2420 - accuracy: 0.5000 - val_loss: 0.2418 - val_accuracy: 0.5000\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2418 - accuracy: 0.5000 - val_loss: 0.2413 - val_accuracy: 0.5000\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2418 - accuracy: 0.5000 - val_loss: 0.2409 - val_accuracy: 0.5000\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2408 - accuracy: 0.5000 - val_loss: 0.2406 - val_accuracy: 0.5000\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2405 - accuracy: 0.5000 - val_loss: 0.2402 - val_accuracy: 0.5000\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2401 - accuracy: 0.5000 - val_loss: 0.2398 - val_accuracy: 0.5000\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2399 - accuracy: 0.5000 - val_loss: 0.2394 - val_accuracy: 0.5000\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2400 - accuracy: 0.5000 - val_loss: 0.2389 - val_accuracy: 0.5000\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2396 - accuracy: 0.5000 - val_loss: 0.2385 - val_accuracy: 0.5000\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2384 - accuracy: 0.5833 - val_loss: 0.2381 - val_accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2380 - accuracy: 1.0000 - val_loss: 0.2377 - val_accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2377 - accuracy: 1.0000 - val_loss: 0.2373 - val_accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2377 - accuracy: 1.0000 - val_loss: 0.2369 - val_accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2368 - accuracy: 1.0000 - val_loss: 0.2366 - val_accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2372 - accuracy: 1.0000 - val_loss: 0.2364 - val_accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2370 - accuracy: 1.0000 - val_loss: 0.2359 - val_accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2359 - accuracy: 1.0000 - val_loss: 0.2356 - val_accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2358 - accuracy: 1.0000 - val_loss: 0.2353 - val_accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2370 - accuracy: 1.0000 - val_loss: 0.2351 - val_accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2350 - accuracy: 1.0000 - val_loss: 0.2346 - val_accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2346 - accuracy: 1.0000 - val_loss: 0.2344 - val_accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2343 - accuracy: 1.0000 - val_loss: 0.2342 - val_accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2343 - accuracy: 1.0000 - val_loss: 0.2340 - val_accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2346 - accuracy: 0.8333 - val_loss: 0.2337 - val_accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2335 - accuracy: 1.0000 - val_loss: 0.2331 - val_accuracy: 1.0000\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2330 - accuracy: 1.0000 - val_loss: 0.2328 - val_accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2328 - accuracy: 1.0000 - val_loss: 0.2328 - val_accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2328 - accuracy: 1.0000 - val_loss: 0.2327 - val_accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2327 - accuracy: 1.0000 - val_loss: 0.2322 - val_accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2327 - accuracy: 1.0000 - val_loss: 0.2317 - val_accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2317 - accuracy: 1.0000 - val_loss: 0.2314 - val_accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2318 - accuracy: 1.0000 - val_loss: 0.2309 - val_accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2308 - accuracy: 1.0000 - val_loss: 0.2303 - val_accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2302 - accuracy: 1.0000 - val_loss: 0.2300 - val_accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2311 - accuracy: 1.0000 - val_loss: 0.2301 - val_accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2300 - accuracy: 1.0000 - val_loss: 0.2294 - val_accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2293 - accuracy: 1.0000 - val_loss: 0.2288 - val_accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2287 - accuracy: 1.0000 - val_loss: 0.2285 - val_accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2285 - accuracy: 1.0000 - val_loss: 0.2282 - val_accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2289 - accuracy: 1.0000 - val_loss: 0.2281 - val_accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2293 - accuracy: 1.0000 - val_loss: 0.2276 - val_accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2275 - accuracy: 1.0000 - val_loss: 0.2273 - val_accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2276 - accuracy: 1.0000 - val_loss: 0.2268 - val_accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2274 - accuracy: 1.0000 - val_loss: 0.2262 - val_accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2274 - accuracy: 1.0000 - val_loss: 0.2258 - val_accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2257 - accuracy: 1.0000 - val_loss: 0.2256 - val_accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2255 - accuracy: 1.0000 - val_loss: 0.2254 - val_accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2253 - accuracy: 1.0000 - val_loss: 0.2253 - val_accuracy: 0.7500\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2260 - accuracy: 0.7500 - val_loss: 0.2252 - val_accuracy: 0.7500\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2250 - accuracy: 0.7500 - val_loss: 0.2243 - val_accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2241 - accuracy: 1.0000 - val_loss: 0.2237 - val_accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2247 - accuracy: 1.0000 - val_loss: 0.2235 - val_accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2236 - accuracy: 1.0000 - val_loss: 0.2232 - val_accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2231 - accuracy: 1.0000 - val_loss: 0.2228 - val_accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2226 - accuracy: 1.0000 - val_loss: 0.2220 - val_accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2218 - accuracy: 1.0000 - val_loss: 0.2213 - val_accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2213 - accuracy: 1.0000 - val_loss: 0.2209 - val_accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2211 - accuracy: 1.0000 - val_loss: 0.2206 - val_accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2206 - accuracy: 1.0000 - val_loss: 0.2204 - val_accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2203 - accuracy: 1.0000 - val_loss: 0.2203 - val_accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2203 - accuracy: 1.0000 - val_loss: 0.2201 - val_accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2203 - accuracy: 1.0000 - val_loss: 0.2194 - val_accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2195 - accuracy: 1.0000 - val_loss: 0.2187 - val_accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2187 - accuracy: 1.0000 - val_loss: 0.2182 - val_accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2182 - accuracy: 1.0000 - val_loss: 0.2177 - val_accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2179 - accuracy: 1.0000 - val_loss: 0.2170 - val_accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2169 - accuracy: 1.0000 - val_loss: 0.2166 - val_accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2166 - accuracy: 1.0000 - val_loss: 0.2166 - val_accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2172 - accuracy: 1.0000 - val_loss: 0.2160 - val_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2156 - accuracy: 1.0000 - val_loss: 0.2149 - val_accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2159 - accuracy: 1.0000 - val_loss: 0.2148 - val_accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2147 - accuracy: 1.0000 - val_loss: 0.2143 - val_accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.2141 - accuracy: 1.0000 - val_loss: 0.2135 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20962762ca0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "D = np.array([[4,500,6],\n",
    "              [4,550,5.5],\n",
    "              [2,200,3.5],\n",
    "              [2,250,4],\n",
    "              [4,500,6],\n",
    "              [4,550,5.5],\n",
    "              [2,200,3.5],\n",
    "              [2,250,4],\n",
    "              [4,500,6],\n",
    "              [4,550,5.5],\n",
    "              [2,200,3.5],\n",
    "              [2,250,4],\n",
    "              [4,500,6],\n",
    "              [4,550,5.5],\n",
    "              [2,200,3.5],\n",
    "              [2,250,4]\n",
    "             ])\n",
    "label = np.array([[1,0],[1,0],[0,1],[0,1],[1,0],[1,0],[0,1],[0,1],[1,0],[1,0],[0,1],[0,1],[1,0],[1,0],[0,1],[0,1]])\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_shape=(3,),activation='sigmoid'))   \n",
    "model.add(Dense(20,activation='sigmoid'))    \n",
    "model.add(Dense(2,  activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy']) #categorical_crossentropy\n",
    "model.fit(D, label, epochs=100, batch_size=8, validation_split=0.2) #validation_data=(D,label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b37543b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 59ms/step\n",
      "[[0.5074549 0.4925451]\n",
      " [0.5417739 0.4582261]]\n"
     ]
    }
   ],
   "source": [
    "test_x = [[2,300,3],\n",
    "          [4,600,6]]\n",
    "test_y = [[0,1],[1,0]]\n",
    "#model.evaluate(test_x,test_y)\n",
    "prediction = model.predict(test_x)\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffef848c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
