{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a **Jupyter Notebook structure** with **Python code and Markdown explanations** so you can copy it directly into a new Jupyter Notebook. ğŸš€  \n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ“˜ **Basic Neural Network from Scratch in Python**  \n",
    "This notebook builds a **simple neural network** using NumPy, implementing forward propagation, loss calculation, and backpropagation.  \n",
    "\n",
    "---\n",
    "\n",
    "### **1ï¸âƒ£ Import Required Libraries**\n",
    "```python\n",
    "import numpy as np\n",
    "```\n",
    "---\n",
    "## ğŸ“ **Markdown Explanation**  \n",
    "```markdown\n",
    "# ğŸ“Œ Step 1: Import Required Libraries\n",
    "We use **NumPy** for numerical computations.\n",
    "```\n",
    "---\n",
    "\n",
    "### **2ï¸âƒ£ Initialize Neural Network Parameters**\n",
    "```python\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define network structure\n",
    "input_size = 2\n",
    "hidden_size = 2\n",
    "output_size = 1\n",
    "\n",
    "# Initialize weights and biases\n",
    "W1 = np.random.randn(input_size, hidden_size)  # Weights between input and hidden layer\n",
    "b1 = np.zeros((1, hidden_size))                # Bias for hidden layer\n",
    "\n",
    "W2 = np.random.randn(hidden_size, output_size) # Weights between hidden and output layer\n",
    "b2 = np.zeros((1, output_size))                # Bias for output layer\n",
    "```\n",
    "---\n",
    "## ğŸ“ **Markdown Explanation**  \n",
    "```markdown\n",
    "# ğŸ“Œ Step 2: Initialize Neural Network Parameters\n",
    "- **input_size = 2** â†’ Two input neurons\n",
    "- **hidden_size = 2** â†’ Two neurons in the hidden layer\n",
    "- **output_size = 1** â†’ One output neuron\n",
    "- **Weights and biases are initialized randomly**\n",
    "```\n",
    "---\n",
    "\n",
    "### **3ï¸âƒ£ Define Activation Function**\n",
    "```python\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)  # Derivative for backpropagation\n",
    "```\n",
    "---\n",
    "## ğŸ“ **Markdown Explanation**  \n",
    "```markdown\n",
    "# ğŸ“Œ Step 3: Define Activation Function\n",
    "- We use **Sigmoid Activation Function**:\n",
    "  - Output range: (0,1)\n",
    "  - Formula: `1 / (1 + exp(-x))`\n",
    "  - Helps in non-linearity\n",
    "- Also define **Sigmoid Derivative** for backpropagation\n",
    "```\n",
    "---\n",
    "\n",
    "### **4ï¸âƒ£ Forward Propagation**\n",
    "```python\n",
    "def forward_propagation(X):\n",
    "    global W1, b1, W2, b2\n",
    "    \n",
    "    # Hidden layer computation\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "\n",
    "    # Output layer computation\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = sigmoid(Z2)  # Final prediction\n",
    "    \n",
    "    return Z1, A1, Z2, A2\n",
    "```\n",
    "---\n",
    "## ğŸ“ **Markdown Explanation**  \n",
    "```markdown\n",
    "# ğŸ“Œ Step 4: Forward Propagation\n",
    "- Compute activations in **Hidden Layer**: `A1 = sigmoid(X.W1 + b1)`\n",
    "- Compute activations in **Output Layer**: `A2 = sigmoid(A1.W2 + b2)`\n",
    "- A2 gives final predictions.\n",
    "```\n",
    "---\n",
    "\n",
    "### **5ï¸âƒ£ Compute Loss**\n",
    "```python\n",
    "def compute_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)  # MSE Loss\n",
    "```\n",
    "---\n",
    "## ğŸ“ **Markdown Explanation**  \n",
    "```markdown\n",
    "# ğŸ“Œ Step 5: Compute Loss\n",
    "- We use **Mean Squared Error (MSE)**\n",
    "- Formula: `(1/n) * Î£(y_true - y_pred)Â²`\n",
    "- Measures how close the prediction is to the actual output.\n",
    "```\n",
    "---\n",
    "\n",
    "### **6ï¸âƒ£ Backpropagation**\n",
    "```python\n",
    "def backward_propagation(X, y, Z1, A1, Z2, A2, learning_rate=0.1):\n",
    "    global W1, b1, W2, b2\n",
    "\n",
    "    # Compute output layer error\n",
    "    error_output = A2 - y\n",
    "    delta_output = error_output * sigmoid_derivative(A2)  # Gradient w.r.t output\n",
    "\n",
    "    # Compute hidden layer error\n",
    "    error_hidden = delta_output.dot(W2.T)\n",
    "    delta_hidden = error_hidden * sigmoid_derivative(A1)  # Gradient w.r.t hidden layer\n",
    "\n",
    "    # Update weights and biases\n",
    "    W2 -= A1.T.dot(delta_output) * learning_rate\n",
    "    b2 -= np.sum(delta_output, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    W1 -= X.T.dot(delta_hidden) * learning_rate\n",
    "    b1 -= np.sum(delta_hidden, axis=0, keepdims=True) * learning_rate\n",
    "```\n",
    "---\n",
    "## ğŸ“ **Markdown Explanation**  \n",
    "```markdown\n",
    "# ğŸ“Œ Step 6: Backpropagation\n",
    "- Compute **error at output layer** and adjust weights using **Gradient Descent**.\n",
    "- Compute **error at hidden layer** using chain rule.\n",
    "- Update weights `W1, W2` and biases `b1, b2` using **learning rate**.\n",
    "```\n",
    "---\n",
    "\n",
    "### **7ï¸âƒ£ Train the Neural Network**\n",
    "```python\n",
    "# Sample training data (XOR Problem)\n",
    "X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Inputs\n",
    "y_train = np.array([[0], [1], [1], [0]])  # Expected outputs\n",
    "\n",
    "# Training loop\n",
    "epochs = 10000\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    Z1, A1, Z2, A2 = forward_propagation(X_train)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = compute_loss(y_train, A2)\n",
    "    \n",
    "    # Backward pass\n",
    "    backward_propagation(X_train, y_train, Z1, A1, Z2, A2)\n",
    "\n",
    "    # Print loss every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.5f}\")\n",
    "```\n",
    "---\n",
    "## ğŸ“ **Markdown Explanation**  \n",
    "```markdown\n",
    "# ğŸ“Œ Step 7: Train the Neural Network\n",
    "- Train for **10,000 epochs**.\n",
    "- Update weights and biases using **backpropagation**.\n",
    "- Print **loss every 1000 epochs** to track improvement.\n",
    "```\n",
    "---\n",
    "\n",
    "### **8ï¸âƒ£ Make Predictions**\n",
    "```python\n",
    "# Forward propagation for new inputs\n",
    "_, _, _, predictions = forward_propagation(X_train)\n",
    "\n",
    "# Convert probabilities to binary outputs (0 or 1)\n",
    "predictions = (predictions > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "print(predictions)\n",
    "```\n",
    "---\n",
    "## ğŸ“ **Markdown Explanation**  \n",
    "```markdown\n",
    "# ğŸ“Œ Step 8: Make Predictions\n",
    "- Pass training data through the trained model.\n",
    "- Convert output probabilities to **binary values (0 or 1)**.\n",
    "```\n",
    "---\n",
    "\n",
    "## ğŸ¯ **Final Output Example**\n",
    "```\n",
    "Epoch 0, Loss: 0.25033\n",
    "Epoch 1000, Loss: 0.24973\n",
    "...\n",
    "Epoch 9000, Loss: 0.24778\n",
    "\n",
    "Predictions:\n",
    "[[0]\n",
    " [1]\n",
    " [1]\n",
    " [0]]\n",
    "```\n",
    "âœ… **The neural network successfully learns the XOR function!** ğŸ‰\n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ’¾ Download Jupyter Notebook File (.ipynb)**\n",
    "Would you like me to generate and provide a **direct .ipynb download link** for this? ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
