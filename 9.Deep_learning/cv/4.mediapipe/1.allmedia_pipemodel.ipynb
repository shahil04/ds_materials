{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3900b0c",
   "metadata": {},
   "source": [
    "# **Lesson Notes: MediaPipe for Computer Vision & AI**\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… **Lesson Introduction**\n",
    "\n",
    "MediaPipe is an open-source, cross-platform framework by Google for building **AI-powered real-time computer vision pipelines**. It provides **pre-trained solutions** for face detection, hand tracking, pose estimation, object detection, and more.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… **Real-World Hook**\n",
    "\n",
    "* **Face Filters** in Instagram and Snapchat.\n",
    "* **Hand Gesture Control** in AR/VR games.\n",
    "* **Pose Tracking** in fitness apps like **Google Fit**.\n",
    "* **Face Mesh** in virtual makeup or face beautification apps.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… **Introduce MediaPipe as a Solution**\n",
    "\n",
    "Traditional computer vision approaches require **manual implementation** of detection algorithms (e.g., Haar cascades, keypoint extraction).\n",
    "MediaPipe simplifies this with:\n",
    "\n",
    "* **Pre-trained ML solutions** (Face Detection, Hand Tracking, etc.).\n",
    "* **Lightweight & Fast** (supports real-time inference on mobile, desktop, and web).\n",
    "* **Cross-platform**: Python, C++, Android, iOS, Web.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… **Theory**\n",
    "\n",
    "### **What is MediaPipe?**\n",
    "\n",
    "* A framework for building **perception pipelines** for real-time computer vision and ML.\n",
    "* Developed by **Google AI**.\n",
    "* Provides **ready-to-use solutions** for:\n",
    "\n",
    "  * Face Detection & Mesh\n",
    "  * Hand Tracking\n",
    "  * Pose Estimation\n",
    "  * Objectron (3D Object Detection)\n",
    "  * Holistic Tracking (Face + Hands + Pose)\n",
    "\n",
    "---\n",
    "\n",
    "### **Why use MediaPipe?**\n",
    "\n",
    "* **Real-time performance**\n",
    "* **Pre-trained, optimized models**\n",
    "* **Runs on CPU, GPU, and Edge devices**\n",
    "* **Easy Python integration**\n",
    "\n",
    "---\n",
    "\n",
    "### **When to use it?**\n",
    "\n",
    "* Gesture-based interaction systems.\n",
    "* AR/VR applications.\n",
    "* Fitness/Healthcare apps for posture analysis.\n",
    "* Sign language recognition.\n",
    "* Virtual try-on (makeup, glasses, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "### **How does MediaPipe work?**\n",
    "\n",
    "* **Graph-based Framework:**\n",
    "\n",
    "  * Nodes (calculators) â†’ perform operations (detection, tracking).\n",
    "  * Packets â†’ carry data between nodes.\n",
    "* Uses **ML models + Computer Vision techniques** for detection & tracking.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcc86a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### âœ… **Syntax & Installation**\n",
    "\n",
    "# !pip install mediapipe opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e4f1f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "## âœ… **Practical Example: Face Detection**\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_face_detection.FaceDetection(min_detection_confidence=0.5) as face_detection:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_detection.process(rgb_frame)\n",
    "        \n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "                mp_drawing.draw_detection(frame, detection)\n",
    "        \n",
    "        cv2.imshow('Face Detection', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ac4dfe",
   "metadata": {},
   "source": [
    "# 2. HandTracking project 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0922487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(image_rgb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            mp.solutions.drawing_utils.draw_landmarks(\n",
    "                image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    cv2.imshow('Hand Tracking', image)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85fb291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ec02e76",
   "metadata": {},
   "source": [
    "# 3. Feshmesh --> face motion detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh()\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(image_rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for landmarks in results.multi_face_landmarks:\n",
    "            mp.solutions.drawing_utils.draw_landmarks(\n",
    "                image, landmarks, mp_face_mesh.FACEMESH_TESSELATION)\n",
    "\n",
    "    cv2.imshow('FaceMesh', image)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3973e1",
   "metadata": {},
   "source": [
    "# 4. PoseDetection projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "474c612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    result = pose.process(image_rgb)\n",
    "\n",
    "    if result.pose_landmarks:\n",
    "        mp.solutions.drawing_utils.draw_landmarks(\n",
    "            image, result.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    cv2.imshow('Pose Detection', image)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab831072",
   "metadata": {},
   "source": [
    "# 5. Holistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2cd5068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic = mp_holistic.Holistic()\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    result = holistic.process(image_rgb)\n",
    "\n",
    "    # Draw all components\n",
    "    mp.solutions.drawing_utils.draw_landmarks(image, result.face_landmarks, mp_holistic.FACEMESH_TESSELATION)\n",
    "    mp.solutions.drawing_utils.draw_landmarks(image, result.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    mp.solutions.drawing_utils.draw_landmarks(image, result.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    mp.solutions.drawing_utils.draw_landmarks(image, result.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "\n",
    "    cv2.imshow('Holistic Model', image)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e593b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e75e3aa0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## âœ… **Business Scenario**\n",
    "\n",
    "A fitness company wants to **track user poses in real-time** during online workouts. Using **MediaPipe Pose**, they can:\n",
    "\n",
    "* Detect **key body landmarks** (shoulders, knees, etc.).\n",
    "* Calculate angles for **exercise form correction**.\n",
    "* Provide **real-time feedback** without expensive hardware.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… **Practice Session**\n",
    "\n",
    "### **Questions**\n",
    "\n",
    "1. What is MediaPipe, and why is it used?\n",
    "2. List 5 real-world applications of MediaPipe.\n",
    "3. Which function is used to initialize Face Detection in MediaPipe?\n",
    "4. Explain the difference between **Face Detection** and **Face Mesh**.\n",
    "5. Write Python code to **detect hand landmarks** using MediaPipe.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… **Case Study**\n",
    "\n",
    "**Project:** **Hand Gesture Volume Control**\n",
    "\n",
    "* Use **MediaPipe Hands** to detect hand landmarks.\n",
    "* Recognize **thumb & index finger distance**.\n",
    "* Control system volume based on finger distance.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… **Key MediaPipe Solutions**\n",
    "\n",
    "| **Solution**   | **Description**                |\n",
    "| -------------- | ------------------------------ |\n",
    "| Face Detection | Detects faces in images/videos |\n",
    "| Face Mesh      | Detects 468 facial landmarks   |\n",
    "| Hands          | Detects 21 hand landmarks      |\n",
    "| Pose           | Detects 33 body landmarks      |\n",
    "| Holistic       | Combines face, hands, and pose |\n",
    "| Objectron      | Detects 3D objects             |\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ‘‰ Do you want me to **prepare a full course module** on MediaPipe for your students, including:\n",
    "âœ… **Lesson Plan (Beginner â†’ Advanced)**\n",
    "âœ… **Real-time Projects** (Face Detection, Gesture Control, Pose Estimation)\n",
    "âœ… **Assignments + Case Studies + Datasets**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989aeedb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
