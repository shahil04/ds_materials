{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Building a **production-level MLOps project on GCP** with all the bells and whistles — experiments, pipelines, Docker, CI/CD, MLflow, DVC, Kubernetes, Prometheus & Grafana — is a great comprehensive approach.\n",
    "\n",
    "I'll outline the **end-to-end MLOps architecture** and detailed steps to implement each component on GCP.\n",
    "\n",
    "---\n",
    "\n",
    "# MLOps Project Architecture on GCP (with All Key Components)\n",
    "\n",
    "---\n",
    "\n",
    "### Core Components & Flow\n",
    "\n",
    "| Step                       | Purpose & Tools                                                                     |\n",
    "| -------------------------- | ----------------------------------------------------------------------------------- |\n",
    "| **Experiment Tracking**    | Track model runs, parameters, metrics — MLflow or Vertex AI Experiments             |\n",
    "| **Data Versioning**        | Version datasets and preprocessing code — DVC or Google Cloud Storage (GCS) + DVC   |\n",
    "| **Pipeline Orchestration** | Automate workflows — Vertex AI Pipelines or Kubeflow Pipelines                      |\n",
    "| **Containerization**       | Package training & inference code — Docker                                          |\n",
    "| **Model Registry**         | Store and manage model versions — MLflow Model Registry or Vertex AI Model Registry |\n",
    "| **CI/CD**                  | Automate build, test, deployment — GitHub Actions or Jenkins                        |\n",
    "| **Model Deployment**       | Serve models at scale — Kubernetes on GKE or Cloud Run                              |\n",
    "| **Monitoring & Logging**   | Track system & app health — Prometheus + Grafana + Stackdriver Logging              |\n",
    "| **Caching**                | Improve inference latency — Redis (Memorystore on GCP)                              |\n",
    "\n",
    "---\n",
    "\n",
    "# Step-by-Step Guide\n",
    "\n",
    "---\n",
    "\n",
    "## 1️⃣ Experiment Tracking with MLflow on GCP\n",
    "\n",
    "* Run MLflow Tracking Server on GKE or Cloud Run with backend store on **Cloud SQL** (PostgreSQL).\n",
    "* Log parameters, metrics, and artifacts during training.\n",
    "* Use MLflow UI to compare experiments.\n",
    "\n",
    "**Basic MLflow Setup:**\n",
    "\n",
    "```bash\n",
    "pip install mlflow\n",
    "```\n",
    "\n",
    "Sample MLflow code snippet:\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://your-mlflow-server:5000\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"learning_rate\", 0.01)\n",
    "    mlflow.log_metric(\"accuracy\", 0.95)\n",
    "    mlflow.log_artifact(\"model.pkl\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2️⃣ Data Versioning with DVC + GCS\n",
    "\n",
    "* Use DVC to track datasets & code.\n",
    "* Store large files on **Google Cloud Storage** buckets.\n",
    "\n",
    "```bash\n",
    "dvc init\n",
    "dvc remote add -d gcsremote gs://your-gcs-bucket/path\n",
    "dvc add data/dataset.csv\n",
    "dvc push\n",
    "```\n",
    "\n",
    "* Integrate DVC commands into your CI/CD pipelines to fetch correct data versions.\n",
    "\n",
    "---\n",
    "\n",
    "## 3️⃣ Pipeline Orchestration with Vertex AI Pipelines / Kubeflow Pipelines\n",
    "\n",
    "* Define your end-to-end ML workflow (data prep, training, evaluation, deployment).\n",
    "* Use **Kubeflow Pipelines** or Google’s managed **Vertex AI Pipelines** for orchestration.\n",
    "* Pipelines are defined in Python (using KFP SDK) and deployed on GKE or Vertex AI.\n",
    "\n",
    "Example minimal pipeline snippet:\n",
    "\n",
    "```python\n",
    "from kfp.v2 import dsl\n",
    "\n",
    "@dsl.pipeline(name='sentiment-analysis-pipeline')\n",
    "def pipeline():\n",
    "    preprocess_op = dsl.ContainerOp(\n",
    "        name='Preprocess',\n",
    "        image='gcr.io/project/preprocess:latest',\n",
    "        command=['python', 'preprocess.py']\n",
    "    )\n",
    "    train_op = dsl.ContainerOp(\n",
    "        name='Train',\n",
    "        image='gcr.io/project/train:latest',\n",
    "        command=['python', 'train.py']\n",
    "    )\n",
    "    train_op.after(preprocess_op)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4️⃣ Containerization with Docker\n",
    "\n",
    "* Containerize all steps (preprocessing, training, inference).\n",
    "* Push images to **Google Container Registry (GCR)** or **Artifact Registry**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5️⃣ Model Registry\n",
    "\n",
    "* Register models in MLflow Model Registry or **Vertex AI Model Registry**.\n",
    "* Enable model versioning and stage transitions (staging, production).\n",
    "\n",
    "---\n",
    "\n",
    "## 6️⃣ CI/CD Pipeline (GitHub Actions)\n",
    "\n",
    "* Automate build, test, and deploy pipelines.\n",
    "* Trigger training or deployment pipelines on code/data changes.\n",
    "\n",
    "Example steps:\n",
    "\n",
    "* Checkout code\n",
    "* Set up environment & dependencies\n",
    "* Run tests\n",
    "* Build Docker image and push to GCR\n",
    "* Trigger Vertex AI pipeline or deploy model to GKE/Cloud Run\n",
    "\n",
    "---\n",
    "\n",
    "## 7️⃣ Model Deployment on Kubernetes (GKE)\n",
    "\n",
    "* Deploy model server (FastAPI, TorchServe, TensorFlow Serving) on Kubernetes.\n",
    "* Use **Kubernetes Deployments**, **Services**, and **Ingress**.\n",
    "* Enable autoscaling with Horizontal Pod Autoscaler (HPA).\n",
    "\n",
    "---\n",
    "\n",
    "## 8️⃣ Monitoring & Logging\n",
    "\n",
    "* Integrate **Prometheus** to scrape app and Kubernetes metrics.\n",
    "* Visualize with **Grafana** dashboards.\n",
    "* Use **Google Cloud Operations (Stackdriver)** for centralized logging and alerting.\n",
    "\n",
    "---\n",
    "\n",
    "## 9️⃣ Caching with Redis (Google Memorystore)\n",
    "\n",
    "* Deploy Redis for caching repeated inference results.\n",
    "* Integrate Redis client into your inference API.\n",
    "\n",
    "---\n",
    "\n",
    "# Bonus: Infrastructure as Code (IaC)\n",
    "\n",
    "* Use **Terraform** or **Google Deployment Manager** for repeatable infra provisioning.\n",
    "* Manage Kubernetes clusters, Cloud SQL, Cloud Storage, Memorystore, etc.\n",
    "\n",
    "---\n",
    "\n",
    "# Summary Table\n",
    "\n",
    "| Stage                  | Tool/Service                             | GCP Equivalent / Notes               |\n",
    "| ---------------------- | ---------------------------------------- | ------------------------------------ |\n",
    "| Experiment Tracking    | MLflow                                   | MLflow on GKE / Cloud Run            |\n",
    "| Data Versioning        | DVC + GCS                                | DVC + Google Cloud Storage           |\n",
    "| Pipeline Orchestration | Kubeflow Pipelines / Vertex AI Pipelines | Vertex AI Pipelines (managed)        |\n",
    "| Containerization       | Docker                                   | GCR or Artifact Registry             |\n",
    "| Model Registry         | MLflow Model Registry / Vertex AI        | Vertex AI Model Registry             |\n",
    "| CI/CD                  | GitHub Actions / Jenkins                 | GitHub Actions / Jenkins             |\n",
    "| Deployment             | Kubernetes (GKE) / Cloud Run             | GKE clusters / Cloud Run             |\n",
    "| Monitoring & Logging   | Prometheus + Grafana + Stackdriver       | Prometheus + Grafana + Cloud Logging |\n",
    "| Caching                | Redis                                    | Google Memorystore (Managed Redis)   |\n",
    "| IaC                    | Terraform / Deployment Manager           | Terraform GCP Provider               |\n",
    "\n",
    "---\n",
    "\n",
    "# Next Steps\n",
    "\n",
    "I can generate for you:\n",
    "\n",
    "* Sample repo with **MLflow + DVC + FastAPI + Dockerfile**\n",
    "* Sample **Kubeflow pipeline definition** and Vertex AI pipeline setup\n",
    "* Full **GitHub Actions CI/CD pipeline** for build, test, deploy\n",
    "* **Terraform scripts** to provision GCP infra (GKE, Cloud SQL, Memorystore, etc.)\n",
    "* Sample **Prometheus + Grafana** setup for app monitoring\n",
    "* Redis integration example in FastAPI inference API\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to start by creating a **sample MLOps repo with core components (MLflow + DVC + Docker + FastAPI)** and then add the CI/CD and GCP deployment scripts? Or should I focus on the pipeline orchestration with Vertex AI or Kubeflow first?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Here’s the **full detailed guide and all the code snippets** you need to create your production-level MLOps project step-by-step on your local machine.\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Folder structure\n",
    "\n",
    "Create folders and files as below:\n",
    "\n",
    "```\n",
    "mlops-sentiment-analysis/\n",
    "├── data/\n",
    "│   └── train.csv          # sample dataset\n",
    "├── models/\n",
    "│   └── saved_model.pkl    # saved model after training\n",
    "├── src/\n",
    "│   └── train.py           # training + MLflow tracking\n",
    "├── app/\n",
    "│   ├── main.py            # FastAPI inference app\n",
    "├── Dockerfile\n",
    "├── requirements.txt\n",
    "├── README.md\n",
    "└── .github/\n",
    "    └── workflows/\n",
    "        └── ci-cd.yml      # GitHub Actions CI/CD pipeline\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Sample data `data/train.csv`\n",
    "\n",
    "```csv\n",
    "text,label\n",
    "I love this movie,1\n",
    "This movie is terrible,0\n",
    "Amazing acting and story,1\n",
    "Worst film I have ever seen,0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Training script with MLflow — `src/train.py`\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "def load_data(path):\n",
    "    data = pd.read_csv(path)\n",
    "    return data['text'], data['label']\n",
    "\n",
    "def train():\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")  # Change if using remote MLflow server\n",
    "    mlflow.set_experiment(\"Sentiment_Analysis_Experiment\")\n",
    "\n",
    "    X, y = load_data(\"data/train.csv\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    preds = model.predict(X_test_tfidf)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param(\"max_features\", 5000)\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        mlflow.log_artifact(\"vectorizer.pkl\")\n",
    "\n",
    "    joblib.dump(vectorizer, \"vectorizer.pkl\")\n",
    "    joblib.dump(model, \"models/saved_model.pkl\")\n",
    "    print(f\"Model accuracy: {acc}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 4. FastAPI inference app — `app/main.py`\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "import joblib\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "model = joblib.load(\"../models/saved_model.pkl\")\n",
    "vectorizer = joblib.load(\"../vectorizer.pkl\")\n",
    "\n",
    "class InputData(BaseModel):\n",
    "    text: str\n",
    "\n",
    "@app.post(\"/predict/\")\n",
    "def predict(data: InputData):\n",
    "    X = vectorizer.transform([data.text])\n",
    "    pred = model.predict(X)\n",
    "    label = int(pred[0])\n",
    "    return {\"prediction\": label}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Dockerfile\n",
    "\n",
    "```dockerfile\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "COPY src/ ./src/\n",
    "COPY app/ ./app/\n",
    "COPY models/ ./models/\n",
    "COPY vectorizer.pkl ./\n",
    "\n",
    "CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 6. Requirements — `requirements.txt`\n",
    "\n",
    "```\n",
    "fastapi\n",
    "uvicorn\n",
    "scikit-learn\n",
    "pandas\n",
    "joblib\n",
    "mlflow\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 7. GitHub Actions workflow `.github/workflows/ci-cd.yml`\n",
    "\n",
    "```yaml\n",
    "name: CI/CD Pipeline\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [main]\n",
    "\n",
    "jobs:\n",
    "  build-and-deploy:\n",
    "    runs-on: ubuntu-latest\n",
    "\n",
    "    steps:\n",
    "    - name: Checkout code\n",
    "      uses: actions/checkout@v3\n",
    "\n",
    "    - name: Set up Python\n",
    "      uses: actions/setup-python@v4\n",
    "      with:\n",
    "        python-version: 3.9\n",
    "\n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        pip install -r requirements.txt\n",
    "\n",
    "    - name: Run training script\n",
    "      run: |\n",
    "        python src/train.py\n",
    "\n",
    "    - name: Log in to Google Cloud\n",
    "      uses: google-github-actions/auth@v1\n",
    "      with:\n",
    "        credentials_json: ${{ secrets.GCP_SA_KEY }}\n",
    "\n",
    "    - name: Configure Docker\n",
    "      run: gcloud auth configure-docker\n",
    "\n",
    "    - name: Build Docker image\n",
    "      run: |\n",
    "        docker build -t gcr.io/${{ secrets.GCP_PROJECT }}/sentiment-analysis-app .\n",
    "\n",
    "    - name: Push Docker image\n",
    "      run: |\n",
    "        docker push gcr.io/${{ secrets.GCP_PROJECT }}/sentiment-analysis-app\n",
    "\n",
    "    - name: Deploy to Cloud Run\n",
    "      run: |\n",
    "        gcloud run deploy sentiment-analysis-api \\\n",
    "          --image gcr.io/${{ secrets.GCP_PROJECT }}/sentiment-analysis-app \\\n",
    "          --platform managed \\\n",
    "          --region us-central1 \\\n",
    "          --allow-unauthenticated\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 8. README.md (summary)\n",
    "\n",
    "```markdown\n",
    "# MLOps Sentiment Analysis Project\n",
    "\n",
    "## Overview\n",
    "This repo demonstrates a production MLOps pipeline with:\n",
    "- MLflow experiment tracking\n",
    "- FastAPI inference API\n",
    "- Docker containerization\n",
    "- GitHub Actions CI/CD for build, test, deploy to Google Cloud Run\n",
    "\n",
    "## Setup\n",
    "\n",
    "1. Install dependencies\n",
    "```\n",
    "\n",
    "pip install -r requirements.txt\n",
    "\n",
    "```\n",
    "\n",
    "2. Run training locally\n",
    "```\n",
    "\n",
    "python src/train.py\n",
    "\n",
    "```\n",
    "\n",
    "3. Run FastAPI server locally\n",
    "```\n",
    "\n",
    "uvicorn app.main\\:app --reload\n",
    "\n",
    "```\n",
    "\n",
    "4. Push to GitHub main branch to trigger CI/CD pipeline.\n",
    "\n",
    "## Notes\n",
    "- Update MLflow tracking URI in `src/train.py` to your MLflow server.\n",
    "- Set GitHub secrets `GCP_SA_KEY` and `GCP_PROJECT` for GCP access in pipeline.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 9. Additional MLOps integration pointers:\n",
    "\n",
    "* **DVC:** For data versioning, run locally:\n",
    "\n",
    "  ```bash\n",
    "  dvc init\n",
    "  dvc remote add -d gcsremote gs://your-gcs-bucket/path\n",
    "  dvc add data/train.csv\n",
    "  dvc push\n",
    "  ```\n",
    "* **MLflow:** Deploy MLflow server on GCP (Cloud Run or GKE) and update `mlflow.set_tracking_uri`.\n",
    "* **Monitoring:** Setup Prometheus + Grafana on your Kubernetes cluster for API + model monitoring.\n",
    "* **Caching:** Add Redis caching in FastAPI to speed up repeated queries.\n",
    "* **Kubernetes:** Deploy your docker container on GKE for scalability.\n",
    "* **CI/CD:** Use GitHub Actions (already provided) for automation.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can help you write detailed commands and YAML for DVC pipelines, MLflow server setup on GCP, or Kubernetes manifests next. Just ask! Would you like me to?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
