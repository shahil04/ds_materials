{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb2a7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (4.50.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "Installing collected packages: huggingface-hub\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.24.7\n",
      "    Uninstalling huggingface-hub-0.24.7:\n",
      "      Successfully uninstalled huggingface-hub-0.24.7\n",
      "Successfully installed huggingface-hub-0.33.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "yolov5 7.0.14 requires huggingface-hub<0.25.0,>=0.12.0, but you have huggingface-hub 0.33.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b17a2ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c84637a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'this is good work in class 12th', 'labels': ['education', 'sports', 'politics'], 'scores': [0.9495951533317566, 0.025724448263645172, 0.02468038722872734]}\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "res= classifier(\n",
    "    \"this is good work in class 12th\", \n",
    "    candidate_labels=[\"education\",\"sports\",\"politics\"],\n",
    "                )\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9f7b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "    d =\"\"\"\n",
    "    Mahendra Singh Dhoni (/m…ôÀàhe…™ndr…ô Ààs…™≈ã dh√¶Ààn…™/ ‚ìò; born 7 July 1981) is an Indian professional cricketer who plays as a right-handed batter and a wicket-keeper. Widely regarded as one of the most prolific wicket-keeper batsmen and captains and one of the greatest ODI batsmen, he represented the Indian cricket team and was the captain of the side in limited overs formats from 2007 to 2017 and in test cricket from 2008 to 2014. Dhoni has captained the most international matches and is the most successful Indian captain. He has led India to victory in the 2007 ICC World Twenty20, the 2011 Cricket World Cup, and the 2013 ICC Champions Trophy, being the only captain to win three different limited overs ICC tournaments. He also led the teams that won the Asia Cup in 2010, 2016 and was a member of the title winning squad in 2018.\n",
    "\n",
    "    Born in Ranchi, Dhoni made his first class debut for Bihar in 1999. He made his debut for the Indian cricket team on 23 December 2004 in an ODI against Bangladesh and played his first test a year later against Sri Lanka. In 2007, he became the captain of the ODI side before taking over in all formats by 2008. Dhoni retired from test cricket in 2014 but continued playing in limited overs cricket till 2019. He has scored 17,266 runs in international cricket including 10,000 plus runs at an average of more than 50 in ODIs.\n",
    "\n",
    "    In the Indian Premier League (IPL), Dhoni plays for Chennai Super Kings (CSK), leading them to the final on ten occasions and winning it five times (2010, 2011, 2018, 2021 and 2023 ) jointly sharing this title with Rohit Sharma . He has also led CSK to two Champions League T20 titles in 2010 and 2014. Dhoni is among the few batsmen to have scored more than five thousand runs in the IPL, as well as being the first wicket-keeper to do so.\n",
    "\n",
    "    In 2008, Dhoni was awarded India's highest sport honour Major Dhyan Chand Khel Ratna Award by Government of India. He received the fourth highest civilian award Padma Shri in 2009 and third highest civilian award Padma Bhushan in 2018. Dhoni holds an honorary rank of Lieutenant colonel in the Parachute Regiment of the Indian Territorial Army which was presented to him by the Indian Army in 2011. In June 2025, he was inducted into ICC Cricket Hall of Fame.[3]\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac9326ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hp\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-cased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.0026320188771933317,\n",
       " 'start': 120,\n",
       " 'end': 159,\n",
       " 'answer': 'right-handed batter and a wicket-keeper'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=d,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b4bd0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a =question_answerer(\n",
    "    question=\"when i born\",\n",
    "    context=d,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f1e4031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7 July 1981'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d729e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "def generate_story(topic):\n",
    "    prompt = f\"Write a short story about {topic}.\"\n",
    "    story = generator(prompt, max_length=200, do_sample=True, temperature=0.9)[0][\"generated_text\"]\n",
    "    return story\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cecb2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "story = generate_story(\"love story on robot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a short story about love story on robot. And if you're not sure what you want.\n",
      "\n",
      "A. A short story by the author, or a short story by the editor.\n",
      "\n",
      "I have to admit I was not expecting this.\n",
      "\n",
      "B. A short story by a person named, and I'll admit I was never happy with his choice of word choice. (I feel a lot of guilt about that.)\n",
      "\n",
      "C. A full story by a person named. It's a little different.\n",
      "\n",
      "You can choose a full story story style for the story length, and choose different words for it depending on which story you are. For example, if someone chose a story to be at 2 minutes, it will be 10 minutes for each story length. For stories with shorter stories, stories will be written in 3 minutes or less for each length.\n",
      "\n",
      "D. An artist's story with a story length of 20 minutes for a short story. It's a\n"
     ]
    }
   ],
   "source": [
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e850e792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this video course, Amit Diwan teaches Hugging Face. Hugging Face is a company and open-source community that focuses on NLP and AI.\n",
    "\n",
    "# The following lessons are covered in the Hugging Face course:\n",
    "# 00:00:00 About Course\n",
    "# 00:00:45 Hugging Face - Intro and Features\n",
    "# 00:04:07 Hugging Face - Use Cases\n",
    "# 00:11:09 Transformers Library of Hugging Face\n",
    "# 00:15:30 Datasets Library of Hugging Face\n",
    "# 00:20:39 Tokenizers Library of Hugging Face\n",
    "# 00:25:24 Hugging Face Access Token (API Key) & How to Create\n",
    "# 00:30:45 Download a dataset on Hugging Face\n",
    "# 00:33:57 Download a model from Hugging Face\n",
    "# 00:36:35 Sentiment Analysis using Hugging Face\n",
    "# 00:43:07 Text Classification using Hugging Face\n",
    "# 00:48:50 Text Summarizations using Hugging Face\n",
    "# 00:52:38 Text to Text (Translate) using Hugging Face\n",
    "# 00:56:20 Question Answering using Hugging Face\n",
    "# 00:58:57 Text to Image using Hugging Face\n",
    "# 01:03:07 Text to Video Synthesis using Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1a26b37",
   "metadata": {},
   "source": [
    "Great! Creating a **story generator project using Hugging Face** is a fantastic idea ‚Äî both educational and practical. I'll guide you step-by-step from **scratch to deployment**, including:\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Project: *AI Story Generator on Given Topic*\n",
    "\n",
    "### ‚úÖ Overview:\n",
    "\n",
    "> A web app that takes a user-given topic and generates a short creative story using Hugging Face Transformers.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Step-by-Step Plan\n",
    "\n",
    "### **1. Setup and Environment**\n",
    "\n",
    "* Use: `Python`, `Transformers`, `Streamlit` (or Flask), `HuggingFace models`.\n",
    "\n",
    "* Create a virtual environment:\n",
    "\n",
    "  ```bash\n",
    "  python -m venv storyenv\n",
    "  source storyenv/bin/activate  # Linux/Mac\n",
    "  storyenv\\Scripts\\activate     # Windows\n",
    "  ```\n",
    "\n",
    "* Install dependencies:\n",
    "\n",
    "  ```bash\n",
    "  pip install transformers streamlit torch\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Choose a Model from Hugging Face**\n",
    "\n",
    "Some good story-generation models:\n",
    "\n",
    "| Model Name                           | Description                                       |\n",
    "| ------------------------------------ | ------------------------------------------------- |\n",
    "| `gpt2`                               | Small, fast, decent quality                       |\n",
    "| `EleutherAI/gpt-neo-1.3B`            | High-quality open GPT-like model                  |\n",
    "| `microsoft/DialoGPT-medium`          | Good for dialogue, can be adapted                 |\n",
    "| `mistralai/Mistral-7B-Instruct-v0.1` | Advanced, for creative outputs (needs strong GPU) |\n",
    "\n",
    "For now, let‚Äôs use GPT-2 for simplicity.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Basic Python Script (Offline / API-Free)**\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "def generate_story(topic):\n",
    "    prompt = f\"Write a short story about {topic}.\"\n",
    "    story = generator(prompt, max_length=200, do_sample=True, temperature=0.9)[0][\"generated_text\"]\n",
    "    return story\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Streamlit Web App**\n",
    "\n",
    "Create a file `app.py`:\n",
    "\n",
    "```python\n",
    "import streamlit as st\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load model\n",
    "@st.cache_resource\n",
    "def load_generator():\n",
    "    return pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "generator = load_generator()\n",
    "\n",
    "# Streamlit UI\n",
    "st.title(\"üìù AI Story Generator\")\n",
    "topic = st.text_input(\"Enter a story topic:\")\n",
    "\n",
    "if st.button(\"Generate Story\"):\n",
    "    if topic:\n",
    "        with st.spinner(\"Generating your story...\"):\n",
    "            prompt = f\"Write a short story about {topic}.\"\n",
    "            story = generator(prompt, max_length=200, do_sample=True, temperature=0.9)[0]['generated_text']\n",
    "        st.success(\"Here's your story:\")\n",
    "        st.write(story)\n",
    "    else:\n",
    "        st.warning(\"Please enter a topic.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Run Locally**\n",
    "\n",
    "```bash\n",
    "streamlit run app.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Optional: Use Hugging Face Inference API (no GPU needed)**\n",
    "\n",
    "If you want to avoid loading models locally:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\", use_auth_token=True)\n",
    "```\n",
    "\n",
    "Or use:\n",
    "\n",
    "```bash\n",
    "pip install huggingface_hub\n",
    "```\n",
    "\n",
    "And fetch via:\n",
    "\n",
    "```python\n",
    "from huggingface_hub import InferenceClient\n",
    "client = InferenceClient()\n",
    "response = client.text_generation(prompt=\"Write a story about a dragon in space\")\n",
    "print(response)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Deployment (Free)**\n",
    "\n",
    "* Use **Streamlit Cloud**:\n",
    "\n",
    "  1. Push your app (`app.py`) and `requirements.txt` to GitHub.\n",
    "  2. Go to [streamlit.io/cloud](https://streamlit.io/cloud) and deploy the app.\n",
    "\n",
    "**requirements.txt**\n",
    "\n",
    "```txt\n",
    "transformers\n",
    "torch\n",
    "streamlit\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üî• Bonus Ideas\n",
    "\n",
    "* Add story length control (short, medium, long)\n",
    "* Let users choose tone: funny, scary, romantic, etc.\n",
    "* Use a dropdown to select model (GPT-2, GPT-Neo)\n",
    "* Save stories in a text file or allow download\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Ready-to-Build?\n",
    "\n",
    "Would you like:\n",
    "\n",
    "* üì¶ A starter GitHub repo?\n",
    "* üéì A mini course or assignment format for your students?\n",
    "* üí¨ Hindi/vernacular language support?\n",
    "\n",
    "Let me know ‚Äî I can generate and zip the project files for you too!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
