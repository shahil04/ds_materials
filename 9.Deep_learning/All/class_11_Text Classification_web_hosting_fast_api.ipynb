{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“˜ **Lesson Plan: Text Classification Using Email Spam Dataset**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ **1. Problem Statement**\n",
    "\n",
    "Classify email messages as **Spam** or **Not Spam (Ham)** using machine learning and deep learning techniques.\n",
    "We will use:\n",
    "\n",
    "* **CountVectorizer** and **TF-IDF** for feature extraction\n",
    "* ML models: **Naive Bayes**, **Logistic Regression**, **SVM**\n",
    "* **ANN** using Keras\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“¦ **Dataset Used**\n",
    "\n",
    "* Email spam dataset with two columns:\n",
    "\n",
    "  * `label`: 'spam' or 'ham'\n",
    "  * `text`: email content\n",
    "\n",
    "> Example dataset: [Kaggle Email Spam Dataset](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset)\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… **Steps for All Approaches**\n",
    "\n",
    "### **Step 1: Load & Preprocess Data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv(\"class_11_spam.csv\", encoding='latin-1')[['v1', 'v2']]\n",
    "df.columns = ['label', 'text']\n",
    "\n",
    "# Encode label\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['label'])  # ham = 0, spam = 1\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## ğŸ”¹ **2. CountVectorizer + ML Models**\n",
    "\n",
    "### Vectorization\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_cv = vectorizer.fit_transform(X_train)\n",
    "X_test_cv = vectorizer.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9838565022421525\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       965\n",
      "           1       0.99      0.89      0.94       150\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.98      0.95      0.96      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Classification\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_cv, y_train)\n",
    "y_pred = model.predict(X_test_cv)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9623318385650225\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ğŸ“Œ **Try with Logistic Regression and SVM as well.**\n",
    "\n",
    "\n",
    "## ğŸ”¹ **3. TF-IDF + ML Models**\n",
    "\n",
    "### TF-IDF Vectorization\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "### Classification (same as above)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## ğŸ”¹ **4. Text Classification using ANN (Keras)**\n",
    "\n",
    "### Preprocess using TF-IDF and Convert to Array\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "X_train_arr = X_train_tfidf.toarray()\n",
    "X_test_arr = X_test_tfidf.toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\.conda\\envs\\nlp\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.8342 - loss: 0.5842 - val_accuracy: 0.8987 - val_loss: 0.2784\n",
      "Epoch 2/5\n",
      "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.9269 - loss: 0.2271 - val_accuracy: 0.9722 - val_loss: 0.1204\n",
      "Epoch 3/5\n",
      "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.9823 - loss: 0.0837 - val_accuracy: 0.9794 - val_loss: 0.0793\n",
      "Epoch 4/5\n",
      "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.9938 - loss: 0.0448 - val_accuracy: 0.9785 - val_loss: 0.0677\n",
      "Epoch 5/5\n",
      "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.9969 - loss: 0.0247 - val_accuracy: 0.9803 - val_loss: 0.0629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x212383619a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Define ANN Model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train_arr.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train_arr, y_train, epochs=5, batch_size=64, validation_data=(X_test_arr, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Predict with Sample Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n",
      "Prediction: Spam (0.9854)\n"
     ]
    }
   ],
   "source": [
    "# Sample email text\n",
    "sample_email = [\"Congratulations! You've won a free iPhone. Click the link to claim now!\"]\n",
    "\n",
    "# Vectorize using the same TF-IDF vectorizer\n",
    "sample_tfidf = tfidf.transform(sample_email).toarray()\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(sample_tfidf)\n",
    "\n",
    "# Interpret result\n",
    "label = \"Spam\" if prediction[0][0] >= 0.5 else \"Ham\"\n",
    "print(f\"Prediction: {label} ({prediction[0][0]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step\n",
      "Email: Congratulations! You have won a lottery worth $1,000,000.\n",
      "Prediction: Spam (0.9313)\n",
      "\n",
      "Email: Hey, are we still meeting for lunch today?\n",
      "Prediction: Ham (0.0016)\n",
      "\n",
      "Email: Limited-time offer just for you! Get 90% off on your next purchase.\n",
      "Prediction: Ham (0.1808)\n",
      "\n",
      "Email: Please find the attached report for the project.\n",
      "Prediction: Ham (0.1389)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try with Multiple Samples\n",
    "sample_emails = [\n",
    "    \"Congratulations! You have won a lottery worth $1,000,000.\",\n",
    "    \"Hey, are we still meeting for lunch today?\",\n",
    "    \"Limited-time offer just for you! Get 90% off on your next purchase.\",\n",
    "    \"Please find the attached report for the project.\"\n",
    "]\n",
    "\n",
    "sample_tfidf = tfidf.transform(sample_emails).toarray()\n",
    "predictions = model.predict(sample_tfidf)\n",
    "\n",
    "for i, pred in enumerate(predictions):\n",
    "    label = \"Spam\" if pred[0] >= 0.5 else \"Ham\"\n",
    "    print(f\"Email: {sample_emails[i]}\\nPrediction: {label} ({pred[0]:.4f})\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… How to Save and Prepare the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(\"ann_spam_model.h5\")\n",
    "\n",
    "# Save TF-IDF vectorizer\n",
    "import pickle\n",
    "with open(\"tfidf_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tfidf, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Streamlit App: spam_detector_app.py\n",
    "\n",
    "âœ… How to Run Streamlit App\n",
    "\n",
    "streamlit run spam_detector_app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load trained model\n",
    "model = load_model(\"ann_spam_model.h5\")\n",
    "\n",
    "# Load saved TF-IDF vectorizer\n",
    "with open(\"tfidf_vectorizer.pkl\", \"rb\") as f:\n",
    "    tfidf = pickle.load(f)\n",
    "\n",
    "# Streamlit UI\n",
    "st.title(\"ğŸ“§ Email Spam Classifier\")\n",
    "st.write(\"Enter an email below to check if it's **Spam** or **Ham**\")\n",
    "\n",
    "email_text = st.text_area(\"âœ‰ï¸ Email Content\")\n",
    "\n",
    "if st.button(\"Predict\"):\n",
    "    if email_text.strip() == \"\":\n",
    "        st.warning(\"Please enter some email text.\")\n",
    "    else:\n",
    "        # Transform text\n",
    "        email_vector = tfidf.transform([email_text]).toarray()\n",
    "        \n",
    "        # Predict\n",
    "        prediction = model.predict(email_vector)\n",
    "        label = \"ğŸ›‘ Spam\" if prediction[0][0] >= 0.5 else \"âœ… Ham\"\n",
    "        confidence = prediction[0][0] if label == \"ğŸ›‘ Spam\" else 1 - prediction[0][0]\n",
    "        \n",
    "        st.subheader(\"ğŸ“Š Prediction Result\")\n",
    "        st.write(f\"**Prediction:** {label}\")\n",
    "        st.write(f\"**Confidence:** {confidence:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flask API: app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 2. Save Model and Vectorizer\n",
    "# After training in your main notebook:\n",
    "\n",
    "model.save(\"ann_spam_model.h5\")\n",
    "\n",
    "import pickle\n",
    "with open(\"tfidf_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tfidf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load model and vectorizer\n",
    "model = load_model('ann_spam_model.h5')\n",
    "\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    tfidf = pickle.load(f)\n",
    "\n",
    "@app.route('/', methods=['GET'])\n",
    "def index():\n",
    "    return jsonify({'message': 'Email Spam Detection API is running!'})\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.get_json()\n",
    "    \n",
    "    if 'email' not in data:\n",
    "        return jsonify({'error': 'No email content provided'}), 400\n",
    "\n",
    "    email_text = data['email']\n",
    "    \n",
    "    # Preprocess and predict\n",
    "    email_vector = tfidf.transform([email_text]).toarray()\n",
    "    prediction = model.predict(email_vector)\n",
    "    \n",
    "    label = 'Spam' if prediction[0][0] >= 0.5 else 'Ham'\n",
    "    confidence = float(prediction[0][0]) if label == \"Spam\" else float(1 - prediction[0][0])\n",
    "\n",
    "    return jsonify({\n",
    "        'prediction': label,\n",
    "        'confidence': round(confidence, 4)\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 3. Run the Flask API\n",
    "\n",
    "# pip install flask tensorflow scikit-learn\n",
    "# python app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## âœ… **4. Test the API (Using curl or Postman)**\n",
    "\n",
    "### Example `POST` request using **curl**:\n",
    "\n",
    "```bash\n",
    "curl -X POST http://127.0.0.1:5000/predict \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\"email\":\"Congratulations! You have won a free ticket. Click here!\"}'\n",
    "```\n",
    "\n",
    "### Sample Response:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"prediction\": \"Spam\",\n",
    "  \"confidence\": 0.9813\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load trained model and TFIDF vectorizer\n",
    "model = load_model('ann_spam_model.h5')\n",
    "\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    tfidf = pickle.load(f)\n",
    "\n",
    "# Request schema\n",
    "class EmailRequest(BaseModel):\n",
    "    email: str\n",
    "\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\"message\": \"Email Spam Detection API is running!\"}\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict_email(data: EmailRequest):\n",
    "    email_text = data.email\n",
    "    email_vector = tfidf.transform([email_text]).toarray()\n",
    "    prediction = model.predict(email_vector)\n",
    "\n",
    "    label = \"Spam\" if prediction[0][0] >= 0.5 else \"Ham\"\n",
    "    confidence = float(prediction[0][0]) if label == \"Spam\" else float(1 - prediction[0][0])\n",
    "\n",
    "    return {\n",
    "        \"prediction\": label,\n",
    "        \"confidence\": round(confidence, 4)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the **complete FastAPI code** for your **Email Spam Detection** project, including the API file and requirements.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“„ `main.py`\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load trained model and TFIDF vectorizer\n",
    "model = load_model('ann_spam_model.h5')\n",
    "\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    tfidf = pickle.load(f)\n",
    "\n",
    "# Request schema\n",
    "class EmailRequest(BaseModel):\n",
    "    email: str\n",
    "\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\"message\": \"Email Spam Detection API is running!\"}\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict_email(data: EmailRequest):\n",
    "    email_text = data.email\n",
    "    email_vector = tfidf.transform([email_text]).toarray()\n",
    "    prediction = model.predict(email_vector)\n",
    "\n",
    "    label = \"Spam\" if prediction[0][0] >= 0.5 else \"Ham\"\n",
    "    confidence = float(prediction[0][0]) if label == \"Spam\" else float(1 - prediction[0][0])\n",
    "\n",
    "    return {\n",
    "        \"prediction\": label,\n",
    "        \"confidence\": round(confidence, 4)\n",
    "    }\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“„ `requirements.txt`\n",
    "\n",
    "```txt\n",
    "fastapi\n",
    "uvicorn\n",
    "tensorflow\n",
    "scikit-learn\n",
    "numpy\n",
    "pydantic\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## â–¶ï¸ How to Run Locally\n",
    "\n",
    "1. Install dependencies:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "2. Start the FastAPI server:\n",
    "\n",
    "```bash\n",
    "uvicorn main:app --reload\n",
    "```\n",
    "\n",
    "3. Visit the API:\n",
    "\n",
    "* Root endpoint: [http://127.0.0.1:8000](http://127.0.0.1:8000)\n",
    "* Swagger UI: [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§ª Sample API Test (POST Request)\n",
    "\n",
    "```bash\n",
    "curl -X POST http://127.0.0.1:8000/predict \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\"email\": \"Congratulations! You have won a free vacation!\"}'\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like:\n",
    "\n",
    "* Help uploading this to **Railway**\n",
    "* A `README.md` for the GitHub repo\n",
    "* A version with **Gradio** or **Streamlit** interface for end users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your **Hugging Face Spaces-ready FastAPI project** is now packaged!\n",
    "ğŸ“¦ [Click here to download `email-spam-api-hf.zip`](sandbox:/mnt/data/email-spam-api-hf.zip)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Structure Inside the ZIP\n",
    "\n",
    "```\n",
    "email-spam-api-hf/\n",
    "â”œâ”€â”€ Dockerfile\n",
    "â”œâ”€â”€ README.md\n",
    "â””â”€â”€ app/\n",
    "    â”œâ”€â”€ main.py\n",
    "    â”œâ”€â”€ requirements.txt\n",
    "    â”œâ”€â”€ ann_spam_model.h5          â† Replace with your real model\n",
    "    â””â”€â”€ tfidf_vectorizer.pkl       â† Replace with your real vectorizer\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ How to Deploy to Hugging Face Spaces\n",
    "\n",
    "1. Go to [https://huggingface.co/spaces](https://huggingface.co/spaces)\n",
    "2. Click **\"Create new Space\"**\n",
    "3. Name your space and set:\n",
    "\n",
    "   * **SDK**: `Docker`\n",
    "   * **License**: `MIT` or other\n",
    "4. Upload the ZIP contents or push via GitHub\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want help:\n",
    "\n",
    "* Replacing dummy model/vectorizer with real ones\n",
    "* Pushing this as a GitHub repo before deploying\n",
    "* Creating a public Gradio version for demo testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
