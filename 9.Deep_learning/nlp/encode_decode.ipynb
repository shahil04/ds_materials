{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08348616",
   "metadata": {},
   "source": [
    "Great question!\n",
    "\n",
    "If you're already using an **LSTM**, you're halfway to building an **Encoder-Decoder** architecture. But here's the key:\n",
    "\n",
    "> **LSTM is the building block. Encoder-Decoder is the architecture built using multiple LSTMs.**\n",
    "\n",
    "Let me break it down for clarity:\n",
    "\n",
    "---\n",
    "\n",
    "### üîß What LSTM Does (Alone):\n",
    "\n",
    "An **LSTM** (Long Short-Term Memory) processes **a sequence** and produces **a sequence of hidden states**. It is great for handling temporal dependencies in sequences ‚Äî like time series or single-sentence analysis (e.g., sentiment).\n",
    "\n",
    "But:\n",
    "\n",
    "* It typically handles **one sequence** (input).\n",
    "* It outputs a single prediction (e.g., next word) or sequence (e.g., classification, regression).\n",
    "* It does **not** natively support **sequence-to-sequence transformation**.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† What Encoder-Decoder Does with LSTM:\n",
    "\n",
    "The **Encoder-Decoder** is a **higher-level design** that uses **two LSTMs**:\n",
    "\n",
    "1. **Encoder LSTM**:\n",
    "\n",
    "   * Reads the **entire input sequence**.\n",
    "   * Compresses the sequence into a **context vector** (last hidden and cell states).\n",
    "   * This vector captures the **meaning or summary** of the input.\n",
    "\n",
    "2. **Decoder LSTM**:\n",
    "\n",
    "   * Starts from the context vector.\n",
    "   * Generates **an output sequence**, one step at a time.\n",
    "   * Often starts with a special `<start>` token.\n",
    "\n",
    "‚û°Ô∏è So you‚Äôre going from:\n",
    "\n",
    "```\n",
    "Input: \"Nice to meet you\"\n",
    "Output: \"‡§Ü‡§™ ‡§∏‡•á ‡§Æ‡§ø‡§≤‡§ï‡§∞ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§≤‡§ó‡§æ\"\n",
    "```\n",
    "\n",
    "The Encoder summarizes the English sentence.\n",
    "The Decoder **uses that summary** to generate the Hindi translation.\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Analogy:\n",
    "\n",
    "Using a **single LSTM** is like reading a book and saying ‚ÄúI understood it.‚Äù\n",
    "\n",
    "Using **Encoder-Decoder with two LSTMs** is like reading a book in English (Encoder), and then retelling the same story in Hindi (Decoder) ‚Äî sentence by sentence.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Summary:\n",
    "\n",
    "| Component          | Purpose           | What It Does                           |\n",
    "| ------------------ | ----------------- | -------------------------------------- |\n",
    "| **LSTM**           | Sequence model    | Learns dependencies in sequences       |\n",
    "| **Encoder** (LSTM) | Understands input | Summarizes input into context vector   |\n",
    "| **Decoder** (LSTM) | Generates output  | Uses context to create output sequence |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e31fab",
   "metadata": {},
   "source": [
    "https://docs.google.com/document/d/10dHbjQo_EL_U6KVCsQ_eyYCtj_SMOhmiUv8Eat0DHMs/edit?tab=t.0 \n",
    "\n",
    "ENCODER- DECODER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49abc502",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "291f7c6b",
   "metadata": {},
   "source": [
    "### ‚úÖ **Basic Encoder-Decoder (Seq2Seq) Implementation in PyTorch**\n",
    "\n",
    "We‚Äôll:\n",
    "\n",
    "* Use LSTM-based Encoder and Decoder\n",
    "* Apply Teacher Forcing during training\n",
    "* Keep it simple and understandable\n",
    "\n",
    "---\n",
    "\n",
    "To proceed, I‚Äôll now give you a full working implementation step-by-step for a basic Seq2Seq Encoder-Decoder model with LSTMs in PyTorch using a toy English-to-Hindi dataset.\n",
    "\n",
    "‚úÖ Step-by-Step LSTM Encoder-Decoder Seq2Seq in PyTorch (Working Code + Use Case)\n",
    "üìå 1. Install Required Packages\n",
    "\n",
    "pip install torch torchtext\n",
    "---\n",
    "\n",
    "### üìä **Next Steps You Can Explore**\n",
    "\n",
    "* üîç Add **attention mechanism**\n",
    "* üéØ Add **BLEU score evaluation**\n",
    "* üìà Use **beam search** in decoding\n",
    "* üßæ Try the **English-Hindi dataset**\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to:\n",
    "\n",
    "1. Build this on a notebook with data pipeline and training?\n",
    "2. Add attention and explain it visually?\n",
    "3. Show inference code (how to generate translation)?\n",
    "\n",
    "Let me know what you'd like next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d664817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå 2. Prepare Dataset\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "data = [\n",
    "    (\"hello\", \"‡§®‡§Æ‡§∏‡•ç‡§§‡•á\"),\n",
    "    (\"how are you\", \"‡§Ü‡§™ ‡§ï‡•à‡§∏‡•á ‡§π‡•à‡§Ç\"),\n",
    "    (\"i am fine\", \"‡§Æ‡•à‡§Ç ‡§†‡•Ä‡§ï ‡§π‡•Ç‡§Å\"),\n",
    "    (\"thank you\", \"‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶\"),\n",
    "    (\"what is your name\", \"‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à\"),\n",
    "    (\"my name is john\", \"‡§Æ‡•á‡§∞‡§æ ‡§®‡§æ‡§Æ ‡§ú‡•â‡§® ‡§π‡•à\"),\n",
    "]\n",
    "\n",
    "# Tokenizers\n",
    "en_tokenizer = get_tokenizer(\"basic_english\")\n",
    "hi_tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data, tokenizer, idx):\n",
    "    for pair in data:\n",
    "        yield tokenizer(pair[idx])\n",
    "\n",
    "SRC_VOCAB = build_vocab_from_iterator(yield_tokens(data, en_tokenizer, 0), specials=[\"<pad>\", \"<sos>\", \"<eos>\"])\n",
    "TGT_VOCAB = build_vocab_from_iterator(yield_tokens(data, hi_tokenizer, 1), specials=[\"<pad>\", \"<sos>\", \"<eos>\"])\n",
    "\n",
    "SRC_VOCAB.set_default_index(SRC_VOCAB[\"<pad>\"])\n",
    "TGT_VOCAB.set_default_index(TGT_VOCAB[\"<pad>\"])\n",
    "\n",
    "def tensorize(pair):\n",
    "    src = [SRC_VOCAB[\"<sos>\"]] + [SRC_VOCAB[tok] for tok in en_tokenizer(pair[0])] + [SRC_VOCAB[\"<eos>\"]]\n",
    "    tgt = [TGT_VOCAB[\"<sos>\"]] + [TGT_VOCAB[tok] for tok in hi_tokenizer(pair[1])] + [TGT_VOCAB[\"<eos>\"]]\n",
    "    return torch.tensor(src), torch.tensor(tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7fe1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå 3. Create Encoder and Decoder\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.embedding(input)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b840b69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå 4. Seq2Seq Training Loop\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "INPUT_DIM = len(SRC_VOCAB)\n",
    "OUTPUT_DIM = len(TGT_VOCAB)\n",
    "HID_DIM = 256\n",
    "EMB_DIM = 128\n",
    "\n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM)\n",
    "dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "enc, dec = enc.to(device), dec.to(device)\n",
    "\n",
    "optimizer = optim.Adam(list(enc.parameters()) + list(dec.parameters()))\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TGT_VOCAB[\"<pad>\"])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    epoch_loss = 0\n",
    "    for src, tgt in map(tensorize, data):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        hidden, cell = enc(src.unsqueeze(1))  # [src_len, batch_size]\n",
    "        input = tgt[0]\n",
    "        loss = 0\n",
    "        for t in range(1, len(tgt)):\n",
    "            output, hidden, cell = dec(input, hidden, cell)\n",
    "            loss += criterion(output, tgt[t].unsqueeze(0))\n",
    "            input = tgt[t]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch} Loss: {epoch_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c23c1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå 5. Translate New Sentences (Inference)\n",
    "\n",
    "def translate_sentence(sentence):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "    tokens = [SRC_VOCAB[\"<sos>\"]] + [SRC_VOCAB[token] for token in en_tokenizer(sentence)] + [SRC_VOCAB[\"<eos>\"]]\n",
    "    src_tensor = torch.tensor(tokens).to(device).unsqueeze(1)\n",
    "    hidden, cell = enc(src_tensor)\n",
    "    input = torch.tensor([TGT_VOCAB[\"<sos>\"]]).to(device)\n",
    "\n",
    "    result = []\n",
    "    for _ in range(20):\n",
    "        output, hidden, cell = dec(input, hidden, cell)\n",
    "        top1 = output.argmax(1)\n",
    "        if top1.item() == TGT_VOCAB[\"<eos>\"]:\n",
    "            break\n",
    "        result.append(top1.item())\n",
    "        input = top1\n",
    "\n",
    "    translated = [TGT_VOCAB.get_itos()[idx] for idx in result]\n",
    "    return \" \".join(translated)\n",
    "\n",
    "# Example\n",
    "print(translate_sentence(\"hello\"))\n",
    "print(translate_sentence(\"what is your name\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be36340",
   "metadata": {},
   "source": [
    "‚úÖ Final Output (Sample)\n",
    "\n",
    "> hello\n",
    "‡§®‡§Æ‡§∏‡•ç‡§§‡•á\n",
    "\n",
    "> what is your name\n",
    "‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913778e2",
   "metadata": {},
   "source": [
    "The **Encoder-Decoder architecture** using RNNs, LSTMs, or GRUs for sequence-to-sequence tasks has been foundational in deep learning, but it comes with several **limitations**. Below are the key limitations and how modern architectures have overcome them:\n",
    "\n",
    "---\n",
    "\n",
    "### üö´ Limitations of the Encoder-Decoder Architecture\n",
    "\n",
    "#### 1. **Fixed-Size Context Vector (Information Bottleneck)**\n",
    "\n",
    "* **Problem**: The entire input sequence is compressed into a single vector (context vector) of fixed length, regardless of the input length.\n",
    "* **Impact**: For long input sequences, this fixed-size vector fails to capture all the necessary details, leading to poor performance, especially in long or complex sentences.\n",
    "* **Example**: Translating a long paragraph accurately becomes difficult because too much information is squeezed into one vector.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Difficulty with Long-Term Dependencies**\n",
    "\n",
    "* **Problem**: Even LSTM and GRU models, while better than vanilla RNNs, struggle with remembering dependencies that are far apart in the input.\n",
    "* **Impact**: Words at the beginning of a sentence may get \"forgotten\" by the time the context vector is produced.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Sequential Decoding**\n",
    "\n",
    "* **Problem**: The decoder generates one token at a time using the previous output, making it inherently sequential.\n",
    "* **Impact**: It cannot fully exploit GPU parallelization during inference, which slows down translation or other sequence generation tasks.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Lack of Interpretability**\n",
    "\n",
    "* **Problem**: There's no mechanism to understand which parts of the input contributed most to a particular output.\n",
    "* **Impact**: Makes the model a black box and harder to debug or explain, especially in sensitive applications.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Handling Variable-Length Inputs and Outputs**\n",
    "\n",
    "* **Problem**: Although LSTMs can technically handle variable lengths, aligning input-output pairs becomes difficult for complex tasks like summarization or question answering.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ How These Are Overcome\n",
    "\n",
    "Modern approaches build on the Encoder-Decoder idea but solve these problems using advanced mechanisms.\n",
    "\n",
    "---\n",
    "\n",
    "#### üß† 1. **Attention Mechanism**\n",
    "\n",
    "* **How it helps**:\n",
    "\n",
    "  * Instead of relying solely on a fixed-size context vector, **attention** allows the decoder to look at different parts of the input sequence for each output token.\n",
    "* **Benefit**:\n",
    "\n",
    "  * Dynamically computes context based on relevance at each step ‚Üí improves performance for long sequences.\n",
    "* **Introduced in**: [Bahdanau et al., 2014](https://arxiv.org/abs/1409.0473)\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö° 2. **Transformers**\n",
    "\n",
    "* **How it helps**:\n",
    "\n",
    "  * Replaces recurrence (LSTMs) with self-attention, allowing models to process the entire sequence simultaneously.\n",
    "* **Benefits**:\n",
    "\n",
    "  * No information bottleneck.\n",
    "  * Much faster to train and more parallelizable.\n",
    "  * Better at modeling long-range dependencies.\n",
    "* **Introduced in**: [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)\n",
    "* **Use Cases**: Used in BERT, GPT, T5, BART, and other state-of-the-art models.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîÅ 3. **Bidirectional Encoders**\n",
    "\n",
    "* **How it helps**:\n",
    "\n",
    "  * Processes the input sequence in both forward and backward directions to get richer context.\n",
    "* **Used in**: BERT and other Transformer-based models.\n",
    "\n",
    "---\n",
    "\n",
    "#### üß† 4. **Pre-trained Language Models**\n",
    "\n",
    "* **How it helps**:\n",
    "\n",
    "  * Leverages vast amounts of unlabelled text data for pretraining, then fine-tunes for specific tasks.\n",
    "* **Examples**:\n",
    "\n",
    "  * BERT (for encoding tasks), GPT (for generation), T5/BART (for Seq2Seq tasks)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary Table\n",
    "\n",
    "| Limitation                     | Solution                                  | Key Model          |\n",
    "| ------------------------------ | ----------------------------------------- | ------------------ |\n",
    "| Fixed-size context vector      | Attention mechanism                       | Bahdanau Attention |\n",
    "| Long-term dependencies         | Transformers & self-attention             | Transformer        |\n",
    "| Sequential decoding            | Transformers (non-recurrent architecture) | Transformer        |\n",
    "| Lack of interpretability       | Attention weights                         | Attention models   |\n",
    "| Poor scalability on long input | Transformer‚Äôs parallel processing         | GPT, BERT, T5      |\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
