{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4055622a",
   "metadata": {},
   "source": [
    "Creating **Word2Vec from scratch (without using `gensim`)** is absolutely possible and a great learning exercise to understand how word embeddings work under the hood.\n",
    "\n",
    "Hereâ€™s a simple, conceptual **implementation of Skip-Gram Word2Vec** from scratch using just **NumPy**, trained on your own text data.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Build Word2Vec (Skip-Gram) from Scratch using NumPy\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 1. Sample Dataset (Your Own Sentences)\n",
    "\n",
    "```python\n",
    "corpus = [\n",
    "    \"I love this phone\",\n",
    "    \"This camera is amazing\",\n",
    "    \"Battery life is great\",\n",
    "    \"The phone has excellent battery\",\n",
    "    \"I love the camera and battery\",\n",
    "    \"This phone is awful\",\n",
    "    \"I hate this battery\",\n",
    "    \"The camera is bad\"\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 2. Preprocess: Tokenization & Vocabulary\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Clean and tokenize\n",
    "def tokenize(sentences):\n",
    "    tokenized = []\n",
    "    for sent in sentences:\n",
    "        words = re.findall(r'\\b\\w+\\b', sent.lower())\n",
    "        tokenized.append(words)\n",
    "    return tokenized\n",
    "\n",
    "tokenized_corpus = tokenize(corpus)\n",
    "\n",
    "# Build vocabulary\n",
    "word2idx = {}\n",
    "idx2word = {}\n",
    "vocab = set([word for sent in tokenized_corpus for word in sent])\n",
    "for i, word in enumerate(vocab):\n",
    "    word2idx[word] = i\n",
    "    idx2word[i] = word\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocabulary:\", word2idx)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 3. Generate Skip-Gram Pairs\n",
    "\n",
    "```python\n",
    "def generate_skip_grams(tokenized, window_size=2):\n",
    "    pairs = []\n",
    "    for sent in tokenized:\n",
    "        for idx, word in enumerate(sent):\n",
    "            for offset in range(-window_size, window_size+1):\n",
    "                context_idx = idx + offset\n",
    "                if context_idx < 0 or context_idx >= len(sent) or offset == 0:\n",
    "                    continue\n",
    "                pairs.append((word, sent[context_idx]))\n",
    "    return pairs\n",
    "\n",
    "skip_gram_pairs = generate_skip_grams(tokenized_corpus)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 4. One-Hot Encoding\n",
    "\n",
    "```python\n",
    "def one_hot_vector(word, vocab_size, word2idx):\n",
    "    vec = np.zeros(vocab_size)\n",
    "    vec[word2idx[word]] = 1\n",
    "    return vec\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 5. Build the Word2Vec Model (Forward + Backward Pass)\n",
    "\n",
    "```python\n",
    "# Initialize weights\n",
    "embedding_dim = 10\n",
    "W1 = np.random.rand(vocab_size, embedding_dim)  # Input -> Hidden\n",
    "W2 = np.random.rand(embedding_dim, vocab_size)  # Hidden -> Output\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for center, context in skip_gram_pairs:\n",
    "        x = one_hot_vector(center, vocab_size, word2idx)  # input vector\n",
    "        y_true = one_hot_vector(context, vocab_size, word2idx)  # target\n",
    "\n",
    "        # Forward\n",
    "        h = np.dot(W1.T, x)\n",
    "        u = np.dot(W2.T, h)\n",
    "        y_pred = softmax(u)\n",
    "\n",
    "        # Loss (cross-entropy)\n",
    "        loss -= np.log(y_pred[np.argmax(y_true)])\n",
    "\n",
    "        # Backprop\n",
    "        e = y_pred - y_true\n",
    "        dW2 = np.outer(h, e)\n",
    "        dW1 = np.outer(x, np.dot(W2, e))\n",
    "\n",
    "        # Update weights\n",
    "        W1 -= learning_rate * dW1\n",
    "        W2 -= learning_rate * dW2\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 6. Get Word Vectors\n",
    "\n",
    "```python\n",
    "def get_embedding(word):\n",
    "    return W1[word2idx[word]]\n",
    "\n",
    "print(\"Vector for 'battery':\", get_embedding(\"battery\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 7. Find Similar Words (Cosine Similarity)\n",
    "\n",
    "```python\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "def most_similar(word, top_n=3):\n",
    "    target_vec = get_embedding(word)\n",
    "    similarities = {}\n",
    "    for other in vocab:\n",
    "        if other == word:\n",
    "            continue\n",
    "        sim = cosine_similarity(target_vec, get_embedding(other))\n",
    "        similarities[other] = sim\n",
    "    sorted_sim = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_sim[:top_n]\n",
    "\n",
    "print(\"Most similar to 'battery':\", most_similar(\"battery\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary\n",
    "\n",
    "| Step           | Description                          |\n",
    "| -------------- | ------------------------------------ |\n",
    "| Tokenize       | Clean and split your own data        |\n",
    "| Generate Pairs | Use Skip-Gram technique              |\n",
    "| Model          | Simple 1-hidden-layer NN using NumPy |\n",
    "| Train          | Use forward and backward pass        |\n",
    "| Use            | Get word vectors, find similar words |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like this code in a downloadable Jupyter Notebook format?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c77753",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73227485",
   "metadata": {},
   "source": [
    "Perfect! Let's now build a **Word2Vec model from scratch using an Artificial Neural Network (ANN)** â€” still using **only NumPy**, no libraries like `gensim` or `TensorFlow`. This ANN-based model is a **Skip-Gram Word2Vec**, where:\n",
    "\n",
    "* **Input**: One-hot vector for center word\n",
    "* **Hidden Layer**: Learns word embeddings\n",
    "* **Output**: Predicts context words via softmax\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Word2Vec Using ANN (from Scratch with NumPy)\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Step 1: Dataset & Tokenization\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Sample data (you can replace this with your own sentences)\n",
    "corpus = [\n",
    "    \"I love this phone\",\n",
    "    \"This camera is amazing\",\n",
    "    \"Battery life is great\",\n",
    "    \"I love the camera and battery\",\n",
    "    \"This phone is awful\",\n",
    "    \"I hate this battery\",\n",
    "    \"The camera is bad\"\n",
    "]\n",
    "\n",
    "# Clean & tokenize\n",
    "def tokenize(corpus):\n",
    "    tokenized = []\n",
    "    for sent in corpus:\n",
    "        words = re.findall(r'\\b\\w+\\b', sent.lower())\n",
    "        tokenized.append(words)\n",
    "    return tokenized\n",
    "\n",
    "tokenized_corpus = tokenize(corpus)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Step 2: Vocabulary & Encoding\n",
    "\n",
    "```python\n",
    "# Vocabulary\n",
    "vocab = sorted(set(word for sent in tokenized_corpus for word in sent))\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "vocab_size = len(vocab)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Step 3: Generate Skip-Gram Training Data\n",
    "\n",
    "```python\n",
    "def generate_training_data(tokenized, window_size=2):\n",
    "    training_data = []\n",
    "    for sentence in tokenized:\n",
    "        for idx, word in enumerate(sentence):\n",
    "            for offset in range(-window_size, window_size + 1):\n",
    "                context_idx = idx + offset\n",
    "                if context_idx < 0 or context_idx >= len(sentence) or offset == 0:\n",
    "                    continue\n",
    "                center = word2idx[word]\n",
    "                context = word2idx[sentence[context_idx]]\n",
    "                training_data.append((center, context))\n",
    "    return training_data\n",
    "\n",
    "training_pairs = generate_training_data(tokenized_corpus)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Step 4: One-Hot Encoding\n",
    "\n",
    "```python\n",
    "def one_hot(index, size):\n",
    "    vec = np.zeros(size)\n",
    "    vec[index] = 1\n",
    "    return vec\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Step 5: ANN Model (1 hidden layer)\n",
    "\n",
    "```python\n",
    "# Model params\n",
    "embedding_dim = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "# Initialize weights\n",
    "W1 = np.random.rand(vocab_size, embedding_dim)  # input -> hidden\n",
    "W2 = np.random.rand(embedding_dim, vocab_size)  # hidden -> output\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Step 6: Training Loop (Forward + Backpropagation)\n",
    "\n",
    "```python\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for center_idx, context_idx in training_pairs:\n",
    "        x = one_hot(center_idx, vocab_size)\n",
    "        y_true = one_hot(context_idx, vocab_size)\n",
    "\n",
    "        # Forward pass\n",
    "        h = np.dot(W1.T, x)              # hidden layer\n",
    "        u = np.dot(W2.T, h)              # output layer\n",
    "        y_pred = softmax(u)\n",
    "\n",
    "        # Loss (cross-entropy)\n",
    "        loss = -np.log(y_pred[context_idx])\n",
    "        total_loss += loss\n",
    "\n",
    "        # Backpropagation\n",
    "        e = y_pred - y_true\n",
    "        dW2 = np.outer(h, e)\n",
    "        dW1 = np.outer(x, np.dot(W2, e))\n",
    "\n",
    "        # Update weights\n",
    "        W1 -= learning_rate * dW1\n",
    "        W2 -= learning_rate * dW2\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Step 7: Word Embeddings & Similarity\n",
    "\n",
    "```python\n",
    "def get_embedding(word):\n",
    "    return W1[word2idx[word]]\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "def most_similar(word, top_n=3):\n",
    "    target_vec = get_embedding(word)\n",
    "    sims = {}\n",
    "    for other in vocab:\n",
    "        if other == word:\n",
    "            continue\n",
    "        sim = cosine_similarity(target_vec, get_embedding(other))\n",
    "        sims[other] = sim\n",
    "    return sorted(sims.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "print(\"Most similar to 'battery':\", most_similar(\"battery\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary\n",
    "\n",
    "| Component | Description                          |\n",
    "| --------- | ------------------------------------ |\n",
    "| Input     | One-hot encoded center word          |\n",
    "| Model     | ANN with one hidden layer            |\n",
    "| Output    | Softmax over context word prediction |\n",
    "| Embedding | Learned in weights `W1`              |\n",
    "| Use       | Get vector, find similar words       |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like this in a Jupyter Notebook or with plots for embedding visualization (e.g., t-SNE)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46d7eb9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
