{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b199149",
   "metadata": {},
   "source": [
    "## Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be8a238d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 0 1 1 0 1 2]\n",
      " [0 0 0 0 1 0 1 0 1 0 1 2]\n",
      " [1 1 0 1 0 1 0 0 0 1 0 0]]\n",
      "['and' 'are' 'cat' 'cats' 'dog' 'dogs' 'log' 'mat' 'on' 'pets' 'sat' 'the']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog sat on the log.\",\n",
    "    \"Cats and dogs are pets.\"\n",
    "]\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert to array and print\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70c4c548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.42755362 0.         0.         0.\n",
      "  0.         0.42755362 0.32516555 0.         0.32516555 0.6503311 ]\n",
      " [0.         0.         0.         0.         0.42755362 0.\n",
      "  0.42755362 0.         0.32516555 0.         0.32516555 0.6503311 ]\n",
      " [0.4472136  0.4472136  0.         0.4472136  0.         0.4472136\n",
      "  0.         0.         0.         0.4472136  0.         0.        ]]\n",
      "['and' 'are' 'cat' 'cats' 'dog' 'dogs' 'log' 'mat' 'on' 'pets' 'sat' 'the']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog sat on the log.\",\n",
    "    \"Cats and dogs are pets.\"\n",
    "]\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert to array and print\n",
    "print(X_tfidf.toarray())\n",
    "print(tfidf_vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1382ac82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\hp\\.conda\\envs\\nlp\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\hp\\.conda\\envs\\nlp\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\hp\\.conda\\envs\\nlp\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\hp\\.conda\\envs\\nlp\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\hp\\.conda\\envs\\nlp\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ae2e0be",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Python program to generate word vectors using Word2Vec\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# importing all necessary modules\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize, word_tokenize\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# Python program to generate word vectors using Word2Vec\n",
    "\n",
    "# importing all necessary modules\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "\n",
    "# Reads ‚Äòalice.txt‚Äô file\n",
    "sample = open(\"C:\\\\Users\\\\Admin\\\\Desktop\\\\alice.txt\")\n",
    "s = sample.read()\n",
    "\n",
    "# Replaces escape character with space\n",
    "f = s.replace(\"\\n\", \" \")\n",
    "\n",
    "data = []\n",
    "\n",
    "# iterate through each sentence in the file\n",
    "for i in sent_tokenize(f):\n",
    "\ttemp = []\n",
    "\n",
    "\t# tokenize the sentence into words\n",
    "\tfor j in word_tokenize(i):\n",
    "\t\ttemp.append(j.lower())\n",
    "\n",
    "\tdata.append(temp)\n",
    "\n",
    "# Create CBOW model\n",
    "model1 = gensim.models.Word2Vec(data, min_count=1,\n",
    "\t\t\t\t\t\t\t\tvector_size=100, window=5)\n",
    "\n",
    "# Print results\n",
    "print(\"Cosine similarity between 'alice' \" +\n",
    "\t\"and 'wonderland' - CBOW : \",\n",
    "\tmodel1.wv.similarity('alice', 'wonderland'))\n",
    "\n",
    "print(\"Cosine similarity between 'alice' \" +\n",
    "\t\"and 'machines' - CBOW : \",\n",
    "\tmodel1.wv.similarity('alice', 'machines'))\n",
    "\n",
    "# Create Skip Gram model\n",
    "model2 = gensim.models.Word2Vec(data, min_count=1, vector_size=100,\n",
    "\t\t\t\t\t\t\t\twindow=5, sg=1)\n",
    "\n",
    "# Print results\n",
    "print(\"Cosine similarity between 'alice' \" +\n",
    "\t\"and 'wonderland' - Skip Gram : \",\n",
    "\tmodel2.wv.similarity('alice', 'wonderland'))\n",
    "\n",
    "print(\"Cosine similarity between 'alice' \" +\n",
    "\t\"and 'machines' - Skip Gram : \",\n",
    "\tmodel2.wv.similarity('alice', 'machines'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a8c78a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e2d530c",
   "metadata": {},
   "source": [
    "Here's a complete **text classification project** that covers:\n",
    "\n",
    "* CountVectorization\n",
    "* TF-IDF Vectorization\n",
    "* Text Classification using ML (e.g., Logistic Regression, Naive Bayes)\n",
    "* Text Classification using ANN\n",
    "* With assignments\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Project Title: **Sentiment Analysis of Product Reviews**\n",
    "\n",
    "### üìÅ Dataset:\n",
    "\n",
    "Use the **Amazon Product Review** dataset or any CSV with `['review_text', 'label']`, where `label` is 0 (negative) or 1 (positive). You can use a Kaggle dataset or a mock dataset for teaching.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå PART 1: **Data Preprocessing**\n",
    "\n",
    "* Load dataset\n",
    "* Clean text: Lowercase, remove punctuation, stopwords, stemming/lemmatization\n",
    "* Split into train/test\n",
    "\n",
    "---\n",
    "\n",
    "### üìå PART 2: **Text Vectorization**\n",
    "\n",
    "#### A. CountVectorizer\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "X_test_cv = cv.transform(X_test)\n",
    "```\n",
    "\n",
    "#### B. TF-IDF Vectorizer\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìå PART 3: **Text Classification using ML**\n",
    "\n",
    "#### A. Logistic Regression (or try Naive Bayes, SVM)\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìå PART 4: **Text Classification using ANN**\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=X_train_tfidf.shape[1]))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train_tfidf.toarray(), y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60702d1d",
   "metadata": {},
   "source": [
    "Yes, absolutely! You **can train your own Word2Vec model** from scratch using your **own dataset** ‚Äî such as customer reviews, product descriptions, or any custom text corpus.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ When Should You Train Word2Vec on Your Own Data?\n",
    "\n",
    "* You have **domain-specific vocabulary** (e.g., legal, medical, retail).\n",
    "* Pre-trained embeddings (like Google News vectors) **miss important context** in your data.\n",
    "* You want **embeddings tailored** to your use case (e.g., customer sentiments, product names, slang).\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è How to Train Word2Vec on Your Own Data (Using `gensim`)\n",
    "\n",
    "#### 1. **Install Required Library**\n",
    "\n",
    "```bash\n",
    "pip install gensim\n",
    "```\n",
    "\n",
    "#### 2. **Prepare Your Text Corpus**\n",
    "\n",
    "Make sure your text is tokenized into sentences and words.\n",
    "\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Example: simple dataset\n",
    "text = \"\"\"I love this phone. The camera quality is amazing. Battery lasts all day.\"\"\"\n",
    "sentences = sent_tokenize(text)\n",
    "tokenized_data = [word_tokenize(sent.lower()) for sent in sentences]\n",
    "```\n",
    "\n",
    "#### 3. **Train Word2Vec Model**\n",
    "\n",
    "```python\n",
    "model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, workers=4)\n",
    "```\n",
    "\n",
    "#### 4. **Use the Model**\n",
    "\n",
    "```python\n",
    "# Get vector for word 'phone'\n",
    "vector = model.wv['phone']\n",
    "\n",
    "# Find most similar words\n",
    "similar = model.wv.most_similar('camera')\n",
    "```\n",
    "\n",
    "#### 5. **Save & Load Model**\n",
    "\n",
    "```python\n",
    "model.save(\"custom_word2vec.model\")\n",
    "# Later: model = Word2Vec.load(\"custom_word2vec.model\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Key Parameters\n",
    "\n",
    "| Parameter     | Description                                   |\n",
    "| ------------- | --------------------------------------------- |\n",
    "| `vector_size` | Dimensionality of word vectors                |\n",
    "| `window`      | Context window size                           |\n",
    "| `min_count`   | Ignores words with total frequency below this |\n",
    "| `workers`     | Threads for training                          |\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Example Use Cases:\n",
    "\n",
    "* Build a recommendation system using similarity between products\n",
    "* Cluster similar reviews\n",
    "* Visualize word relationships using t-SNE\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a complete Jupyter Notebook or a sample dataset to test this on?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
