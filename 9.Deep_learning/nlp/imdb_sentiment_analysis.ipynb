{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data \n",
    "df = pd.read_csv('IMDB.csv')\n",
    "df = df.sample(500)\n",
    "df.to_csv('data.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "\n",
    "# Define text preprocessing functions\n",
    "def lemmatization(text):\n",
    "    \"\"\"Lemmatize the text.\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = text.split()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    \"\"\"Remove stop words from the text.\"\"\"\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    text = [word for word in str(text).split() if word not in stop_words]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def removing_numbers(text):\n",
    "    \"\"\"Remove numbers from the text.\"\"\"\n",
    "    text = ''.join([char for char in text if not char.isdigit()])\n",
    "    return text\n",
    "\n",
    "def lower_case(text):\n",
    "    \"\"\"Convert text to lower case.\"\"\"\n",
    "    text = text.split()\n",
    "    text = [word.lower() for word in text]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def removing_punctuations(text):\n",
    "    \"\"\"Remove punctuations from the text.\"\"\"\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = text.replace('Ø›', \"\")\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def removing_urls(text):\n",
    "    \"\"\"Remove URLs from the text.\"\"\"\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "def normalize_text(df):\n",
    "    \"\"\"Normalize the text data.\"\"\"\n",
    "    try:\n",
    "        df['review'] = df['review'].apply(lower_case)\n",
    "        df['review'] = df['review'].apply(remove_stop_words)\n",
    "        df['review'] = df['review'].apply(removing_numbers)\n",
    "        df['review'] = df['review'].apply(removing_punctuations)\n",
    "        df['review'] = df['review'].apply(removing_urls)\n",
    "        df['review'] = df['review'].apply(lemmatization)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f'Error during text normalization: {e}')\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = normalize_text(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = df['sentiment'].isin(['positive','negative'])\n",
    "# df = df[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['sentiment'].map({'positive':1, 'negative':0})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change text data into number data\n",
    "vectorizer = CountVectorizer(max_features=100)\n",
    "X = vectorizer.fit_transform(df['review'])\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"review\"][651]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.toarray()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification problem \n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_predict = model.predict(X_test)\n",
    "\n",
    "x_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, x_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"review\"][799]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rahul = \"normally spike lee fan take time really get mojo see clear message ability tell story close heart lee genius unlike th hour bamboozled two favorite film his clear story film able understand struggle washington choice play well influenced others odd reason lee never able get true feeling out washington decent job handed him could tell lee s favorite film lee direct film also wrote it could tell camera work horrid writing contributed decay film film coming full circle going pretty lee behind film right thing film seen lee direct brightest modest film almost created hollywood movie instead one own know saw money right thing ran it film demonstrate true talent br br for anyone seen film perhaps stopped watching anything directed spike lee afterwards due film suggest give second chance get wrong see exactly coming film would want put behind you lee grow up work becomes own see transformation desire make money wanting make good film took awhile watch th hour did sheer brilliance perhaps actor perhaps story lee crafted amazing film one man s journey unknown guess hoping mo better blue would turn be really dark journey life man really never grew up instead got denzel denzel really one versatile actor generation consider sydney poitier cinema film showcase talent br br another issue film use spike s sister playing one love interest know you family think could filmed sex scene sister care actor much money getting paid would never it something never wish see apparently different spike went ahead showed full nude image sister without remorse sad even made blush also need somebody answer this flavor flav introducing film so sitting couch ready start film suddenly voice past spelling studio made film acknowledges himself build strong remaining story again felt lee going money film instead actual talent perhaps could afford denzel wesley movie without explosion br br there two great scene film made worth watching end get wrong bad movie always diamond every alleyway scene bleek accidentally forgets woman mesmerizing continually went back forth weaving truth confusion way proved lee actually behind camera visionary scene probably lost shuffle due remaining poor scene scene worth watching way lee introduced ended film keeping pacing direction able bring tragic character around full circle give chance change life two moment rest film pure rubbish worth viewing unless go blind br br grade'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'can t believe anyone would green light let alone voluntarily star it never able get min life back br br this one worst film ever seen film bad good gone far round that s somehow bad terrible exactly expecting much low budget bandwagon jumping rehash b movie still came way expectation level even tv movie higher production value br br there very poor special effect shocking dialogue terrible acting completely unexplained plot cursed why inch snake turn foot snake anyone ever heard highly venomous garter snake python passenger snake many promise none delivered br br some comment would believe film worth watching last five minute even worth rental stay watch low budget tv movie enjoy lot more br br why made oh yes shamelessly cash internet phenomenon soap shame mallachi brother shame'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"review\"][733]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Transform your input\n",
    "\n",
    "df1 = pd.DataFrame({'review': [rahul]})\n",
    "\n",
    "df11 = normalize_text(df1)  # Apply your preprocessing pipeline\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform using the same vectorizer\n",
    "X_input = vectorizer.transform(df1['review'])\n",
    "\n",
    "X_input.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Predict using your trained model\n",
    "prediction = model.predict(X_input)\n",
    "\n",
    "if prediction[0]==0:\n",
    "    print(rahul ,\"Negative review\")\n",
    "else:\n",
    "    print(rahul ,\"Positive review\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Predict using your trained model\n",
    "prediction = model.predict(X_input)\n",
    "\n",
    "if prediction[0]==0:\n",
    "    print(rahul[:20] ,\"Negative review\")\n",
    "else:\n",
    "    print(rahul[:20] ,\"Positive review\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# model = LogisticRegression(max_iter=1000)  # Increase max_iter to prevent non-convergence issues\n",
    "\n",
    "# model.fit(X_train, y_train)\n",
    "# # \n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Evaluations matics\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# precision = precision_score(y_test, y_pred)\n",
    "# recall = recall_score(y_test, y_pred)\n",
    "# f1 = f1_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save  vectorizer and model \n",
    "import pickle\n",
    "with open(\"vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "with open(\"model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the file\n",
    "with open(\"vectorizer.pkl\",\"rb\") as f:\n",
    "    vect = pickle.load(f)\n",
    "\n",
    "with open(\"model.pkl\",\"rb\") as f:\n",
    "    mod = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aznan = \"normally spike lee fan take time really get mojo see clear message ability tell story close heart lee genius unlike th hour bamboozled two favorite film his clear story film able understand struggle washington choice play well influenced others odd reason lee never able get true feeling out washington decent job handed him could tell lee s favorite film lee direct film also wrote it could tell camera work horrid writing contributed decay film film coming full circle going pretty lee behind film right thing film seen lee direct brightest modest film almost created hollywood movie instead one own know saw money right thing ran it film demonstrate true talent br br for anyone seen film perhaps stopped watching anything directed spike lee afterwards due film suggest give second chance get wrong see exactly coming film would want put behind you lee grow up work becomes own see transformation desire make money wanting make good film took awhile watch th hour did sheer brilliance perhaps actor perhaps story lee crafted amazing film one man s journey unknown guess hoping mo better blue would turn be really dark journey life man really never grew up instead got denzel denzel really one versatile actor generation consider sydney poitier cinema film showcase talent br br another issue film use spike s sister playing one love interest know you family think could filmed sex scene sister care actor much money getting paid would never it something never wish see apparently different spike went ahead showed full nude image sister without remorse sad even made blush also need somebody answer this flavor flav introducing film so sitting couch ready start film suddenly voice past spelling studio made film acknowledges himself build strong remaining story again felt lee going money film instead actual talent perhaps could afford denzel wesley movie without explosion br br there two great scene film made worth watching end get wrong bad movie always diamond every alleyway scene bleek accidentally forgets woman mesmerizing continually went back forth weaving truth confusion way proved lee actually behind camera visionary scene probably lost shuffle due remaining poor scene scene worth watching way lee introduced ended film keeping pacing direction able bring tragic character around full circle give chance change life two moment rest film pure rubbish worth viewing unless go blind br br grade'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Transform your input\n",
    "\n",
    "df1 = pd.DataFrame({'review': [aznan]})\n",
    "\n",
    "df11 = normalize_text(df1)  # Apply your preprocessing pipeline\n",
    "df1\n",
    "# Transform using the same vectorizer\n",
    "X_input = vect.transform(df1['review'])\n",
    "\n",
    "X_input.toarray()\n",
    "X_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normally spike lee f :--> Positive review\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Predict using your trained model\n",
    "prediction = mod.predict(X_input)\n",
    "\n",
    "if prediction[0]==0:\n",
    "    print(aznan[:20] ,\":--> Negative review\")\n",
    "else:\n",
    "    print(aznan[:20] ,\":--> Positive review\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also use ML models MultinomialNB ,LogisticRegression,LinearSVC,DecisionTreeClassifier,ensemble\n",
    "# Also we use Deep Learning/ use Neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "â”œâ”€â”€ app.py\n",
    "â”œâ”€â”€ requirements.txt\n",
    "â”œâ”€â”€ model.pkl\n",
    "â”œâ”€â”€ vectorizer.pkl\n",
    "|-- Templates --> html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip freeze -r requirements.txt\n",
    "\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define ANN Model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlopss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
