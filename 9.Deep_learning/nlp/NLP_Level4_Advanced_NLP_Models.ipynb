{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aca05e39",
   "metadata": {},
   "source": [
    "# 📘 Level 4: Advanced NLP Models\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Word Embeddings (Word2Vec, GloVe)\n",
    "\n",
    "### ➔ Definition:  \n",
    "Word embeddings convert words into **dense vector representations** where similar words are **closer** in the vector space.\n",
    "\n",
    "### ➔ Why use Word Embeddings?  \n",
    "- Captures **semantic meaning** (king - man + woman ≈ queen).  \n",
    "- Improves model understanding compared to simple one-hot encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b39e483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'nlp': [-5.3622725e-04  2.3643136e-04  5.1033497e-03  9.0092728e-03\n",
      " -9.3029495e-03 -7.1168090e-03  6.4588725e-03  8.9729885e-03\n",
      " -5.0154282e-03 -3.7633716e-03  7.3805046e-03 -1.5334714e-03\n",
      " -4.5366134e-03  6.5540518e-03 -4.8601604e-03 -1.8160177e-03\n",
      "  2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488179e-03\n",
      "  7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n",
      "  6.3508903e-03 -3.4053659e-03 -9.4640139e-04  5.7685734e-03\n",
      " -7.5216377e-03 -3.9361035e-03 -7.5115822e-03 -9.3004224e-04\n",
      "  9.5381187e-03 -7.3191668e-03 -2.3337686e-03 -1.9377411e-03\n",
      "  8.0774371e-03 -5.9308959e-03  4.5162440e-05 -4.7537340e-03\n",
      " -9.6035507e-03  5.0072931e-03 -8.7595852e-03 -4.3918253e-03\n",
      " -3.5099984e-05 -2.9618145e-04 -7.6612402e-03  9.6147433e-03\n",
      "  4.9820580e-03  9.2331432e-03 -8.1579173e-03  4.4957981e-03\n",
      " -4.1370760e-03  8.2453608e-04  8.4986202e-03 -4.4621765e-03\n",
      "  4.5175003e-03 -6.7869602e-03 -3.5484887e-03  9.3985079e-03\n",
      " -1.5776526e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n",
      " -1.5080082e-03  2.4697948e-03 -8.8802696e-04  5.5336617e-03\n",
      " -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459532e-03\n",
      " -1.4537406e-03 -9.2081428e-03  4.3705525e-03  5.7178497e-04\n",
      "  7.4419081e-03 -8.1328274e-04 -2.6384138e-03 -8.7530091e-03\n",
      " -8.5655687e-04  2.8265631e-03  5.4014288e-03  7.0526563e-03\n",
      " -5.7031214e-03  1.8588197e-03  6.0888636e-03 -4.7980510e-03\n",
      " -3.1072604e-03  6.7976294e-03  1.6314756e-03  1.8991709e-04\n",
      "  3.4736372e-03  2.1777749e-04  9.6188262e-03  5.0606038e-03\n",
      " -8.9173904e-03 -7.0415605e-03  9.0145587e-04  6.3925339e-03]\n",
      "Similar words to 'nlp': [('love', 0.016134681180119514), ('i', -0.01083916611969471), ('fun', -0.02775035798549652), ('learning', -0.05234673246741295), ('is', -0.059876296669244766), ('enjoy', -0.111670583486557)]\n"
     ]
    }
   ],
   "source": [
    "## Code for Word2Vec:\n",
    "\n",
    "# Install gensim if not installed\n",
    "# pip install gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample corpus\n",
    "sentences = [[\"i\", \"love\", \"nlp\"], [\"nlp\", \"is\", \"fun\"], [\"i\", \"enjoy\", \"learning\", \"nlp\"]]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Get vector of a word\n",
    "vector = model.wv['nlp']\n",
    "print(\"Vector for 'nlp':\", vector)\n",
    "\n",
    "# Find similar words\n",
    "similar = model.wv.most_similar('nlp')\n",
    "print(\"Similar words to 'nlp':\", similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ae4cea",
   "metadata": {},
   "source": [
    "✅ **Explanation:**  \n",
    "- `vector_size=100`: Output dimension.  \n",
    "- `window=5`: Context window size.  \n",
    "- `min_count=1`: Minimum count of word to consider.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Transformer Architecture\n",
    "\n",
    "### ➔ Definition:  \n",
    "Transformer is a model architecture based on **self-attention mechanisms**, **without RNNs**.\n",
    "\n",
    "### ➔ Why use Transformers?  \n",
    "- Handle **long dependencies** better.  \n",
    "- Train **faster** than RNNs on large data.\n",
    "\n",
    "---\n",
    "\n",
    "## Basic Structure (No coding here, just theory):\n",
    "\n",
    "| Encoder | Decoder |\n",
    "|:-------:|:-------:|\n",
    "| Self Attention | Masked Self Attention |\n",
    "| Feed Forward | Feed Forward |\n",
    "| Add & Norm | Add & Norm |\n",
    "\n",
    "✅ **Self-Attention** allows the model to **focus on different parts** of the input for each word.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Pre-trained Models (BERT, GPT)\n",
    "\n",
    "### ➔ Definition:  \n",
    "Pre-trained models like **BERT** and **GPT** are trained on large datasets and **fine-tuned** for specific tasks.\n",
    "\n",
    "### ➔ Why use Pre-trained Models?  \n",
    "- Save time and compute.  \n",
    "- Achieve **state-of-the-art performance** quickly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0af07e",
   "metadata": {},
   "source": [
    "Downgrade TensorFlow to 2.12.0 or 2.13.0\n",
    "(Because TensorFlow 2.12 uses Keras 2.x, fully compatible with transformers.)\n",
    "\n",
    "Run these commands in your environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23310ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall keras tensorflow\n",
    "pip install tensorflow==2.12.0\n",
    "pip install transformers\n",
    "pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fddb011c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\.conda\\envs\\nlp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\hp\\.conda\\envs\\nlp\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9988333582878113}]\n"
     ]
    }
   ],
   "source": [
    "## Code for using BERT (Text Classification):\n",
    "\n",
    "# Install transformers library if not installed\n",
    "# pip install transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load sentiment-analysis pipeline\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "\n",
    "# Sample text\n",
    "result = classifier(\"I love working with NLP models!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef617066",
   "metadata": {},
   "source": [
    "✅ **Explanation:**  \n",
    "- `pipeline('sentiment-analysis')`: Ready-to-use BERT under the hood.  \n",
    "- Takes **any text** and returns **positive** or **negative**.\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Text Generation with GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a83214bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time the sky was a sea of a thousand miles wide. This was the night sky, and as you entered the great night, each round of it was a little room of infinite darkness with the same colors as the old night sky.\n"
     ]
    }
   ],
   "source": [
    "## Code for GPT-2 Text Generation:\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load text generation pipeline\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "# Generate text\n",
    "result = generator(\"Once upon a time\", max_length=50, num_return_sequences=1)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abebfc29",
   "metadata": {},
   "source": [
    "✅ **Explanation:**  \n",
    "- Start with a **prompt** and GPT-2 **continues the story**.  \n",
    "- `max_length=50`: Limits how long the generated text is.\n",
    "\n",
    "---\n",
    "\n",
    "# 📚 Mini Assignments\n",
    "\n",
    "➔ 1. Train a Word2Vec model on your own dataset.  \n",
    "➔ 2. Use HuggingFace BERT for text classification on movie reviews.  \n",
    "➔ 3. Generate your own stories using GPT-2.\n",
    "\n",
    "---\n",
    "\n",
    "# ✅ Done!\n",
    "\n",
    "---\n",
    "\n",
    "# 📜 Summary of Topics Covered so far:\n",
    "\n",
    "| Level | Topics |\n",
    "|:----|:------|\n",
    "| Level 1 | Text Preprocessing (Tokenization, Stopwords, etc.) |\n",
    "| Level 2 | Basic Models (BoW, TF-IDF, Naive Bayes) |\n",
    "| Level 3 | Intermediate Models (NER, Language Modeling, Summarization) |\n",
    "| Level 4 | Advanced Models (Word2Vec, Transformer, BERT, GPT) |\n",
    "\n",
    "---\n",
    "\n",
    "✅ Now your NLP foundation is **solid** from basic to advanced!  \n",
    "If you want, next we can also cover:\n",
    "\n",
    "- ✍️ How to **Fine-tune BERT or GPT models** yourself  \n",
    "- 📦 How to **Deploy NLP apps** (Flask, Streamlit, Gradio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8534ff3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
