{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e708511b",
   "metadata": {},
   "source": [
    "## ✅ **Project Overview: Character-Level Text Generation using RNN**\n",
    "\n",
    "### 🧾 **1. Title**:\n",
    "\n",
    "**\"Character-Level Text Generator using RNN on Shakespeare Dataset\"**\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 **2. Project Goal**:\n",
    "\n",
    "To build and train a **Recurrent Neural Network (RNN)** that learns to predict and generate text character by character in the **style of Shakespeare's writing** using the **Tiny Shakespeare dataset** from TensorFlow Datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### 🌍 **3. Real-World Use Cases**:\n",
    "\n",
    "| Use Case                           | Description                                                                    |\n",
    "| ---------------------------------- | ------------------------------------------------------------------------------ |\n",
    "| 📝 **Text Autocomplete**           | Helps in predicting the next word/character in search engines or writing apps. |\n",
    "| 🎭 **Creative Writing Assistance** | Assists authors and screenwriters by generating stylistically consistent text. |\n",
    "| 🗣️ **Dialogue Generation**        | Can be used in chatbots or gaming NPC dialogue generation.                     |\n",
    "| 📚 **Language Modeling**           | Core of many NLP applications like GPT, BERT, etc.                             |\n",
    "| 🤖 **AI Poets / Novelists**        | Used to create AI-generated poetry or literature.                              |\n",
    "\n",
    "---\n",
    "\n",
    "### 🧱 **4. Tools & Frameworks**:\n",
    "\n",
    "* **TensorFlow / Keras**: For model creation and training\n",
    "* **TensorFlow Datasets (TFDS)**: For loading ready-to-use datasets\n",
    "* **Python**: Primary coding language\n",
    "* **NumPy**: For data manipulation\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 **5. Deep Learning Technique Used**:\n",
    "\n",
    "* **Recurrent Neural Networks (RNN)** – specifically **LSTM** for handling long-term dependencies in sequential data.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ **6. Step-by-Step Pipeline**\n",
    "\n",
    "| Step                        | Description                                                                |\n",
    "| --------------------------- | -------------------------------------------------------------------------- |\n",
    "| 1️⃣ **Dataset Loading**     | Load `tiny_shakespeare` from `tensorflow_datasets`.                        |\n",
    "| 2️⃣ **Data Preprocessing**  | Tokenize text, encode characters into integers, create input-output pairs. |\n",
    "| 3️⃣ **Sequence Generation** | Split into fixed-length sequences and targets.                             |\n",
    "| 4️⃣ **Model Building**      | Use Embedding layer + LSTM + Dense layer to predict next character.        |\n",
    "| 5️⃣ **Training**            | Train the model to minimize loss using `sparse_categorical_crossentropy`.  |\n",
    "| 6️⃣ **Text Generation**     | Feed a seed string and let the model generate text character-by-character. |\n",
    "| 7️⃣ **Fine-Tuning**         | Tune sequence length, embedding size, LSTM units, learning rate.           |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 **7. Fine-Tuning Steps**\n",
    "\n",
    "| Area                    | How to Fine-Tune                                                     |\n",
    "| ----------------------- | -------------------------------------------------------------------- |\n",
    "| 📏 **Sequence Length**  | Try different input sequence lengths (50–200).                       |\n",
    "| 🔠 **Vocabulary Size**  | Prune or enrich vocabulary depending on dataset size.                |\n",
    "| 🔧 **LSTM Units**       | Increase units from 128 to 1024 for better long-term memory.         |\n",
    "| ⏱️ **Training Epochs**  | More epochs = better fit (with early stopping to avoid overfitting). |\n",
    "| 🚫 **Overfitting**      | Add `Dropout` layers and use validation split.                       |\n",
    "| ⚖️ **Loss & Optimizer** | Try `Adam`, `RMSprop` optimizers and tune learning rate.             |\n",
    "\n",
    "---\n",
    "\n",
    "### ⚠️ **8. Challenges and How to Overcome**\n",
    "\n",
    "| Challenge               | Explanation                            | Solution                                       |\n",
    "| ----------------------- | -------------------------------------- | ---------------------------------------------- |\n",
    "| 🔁 Long-term Dependency | RNNs forget earlier context            | Use **LSTM** or **GRU**                        |\n",
    "| 🐢 Slow Training        | Training on character-level is slow    | Use batch processing, GPU                      |\n",
    "| 🧠 Overfitting          | Small dataset may lead to memorization | Use dropout, early stopping                    |\n",
    "| 📈 Vanishing Gradient   | Common in vanilla RNN                  | Replace with **LSTM**                          |\n",
    "| 🔤 Prediction Drift     | Generated text may become gibberish    | Adjust **temperature**, increase training time |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **9. Output Example**\n",
    "\n",
    "```plaintext\n",
    "ROMEO: What light through yonder window breaks?\n",
    "It is the east, and Juliet is the sun.\n",
    "...\n",
    "```\n",
    "\n",
    "Generated entirely by the model after training!\n",
    "\n",
    "---\n",
    "\n",
    "### 📂 **10. Extensions**\n",
    "\n",
    "* Switch to **word-level** generation\n",
    "* Use **transformers** (e.g., GPT-style)\n",
    "* Train on **custom corpora** (e.g., your own writing)\n",
    "* Deploy using **Flask** or **Streamlit**\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a downloadable **notebook version (.ipynb)** of this complete project or a version using **PyTorch + Hugging Face Datasets**?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d26438",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "853da7e8",
   "metadata": {},
   "source": [
    "Here's a **complete implementation of a Recurrent Neural Network (RNN) from scratch using only NumPy**, without relying on high-level deep learning libraries like TensorFlow or PyTorch.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 **Goal**\n",
    "\n",
    "Build a basic **character-level RNN** that learns to predict the next character in a sequence (e.g., \"hello\" → \"elloh\").\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ **Step-by-Step RNN from Scratch in Python (Using NumPy)**\n",
    "\n",
    "### 📦 **1. Import Required Libraries**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 📚 **2. Sample Data & Preprocessing**\n",
    "\n",
    "```python\n",
    "data = \"hello world\"\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "# Create mappings\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 100   # size of hidden layer\n",
    "seq_length = 5      # number of steps to unroll the RNN\n",
    "learning_rate = 1e-1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 **3. Model Parameters Initialization**\n",
    "\n",
    "```python\n",
    "# Weight matrices\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01  # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01  # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01  # hidden to output\n",
    "\n",
    "# Bias vectors\n",
    "bh = np.zeros((hidden_size, 1))  # hidden bias\n",
    "by = np.zeros((vocab_size, 1))   # output bias\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 **4. Forward and Backward Pass**\n",
    "\n",
    "```python\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs, targets are list of integers.\n",
    "    hprev is Hx1 array of initial hidden state.\n",
    "    returns the loss, gradients, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "\n",
    "    # Forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1))  # one-hot\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)\n",
    "        ys[t] = np.dot(Why, hs[t]) + by\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))  # softmax\n",
    "        loss += -np.log(ps[t][targets[t],0])  # cross-entropy loss\n",
    "\n",
    "    # Backward pass: compute gradients\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1  # gradient of softmax+cross entropy\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext  # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh  # backprop through tanh\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)  # clip to mitigate exploding gradients\n",
    "\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 **5. Training the RNN**\n",
    "\n",
    "```python\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by)\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length  # loss at iteration 0\n",
    "\n",
    "while n <= 1000:\n",
    "    # Prepare inputs\n",
    "    if p + seq_length + 1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((hidden_size,1))  # reset RNN memory\n",
    "        p = 0  # go from start of data\n",
    "\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # Forward and backward pass\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    if n % 100 == 0:\n",
    "        print(f\"Iter {n}, loss: {smooth_loss:.4f}\")\n",
    "\n",
    "    # Update weights with Adagrad\n",
    "    for param, dparam, mem in zip(\n",
    "        [Wxh, Whh, Why, bh, by],\n",
    "        [dWxh, dWhh, dWhy, dbh, dby],\n",
    "        [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / (np.sqrt(mem) + 1e-8)\n",
    "\n",
    "    p += seq_length\n",
    "    n += 1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 **6. Sample Text from the Model**\n",
    "\n",
    "```python\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\" \n",
    "    Sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "    return txt\n",
    "\n",
    "print(\"Sample: \", sample(hprev, char_to_ix['h'], 10))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **Advantages**\n",
    "\n",
    "| Benefit       | Description                                            |\n",
    "| ------------- | ------------------------------------------------------ |\n",
    "| Full control  | You learn the internals of RNN step-by-step            |\n",
    "| Customization | Easy to change activation functions, input sizes, etc. |\n",
    "| Lightweight   | No dependency on heavy frameworks                      |\n",
    "\n",
    "---\n",
    "\n",
    "## ❌ **Disadvantages & How to Overcome**\n",
    "\n",
    "| Disadvantage       | Solution                                                              |\n",
    "| ------------------ | --------------------------------------------------------------------- |\n",
    "| Slow Training      | Use optimized libraries (e.g., PyTorch) after understanding internals |\n",
    "| Difficult to Scale | Move to LSTM/GRU for long sequences                                   |\n",
    "| No GPU Support     | Switch to PyTorch/TensorFlow for hardware acceleration                |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to help convert this into a **modular class-based implementation**, or add **support for LSTM** or **train on real datasets (like Shakespeare or text8)?**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
