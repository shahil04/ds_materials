{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📘 **Lesson Plan: Text Classification Using Email Spam Dataset**\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **1. Problem Statement**\n",
    "\n",
    "Classify email messages as **Spam** or **Not Spam (Ham)** using machine learning and deep learning techniques.\n",
    "We will use:\n",
    "\n",
    "* **CountVectorizer** and **TF-IDF** for feature extraction\n",
    "* ML models: **Naive Bayes**, **Logistic Regression**, **SVM**\n",
    "* **ANN** using Keras\n",
    "\n",
    "---\n",
    "\n",
    "### 📦 **Dataset Used**\n",
    "\n",
    "* Email spam dataset with two columns:\n",
    "\n",
    "  * `label`: 'spam' or 'ham'\n",
    "  * `text`: email content\n",
    "\n",
    "> Example dataset: [Kaggle Email Spam Dataset](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset)\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **Steps for All Approaches**\n",
    "\n",
    "### **Step 1: Load & Preprocess Data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv(\"class_11_spam.csv\", encoding='latin-1')[['v1', 'v2']]\n",
    "df.columns = ['label', 'text']\n",
    "\n",
    "# Encode label\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['label'])  # ham = 0, spam = 1\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 🔹 **2. CountVectorizer + ML Models**\n",
    "\n",
    "### Vectorization\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_cv = vectorizer.fit_transform(X_train)\n",
    "X_test_cv = vectorizer.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9838565022421525\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       965\n",
      "           1       0.99      0.89      0.94       150\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.98      0.95      0.96      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Classification\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_cv, y_train)\n",
    "y_pred = model.predict(X_test_cv)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9623318385650225\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 📌 **Try with Logistic Regression and SVM as well.**\n",
    "\n",
    "\n",
    "## 🔹 **3. TF-IDF + ML Models**\n",
    "\n",
    "### TF-IDF Vectorization\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "### Classification (same as above)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Naive Bayes Classifier\n",
    "gnb = GaussianNB() \n",
    "%time gnb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = gnb.predict(X_train)\n",
    "y_pred_test = gnb.predict(X_test)\n",
    "print(\"\\nTraining Accuracy score:\",accuracy_score(y_train, y_pred_train))\n",
    "print(\"Testing Accuracy score:\",accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_test, target_names=['not relevant', 'relevant']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "# print('Confusion matrix\\n', cm)\n",
    "\n",
    "cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive', 'Actual Negative'], \n",
    "                        index=['Predict Positive', 'Predict Negative'])\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also use MultinomialNB ,LogisticRegression,LinearSVC,DecisionTreeClassifier,ensemble\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "classifiers = [('Decision Tree', dt),\n",
    "               ('Logistic Regression', lr),\n",
    "                ('Naive Bayes', gnb)\n",
    "              ]\n",
    "vc = VotingClassifier(estimators=classifiers)\n",
    "# Fit 'vc' to the traing set and predict test set labels\n",
    "vc.fit(X_train, y_train)\n",
    "y_pred_train=vc.predict(X_train)\n",
    "y_pred_test = vc.predict(X_test)\n",
    "print(\"Training Accuracy score:\",accuracy_score(y_train, y_pred_train))\n",
    "print(\"Testing Accuracy score:\",accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = gnb.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## 🔹 **4. Text Classification using ANN (Keras)**\n",
    "\n",
    "### Preprocess using TF-IDF and Convert to Array\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "X_train_arr = X_train_tfidf.toarray()\n",
    "X_test_arr = X_test_tfidf.toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\.conda\\envs\\nlp\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.8342 - loss: 0.5842 - val_accuracy: 0.8987 - val_loss: 0.2784\n",
      "Epoch 2/5\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.9269 - loss: 0.2271 - val_accuracy: 0.9722 - val_loss: 0.1204\n",
      "Epoch 3/5\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.9823 - loss: 0.0837 - val_accuracy: 0.9794 - val_loss: 0.0793\n",
      "Epoch 4/5\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.9938 - loss: 0.0448 - val_accuracy: 0.9785 - val_loss: 0.0677\n",
      "Epoch 5/5\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.9969 - loss: 0.0247 - val_accuracy: 0.9803 - val_loss: 0.0629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x212383619a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Define ANN Model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train_arr.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train_arr, y_train, epochs=5, batch_size=64, validation_data=(X_test_arr, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Predict with Sample Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n",
      "Prediction: Spam (0.9854)\n"
     ]
    }
   ],
   "source": [
    "# Sample email text\n",
    "sample_email = [\"Congratulations! You've won a free iPhone. Click the link to claim now!\"]\n",
    "\n",
    "# Vectorize using the same TF-IDF vectorizer\n",
    "sample_tfidf = tfidf.transform(sample_email).toarray()\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(sample_tfidf)\n",
    "\n",
    "# Interpret result\n",
    "label = \"Spam\" if prediction[0][0] >= 0.5 else \"Ham\"\n",
    "print(f\"Prediction: {label} ({prediction[0][0]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step\n",
      "Email: Congratulations! You have won a lottery worth $1,000,000.\n",
      "Prediction: Spam (0.9313)\n",
      "\n",
      "Email: Hey, are we still meeting for lunch today?\n",
      "Prediction: Ham (0.0016)\n",
      "\n",
      "Email: Limited-time offer just for you! Get 90% off on your next purchase.\n",
      "Prediction: Ham (0.1808)\n",
      "\n",
      "Email: Please find the attached report for the project.\n",
      "Prediction: Ham (0.1389)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try with Multiple Samples\n",
    "sample_emails = [\n",
    "    \"Congratulations! You have won a lottery worth $1,000,000.\",\n",
    "    \"Hey, are we still meeting for lunch today?\",\n",
    "    \"Limited-time offer just for you! Get 90% off on your next purchase.\",\n",
    "    \"Please find the attached report for the project.\"\n",
    "]\n",
    "\n",
    "sample_tfidf = tfidf.transform(sample_emails).toarray()\n",
    "predictions = model.predict(sample_tfidf)\n",
    "\n",
    "for i, pred in enumerate(predictions):\n",
    "    label = \"Spam\" if pred[0] >= 0.5 else \"Ham\"\n",
    "    print(f\"Email: {sample_emails[i]}\\nPrediction: {label} ({pred[0]:.4f})\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ How to Save and Prepare the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(\"ann_spam_model.h5\")\n",
    "\n",
    "# Save TF-IDF vectorizer\n",
    "import pickle\n",
    "with open(\"tfidf_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tfidf, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Step 1: Data Preparation\n",
    "\n",
    "* Collect a labeled dataset (e.g., movie reviews with positive/negative labels).\n",
    "* Clean the text (lowercase, remove noise).\n",
    "* Handle emoticons by replacing them with sentiment tokens.\n",
    "* Convert text to numeric vectors (TF-IDF, CountVectorizer, or embeddings).\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: ANN Model Architecture\n",
    "\n",
    "* Input layer size = number of features\n",
    "* Hidden layers (e.g., 1 or 2 layers with ReLU activation)\n",
    "* Output layer with 1 neuron and sigmoid activation (for binary sentiment classification)\n",
    "\n",
    "\n",
    "\n",
    "### Step 3: Code Example (Using Keras)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Sample data\n",
    "texts = [\n",
    "    \"I love this product! :)\",\n",
    "    \"This is the worst movie I've ever seen :(\",\n",
    "    \"Absolutely fantastic experience!\",\n",
    "    \"I do not like this at all :/\",\n",
    "    \"Such a boring day...\"\n",
    "]\n",
    "labels = [1, 0, 1, 0, 0]  # 1 = positive, 0 = negative\n",
    "\n",
    "# Preprocessing function to handle emoticons\n",
    "def preprocess_text(text):\n",
    "    emoticon_dict = {\n",
    "        ':)': ' happy ',\n",
    "        ':(': ' sad ',\n",
    "        ':D': ' happy ',\n",
    "        ':/': ' disappointed ',\n",
    "        ':-)': ' happy ',\n",
    "        ':-(': ' sad '\n",
    "    }\n",
    "    for emot in emoticon_dict:\n",
    "        text = text.replace(emot, emoticon_dict[emot])\n",
    "    return text.lower()\n",
    "\n",
    "texts = [preprocess_text(t) for t in texts]\n",
    "\n",
    "# Vectorize text\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(texts).toarray()\n",
    "y = np.array(labels)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define ANN model\n",
    "model = Sequential([\n",
    "    Dense(16, input_dim=X.shape[1], activation='relu'),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=2, verbose=1)\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation\n",
    "\n",
    "* We replace emoticons with text tokens that the vectorizer can pick up.\n",
    "* We use TF-IDF to convert text to numeric vectors.\n",
    "* The ANN has two hidden layers.\n",
    "* The output layer uses sigmoid activation to output a probability for positive sentiment.\n",
    "* We train and evaluate the model on a small dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### Assignment Ideas\n",
    "\n",
    "1. Collect a larger dataset of social media comments.\n",
    "2. Add more emoticon handling rules.\n",
    "3. Try word embeddings (Word2Vec, GloVe) instead of TF-IDF.\n",
    "4. Experiment with different ANN architectures (more layers, dropout).\n",
    "5. Evaluate the model on a test set and analyze errors.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ Streamlit App: spam_detector_app.py\n",
    "\n",
    "✅ How to Run Streamlit App\n",
    "\n",
    "streamlit run spam_detector_app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load trained model\n",
    "model = load_model(\"ann_spam_model.h5\")\n",
    "\n",
    "# Load saved TF-IDF vectorizer\n",
    "with open(\"tfidf_vectorizer.pkl\", \"rb\") as f:\n",
    "    tfidf = pickle.load(f)\n",
    "\n",
    "# Streamlit UI\n",
    "st.title(\"📧 Email Spam Classifier\")\n",
    "st.write(\"Enter an email below to check if it's **Spam** or **Ham**\")\n",
    "\n",
    "email_text = st.text_area(\"✉️ Email Content\")\n",
    "\n",
    "if st.button(\"Predict\"):\n",
    "    if email_text.strip() == \"\":\n",
    "        st.warning(\"Please enter some email text.\")\n",
    "    else:\n",
    "        # Transform text\n",
    "        email_vector = tfidf.transform([email_text]).toarray()\n",
    "        \n",
    "        # Predict\n",
    "        prediction = model.predict(email_vector)\n",
    "        label = \"🛑 Spam\" if prediction[0][0] >= 0.5 else \"✅ Ham\"\n",
    "        confidence = prediction[0][0] if label == \"🛑 Spam\" else 1 - prediction[0][0]\n",
    "        \n",
    "        st.subheader(\"📊 Prediction Result\")\n",
    "        st.write(f\"**Prediction:** {label}\")\n",
    "        st.write(f\"**Confidence:** {confidence:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flask API: app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 2. Save Model and Vectorizer\n",
    "# After training in your main notebook:\n",
    "\n",
    "model.save(\"ann_spam_model.h5\")\n",
    "\n",
    "import pickle\n",
    "with open(\"tfidf_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tfidf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load model and vectorizer\n",
    "model = load_model('ann_spam_model.h5')\n",
    "\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    tfidf = pickle.load(f)\n",
    "\n",
    "@app.route('/', methods=['GET'])\n",
    "def index():\n",
    "    return jsonify({'message': 'Email Spam Detection API is running!'})\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.get_json()\n",
    "    \n",
    "    if 'email' not in data:\n",
    "        return jsonify({'error': 'No email content provided'}), 400\n",
    "\n",
    "    email_text = data['email']\n",
    "    \n",
    "    # Preprocess and predict\n",
    "    email_vector = tfidf.transform([email_text]).toarray()\n",
    "    prediction = model.predict(email_vector)\n",
    "    \n",
    "    label = 'Spam' if prediction[0][0] >= 0.5 else 'Ham'\n",
    "    confidence = float(prediction[0][0]) if label == \"Spam\" else float(1 - prediction[0][0])\n",
    "\n",
    "    return jsonify({\n",
    "        'prediction': label,\n",
    "        'confidence': round(confidence, 4)\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 3. Run the Flask API\n",
    "\n",
    "# pip install flask tensorflow scikit-learn\n",
    "# python app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ✅ **4. Test the API (Using curl or Postman)**\n",
    "\n",
    "### Example `POST` request using **curl**:\n",
    "\n",
    "```bash\n",
    "curl -X POST http://127.0.0.1:5000/predict \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\"email\":\"Congratulations! You have won a free ticket. Click here!\"}'\n",
    "```\n",
    "\n",
    "### Sample Response:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"prediction\": \"Spam\",\n",
    "  \"confidence\": 0.9813\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
