{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got it! Here's a structured explanation and tutorial content on **Sentiment Analysis**, including challenges, handling emoticons, and an example of building a Sentiment Analysis model using an ANN.\n",
    "\n",
    "---\n",
    "\n",
    "# Sentiment Analysis\n",
    "\n",
    "### What is Sentiment Analysis?\n",
    "\n",
    "Sentiment Analysis, also known as opinion mining, is the process of identifying and extracting subjective information from text. It aims to determine the sentiment expressed by the writer, such as positive, negative, or neutral feelings.\n",
    "\n",
    "**Example use cases:**\n",
    "\n",
    "* Analyzing customer reviews to understand satisfaction\n",
    "* Monitoring social media for public opinion on products or events\n",
    "* Automating feedback categorization for support tickets\n",
    "\n",
    "---\n",
    "\n",
    "### Challenges in Sentiment Analysis\n",
    "\n",
    "1. **Context and Sarcasm:**\n",
    "   Sentiment can change depending on context or sarcasm, which are hard to detect automatically.\n",
    "\n",
    "2. **Ambiguity:**\n",
    "   Words can have different sentiment depending on usage (e.g., ‚Äúsick‚Äù could be negative or positive slang).\n",
    "\n",
    "3. **Domain Dependence:**\n",
    "   Sentiment words can vary in meaning by domain (e.g., \"unpredictable\" might be negative for a car but positive for a thriller movie).\n",
    "\n",
    "4. **Handling Negations:**\n",
    "   Negations like \"not good\" flip sentiment and must be carefully interpreted.\n",
    "\n",
    "5. **Emoticons and Emojis:**\n",
    "   They carry emotional meaning and need special handling to be included correctly.\n",
    "\n",
    "---\n",
    "\n",
    "### Handling Emoticons\n",
    "\n",
    "Emoticons (like `:)`, `:(`, `:D`) and emojis (üòä, üò¢) often express emotions explicitly and can improve sentiment detection if correctly interpreted.\n",
    "\n",
    "**Techniques:**\n",
    "\n",
    "* **Mapping emoticons to sentiment scores:**\n",
    "  Replace emoticons with tokens indicating positive/negative sentiment.\n",
    "\n",
    "* **Using Unicode emoji libraries:**\n",
    "  Convert emojis into textual descriptions or sentiment scores.\n",
    "\n",
    "* **Augmenting datasets:**\n",
    "  Include emoticons as features in training data.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import mlflow.sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "# import the data \n",
    "df = pd.read_csv('IMDB.csv')\n",
    "df = df.sample(500)\n",
    "df.to_csv('data.csv', index=False)\n",
    "df.head()\n",
    "\n",
    "# data preprocessing\n",
    "\n",
    "# Define text preprocessing functions\n",
    "def lemmatization(text):\n",
    "    \"\"\"Lemmatize the text.\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = text.split()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    \"\"\"Remove stop words from the text.\"\"\"\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    text = [word for word in str(text).split() if word not in stop_words]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def removing_numbers(text):\n",
    "    \"\"\"Remove numbers from the text.\"\"\"\n",
    "    text = ''.join([char for char in text if not char.isdigit()])\n",
    "    return text\n",
    "\n",
    "def lower_case(text):\n",
    "    \"\"\"Convert text to lower case.\"\"\"\n",
    "    text = text.split()\n",
    "    text = [word.lower() for word in text]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def removing_punctuations(text):\n",
    "    \"\"\"Remove punctuations from the text.\"\"\"\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = text.replace('ÿõ', \"\")\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def removing_urls(text):\n",
    "    \"\"\"Remove URLs from the text.\"\"\"\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "def normalize_text(df):\n",
    "    \"\"\"Normalize the text data.\"\"\"\n",
    "    try:\n",
    "        df['review'] = df['review'].apply(lower_case)\n",
    "        df['review'] = df['review'].apply(remove_stop_words)\n",
    "        df['review'] = df['review'].apply(removing_numbers)\n",
    "        df['review'] = df['review'].apply(removing_punctuations)\n",
    "        df['review'] = df['review'].apply(removing_urls)\n",
    "        df['review'] = df['review'].apply(lemmatization)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f'Error during text normalization: {e}')\n",
    "        raise\n",
    "\n",
    "df = normalize_text(df)\n",
    "df.head()\n",
    "df['sentiment'].value_counts()\n",
    "df\n",
    "# x = df['sentiment'].isin(['positive','negative'])\n",
    "# df = df[x]\n",
    "df['sentiment'] = df['sentiment'].map({'positive':1, 'negative':0})\n",
    "df.head()\n",
    "df.isnull().sum()\n",
    "# change text data into number data\n",
    "vectorizer = CountVectorizer(max_features=100)\n",
    "X = vectorizer.fit_transform(df['review'])\n",
    "y = df['sentiment']\n",
    "df[\"review\"][651]\n",
    "X.toarray()[1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "# classification problem \n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "x_predict = model.predict(X_test)\n",
    "\n",
    "x_predict\n",
    "accuracy_score(y_test, x_predict)\n",
    "df\n",
    "df[\"review\"][799]\n",
    "rahul = \"normally spike lee fan take time really get mojo see clear message ability tell story close heart lee genius unlike th hour bamboozled two favorite film his clear story film able understand struggle washington choice play well influenced others odd reason lee never able get true feeling out washington decent job handed him could tell lee s favorite film lee direct film also wrote it could tell camera work horrid writing contributed decay film film coming full circle going pretty lee behind film right thing film seen lee direct brightest modest film almost created hollywood movie instead one own know saw money right thing ran it film demonstrate true talent br br for anyone seen film perhaps stopped watching anything directed spike lee afterwards due film suggest give second chance get wrong see exactly coming film would want put behind you lee grow up work becomes own see transformation desire make money wanting make good film took awhile watch th hour did sheer brilliance perhaps actor perhaps story lee crafted amazing film one man s journey unknown guess hoping mo better blue would turn be really dark journey life man really never grew up instead got denzel denzel really one versatile actor generation consider sydney poitier cinema film showcase talent br br another issue film use spike s sister playing one love interest know you family think could filmed sex scene sister care actor much money getting paid would never it something never wish see apparently different spike went ahead showed full nude image sister without remorse sad even made blush also need somebody answer this flavor flav introducing film so sitting couch ready start film suddenly voice past spelling studio made film acknowledges himself build strong remaining story again felt lee going money film instead actual talent perhaps could afford denzel wesley movie without explosion br br there two great scene film made worth watching end get wrong bad movie always diamond every alleyway scene bleek accidentally forgets woman mesmerizing continually went back forth weaving truth confusion way proved lee actually behind camera visionary scene probably lost shuffle due remaining poor scene scene worth watching way lee introduced ended film keeping pacing direction able bring tragic character around full circle give chance change life two moment rest film pure rubbish worth viewing unless go blind br br grade'\"\n",
    "# 2. Transform your input\n",
    "\n",
    "df1 = pd.DataFrame({'review': [rahul]})\n",
    "\n",
    "df11 = normalize_text(df1)  # Apply your preprocessing pipeline\n",
    "df1\n",
    "# Transform using the same vectorizer\n",
    "X_input = vectorizer.transform(df1['review'])\n",
    "\n",
    "X_input.toarray()\n",
    "X_input.shape\n",
    "# 3. Predict using your trained model\n",
    "prediction = model.predict(X_input)\n",
    "\n",
    "if prediction[0]==0:\n",
    "    print(rahul ,\"Negative review\")\n",
    "else:\n",
    "    print(rahul ,\"Positive review\")\n",
    "\n",
    "# 3. Predict using your trained model\n",
    "prediction = model.predict(X_input)\n",
    "\n",
    "if prediction[0]==0:\n",
    "    print(rahul ,\"Negative review\")\n",
    "else:\n",
    "    print(rahul ,\"Positive review\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "logging.info(\"Initializing Logistic Regression model...\")\n",
    "model = LogisticRegression(max_iter=1000)  # Increase max_iter to prevent non-convergence issues\n",
    "\n",
    "logging.info(\"Fitting the model...\")\n",
    "model.fit(X_train, y_train)\n",
    "logging.info(\"Model training complete.\")\n",
    "\n",
    "logging.info(\"Logging model parameters...\")\n",
    "mlflow.log_param(\"model\", \"Logistic Regression\")\n",
    "\n",
    "logging.info(\"Making predictions...\")\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "logging.info(\"Calculating evaluation metrics...\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import dagshub\n",
    "\n",
    "mlflow.set_tracking_uri('https://dagshub.com/vikashdas770/YT-Capstone-Project.mlflow')\n",
    "dagshub.init(repo_owner='vikashdas770', repo_name='YT-Capstone-Project', mlflow=True)\n",
    "\n",
    "# mlflow.set_experiment(\"Logistic Regression Baseline\")\n",
    "mlflow.set_experiment(\"Logistic Regression Baseline\")\n",
    "\n",
    "import mlflow\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "logging.info(\"Starting MLflow run...\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        logging.info(\"Logging preprocessing parameters...\")\n",
    "        mlflow.log_param(\"vectorizer\", \"Bag of Words\")\n",
    "        mlflow.log_param(\"num_features\", 100)\n",
    "        mlflow.log_param(\"test_size\", 0.25)\n",
    "\n",
    "        logging.info(\"Initializing Logistic Regression model...\")\n",
    "        model = LogisticRegression(max_iter=1000)  # Increase max_iter to prevent non-convergence issues\n",
    "\n",
    "        logging.info(\"Fitting the model...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        logging.info(\"Model training complete.\")\n",
    "\n",
    "        logging.info(\"Logging model parameters...\")\n",
    "        mlflow.log_param(\"model\", \"Logistic Regression\")\n",
    "\n",
    "        logging.info(\"Making predictions...\")\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        logging.info(\"Calculating evaluation metrics...\")\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        logging.info(\"Logging evaluation metrics...\")\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "        logging.info(\"Saving and logging the model...\")\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "        # Log execution time\n",
    "        end_time = time.time()\n",
    "        logging.info(f\"Model training and logging completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "        # Save and log the notebook\n",
    "        # notebook_path = \"exp1_baseline_model.ipynb\"\n",
    "        # logging.info(\"Executing Jupyter Notebook. This may take a while...\")\n",
    "        # os.system(f\"jupyter nbconvert --to notebook --execute --inplace {notebook_path}\")\n",
    "        # mlflow.log_artifact(notebook_path)\n",
    "\n",
    "        # logging.info(\"Notebook execution and logging complete.\")\n",
    "\n",
    "        # Print the results for verification\n",
    "        logging.info(f\"Accuracy: {accuracy}\")\n",
    "        logging.info(f\"Precision: {precision}\")\n",
    "        logging.info(f\"Recall: {recall}\")\n",
    "        logging.info(f\"F1 Score: {f1}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\", exc_info=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Naive Bayes Classifier\n",
    "gnb = GaussianNB() \n",
    "%time gnb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = gnb.predict(X_train)\n",
    "y_pred_test = gnb.predict(X_test)\n",
    "print(\"\\nTraining Accuracy score:\",accuracy_score(y_train, y_pred_train))\n",
    "print(\"Testing Accuracy score:\",accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_test, target_names=['not relevant', 'relevant']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "# print('Confusion matrix\\n', cm)\n",
    "\n",
    "cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive', 'Actual Negative'], \n",
    "                        index=['Predict Positive', 'Predict Negative'])\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = gnb.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also use MultinomialNB ,LogisticRegression,LinearSVC,DecisionTreeClassifier,ensemble\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "classifiers = [('Decision Tree', dt),\n",
    "               ('Logistic Regression', lr),\n",
    "                ('Naive Bayes', gnb)\n",
    "              ]\n",
    "vc = VotingClassifier(estimators=classifiers)\n",
    "# Fit 'vc' to the traing set and predict test set labels\n",
    "vc.fit(X_train, y_train)\n",
    "y_pred_train=vc.predict(X_train)\n",
    "y_pred_test = vc.predict(X_test)\n",
    "print(\"Training Accuracy score:\",accuracy_score(y_train, y_pred_train))\n",
    "print(\"Testing Accuracy score:\",accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with ANN (Artificial Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "### Step 1: Data Preparation\n",
    "\n",
    "* Collect a labeled dataset (e.g., movie reviews with positive/negative labels).\n",
    "* Clean the text (lowercase, remove noise).\n",
    "* Handle emoticons by replacing them with sentiment tokens.\n",
    "* Convert text to numeric vectors (TF-IDF, CountVectorizer, or embeddings).\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: ANN Model Architecture\n",
    "\n",
    "* Input layer size = number of features\n",
    "* Hidden layers (e.g., 1 or 2 layers with ReLU activation)\n",
    "* Output layer with 1 neuron and sigmoid activation (for binary sentiment classification)\n",
    "\n",
    "\n",
    "\n",
    "### Step 3: Code Example (Using Keras)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Sample data\n",
    "texts = [\n",
    "    \"I love this product! :)\",\n",
    "    \"This is the worst movie I've ever seen :(\",\n",
    "    \"Absolutely fantastic experience!\",\n",
    "    \"I do not like this at all :/\",\n",
    "    \"Such a boring day...\"\n",
    "]\n",
    "labels = [1, 0, 1, 0, 0]  # 1 = positive, 0 = negative\n",
    "\n",
    "# Preprocessing function to handle emoticons\n",
    "def preprocess_text(text):\n",
    "    emoticon_dict = {\n",
    "        ':)': ' happy ',\n",
    "        ':(': ' sad ',\n",
    "        ':D': ' happy ',\n",
    "        ':/': ' disappointed ',\n",
    "        ':-)': ' happy ',\n",
    "        ':-(': ' sad '\n",
    "    }\n",
    "    for emot in emoticon_dict:\n",
    "        text = text.replace(emot, emoticon_dict[emot])\n",
    "    return text.lower()\n",
    "\n",
    "texts = [preprocess_text(t) for t in texts]\n",
    "\n",
    "# Vectorize text\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(texts).toarray()\n",
    "y = np.array(labels)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define ANN model\n",
    "model = Sequential([\n",
    "    Dense(16, input_dim=X.shape[1], activation='relu'),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=2, verbose=1)\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation\n",
    "\n",
    "* We replace emoticons with text tokens that the vectorizer can pick up.\n",
    "* We use TF-IDF to convert text to numeric vectors.\n",
    "* The ANN has two hidden layers.\n",
    "* The output layer uses sigmoid activation to output a probability for positive sentiment.\n",
    "* We train and evaluate the model on a small dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### Assignment Ideas\n",
    "\n",
    "1. Collect a larger dataset of social media comments.\n",
    "2. Add more emoticon handling rules.\n",
    "3. Try word embeddings (Word2Vec, GloVe) instead of TF-IDF.\n",
    "4. Experiment with different ANN architectures (more layers, dropout).\n",
    "5. Evaluate the model on a test set and analyze errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
