{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZOv5Asgr43NXo4x0wVKQU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìò **Text Representation | NLP Lecture 4**\n",
        "\n",
        "### Topics: Bag of Words | TF-IDF | N-grams (Uni-grams, Bi-grams)\n"
      ],
      "metadata": {
        "id": "mflDZtv5sUXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I. **Introduction to Text Representation**\n",
        "\n",
        "**Goal:** Convert text data into numerical form (a process also known as *Text Vectorisation* or *Feature Extraction from Text*) so that ML algorithms can process it.\n",
        "\n",
        "### üîπ Importance\n",
        "\n",
        "* **Feature Quality:** The effectiveness of ML models heavily depends on feature quality ‚Äî *‚ÄúGarbage in, garbage out.‚Äù*\n",
        "* **NLP Pipeline:** Follows data acquisition and pre-processing; crucial for ML-based NLP.\n",
        "* **Objective:** Numerical representation should capture the *semantic meaning* of text.\n",
        "\n",
        "### üîπ Challenges\n",
        "\n",
        "* Text ‚Üí Numbers is hard (unlike image or audio data).\n",
        "* Requires intelligent mapping of linguistic meaning to mathematical form.\n",
        "\n",
        "### üîπ Techniques Covered\n",
        "\n",
        "1. One-Hot Encoding\n",
        "2. Bag of Words (BoW)\n",
        "3. N-grams (Uni, Bi, Tri-grams)\n",
        "4. TF‚ÄìIDF (Term Frequency‚ÄìInverse Document Frequency)\n",
        "5. Custom Features\n",
        "   *(Future: Word Embeddings ‚Äì Word2Vec, GloVe, etc.)*\n",
        "\n",
        "---\n",
        "\n",
        "## II. **Key Terminology**\n",
        "\n",
        "| Term                     | Description                                      |\n",
        "| ------------------------ | ------------------------------------------------ |\n",
        "| **Corpus (C)**           | All words from all documents (including repeats) |\n",
        "| **Vocabulary (V)**       | Set of *unique* words from the corpus            |\n",
        "| **Document (D)**         | A single text unit (sentence, paragraph, etc.)   |\n",
        "| **Word (W) or Term (T)** | A single token in a document                     |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "1lK2urnEshri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. One-Hot Encoding (1950s‚Äì1960s, very early NLP)**\n",
        "\n",
        "* **Idea:** Represent each word as a binary vector ‚Äî all 0s except for a single 1 in the position corresponding to that word in the vocabulary.\n",
        "* **Example:**\n",
        "  Vocabulary: [\"cat\", \"dog\", \"fish\"]\n",
        "  ‚Äúdog‚Äù ‚Üí [0, **1**, 0]\n",
        "* **Limitation:**\n",
        "\n",
        "  * Ignores context and meaning\n",
        "  * High dimensional and sparse\n",
        "- ‚ö†Ô∏è Drawbacks\n",
        "\n",
        "üîπ Disadvantages\n",
        "\n",
        "- ‚ùå Sparsity: Huge, mostly-zero matrices.\n",
        "- ‚ùå Non-fixed size: Different document lengths = different input sizes.\n",
        "- ‚ùå OOV(Out-Of-Vocabulary) problem: New words can‚Äôt be represented.\n",
        "- ‚ùå No semantics: ‚Äúwalk‚Äù and ‚Äúrun‚Äù are equally distant from ‚Äúbottle‚Äù.\n"
      ],
      "metadata": {
        "id": "MQ3HoX_CNPUw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bsVHTOGCuf9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "corpus = [\"dog barks\", \"cat meows\", \"dog runs\"]\n",
        "words = list(set(\" \".join(corpus).split()))  # unique vocab\n",
        "print(\"Vocabulary:\", words)\n",
        "\n",
        "# Manual one-hot encoding\n",
        "import numpy as np\n",
        "V = len(words)\n",
        "encoding = {word: np.eye(V)[i] for i, word in enumerate(words)}\n",
        "encoding\n"
      ],
      "metadata": {
        "id": "BM3QwCNXusGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "#### **2. Bag of Words (BoW) (1980s‚Äì1990s)**\n",
        "\n",
        "* **Idea:** Represent a document by word **counts** (or frequencies) ‚Äî order of words is ignored.\n",
        "\n",
        "- Vocabulary from corpus ‚Üí Each document = vector of word frequencies.\n",
        "- Ignores order ‚Üí treats text as a bag of words.\n",
        "- Used widely for text classification.\n",
        "\n",
        "* **Example:**\n",
        "  ‚ÄúDog bites man‚Äù and ‚ÄúMan bites dog‚Äù ‚Üí same vector (same counts).\n",
        "* **Limitation:**\n",
        "\n",
        "#### üîπ Advantages\n",
        "\n",
        "- ‚úÖ Fixed-size vector for any document\n",
        "- ‚úÖ Tolerates unseen words (ignored at inference)/OOV words ignored gracefully\n",
        "\n",
        "#### üîπ Disadvantages\n",
        "\n",
        "- ‚ùå Sparse vectors/Large vocabulary ‚Üí sparse vectors\n",
        "- ‚ùå Loses word order/Ignores word order/context (syntax)\n",
        "- ‚ùå Fails with negation ‚Äî ‚Äúgood‚Äù vs. ‚Äúnot good‚Äù appear similar\n",
        "\n",
        "\n",
        "* üîπ Scikit-learn:\n",
        "- CountVectorizer()\n",
        "\n",
        "* üîπ Key Parameters\n",
        "  - binary=True: presence (1/0) instead of count.\n",
        "  - max_features: limit vocabulary to top-N frequent words.\n",
        "  - stop_words: remove common stopwords.\n"
      ],
      "metadata": {
        "id": "uykPTSJ1uyUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"The cat sat on the mat\",\n",
        "    \"The dog sat on the log\",\n",
        "    \"The cat chased the mouse\"\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out()))\n",
        "\n"
      ],
      "metadata": {
        "id": "9pPYxdB_wdWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚öôÔ∏è Key Parameters\n",
        "vectorizer1 = CountVectorizer(\n",
        "    binary=True,             # presence/absence instead of counts\n",
        "    max_features=10,         # limit vocabulary size\n",
        "    stop_words='english',    # remove stopwords\n",
        ")"
      ],
      "metadata": {
        "id": "ERt8O60rw2_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer1.fit_transform(corpus).toarray()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_NWwKX8lF1ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3. N-grams (Unigram, Bigram, Trigram) (1990s‚Äì2000s)**\n",
        "\n",
        "* **Idea:** Capture **local word order** by looking at sequences of *n* words.\n",
        "## * üìò Concept\n",
        "\n",
        "- Extends BoW by including sequences of N words.\n",
        "- Captures local word order and context.\n",
        "\n",
        "* **Examples:**\n",
        "\n",
        "  * Unigrams: ‚Äúdog‚Äù, ‚Äúbites‚Äù, ‚Äúman‚Äù\n",
        "  * Bigrams: ‚Äúdog bites‚Äù, ‚Äúbites man‚Äù\n",
        "  * Trigrams: ‚Äúdog bites man‚Äù\n",
        "* **Benefit:** Adds some context awareness.\n",
        "- Captures context and short phrases\n",
        "\n",
        "- Helps handle negations and idioms\n",
        "\n",
        "* **Limitation:** Still sparse, grows combinatorially with *n*.\n",
        "  - Vocabulary size grows fast (computational cost)\n",
        "  - Still sparse and OOV issues remain"
      ],
      "metadata": {
        "id": "REc6sR6hw3aR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bi-gram example\n",
        "bi_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
        "# (2, 2): Setting the range from 2 (minimum n-value) to 2 (maximum n-value)\n",
        "# so (2,2 means the vectorizer will only consider sequences of exactly two words, known as bigrams.\n",
        "X_bi = bi_vectorizer.fit_transform(corpus)\n",
        "corpus = [\n",
        "    \"The cat sat on the mat\",\n",
        "    \"The dog sat on the log\",\n",
        "    \"The cat chased the mouse\"\n",
        "]\n",
        "print(\"Bi-gram Vocabulary:\", bi_vectorizer.get_feature_names_out())\n",
        "print(pd.DataFrame(X_bi.toarray(), columns=bi_vectorizer.get_feature_names_out()))\n",
        "print(\"==============================\")\n",
        "print(len(bi_vectorizer.get_feature_names_out()))"
      ],
      "metadata": {
        "id": "4dfYVQNLw4vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uni + Bi + Tri-grams\n",
        "combo_vectorizer = CountVectorizer(ngram_range=(1,3))\n",
        "X_combo = combo_vectorizer.fit_transform(corpus)\n",
        "print(\"Combined Vocabulary Size:\", len(combo_vectorizer.get_feature_names_out()))\n",
        "\n",
        "print(X_combo)"
      ],
      "metadata": {
        "id": "m_LQa7TAT-nR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "#### **4. TF‚ÄìIDF (Term Frequency‚ÄìInverse Document Frequency) (1990s‚Äì2000s)**\n",
        "\n",
        "* **Idea:** Weigh words by importance ‚Äî frequent in a document but rare across the corpus.\n",
        "- Assigns importance weights to words instead of raw counts.\n",
        "* **Formula:**\n",
        "  TF-IDF(T,D)=TF(T,D)√óIDF(T)\n",
        "\n",
        "- TF= Word occurrences in doc/Total words in doc'\n",
        "\n",
        "- Inverse Document Frequency (IDF):\n",
        "- IDF=log(N/n_T)\n",
        "\n",
        "Weight = TF √ó IDF\n",
        "* **Benefit:** Reduces impact of common words like ‚Äúthe‚Äù, ‚Äúand‚Äù.\n",
        "* **Limitation:** Still based on counts, no semantic understanding.\n",
        "\n",
        "* ‚úÖ Advantages\n",
        "\n",
        "Reduces weight of common words\n",
        "Useful for information retrieval (e.g., search engines)\n",
        "\n",
        "* ‚ùå Disadvantages\n",
        "\n",
        "- Sparse matrix\n",
        "- OOV issue\n",
        "- Still no deep semantic relation captured\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "boJffXGbUOcY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KVZP2Xr0w7Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"The cat sat on the mat\",\n",
        "    \"The dog sat on the log\",\n",
        "    \"The cat chased the mouse\"\n",
        "]\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "X_tfidf = tfidf.fit_transform(corpus)\n",
        "\n",
        "print(\"Vocabulary:\", tfidf.get_feature_names_out())\n",
        "print(pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out()))\n"
      ],
      "metadata": {
        "id": "OHzzJvUky5Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **5. Word Embeddings (Word2Vec, GloVe, FastText, etc.) (2013 onward)**\n",
        "\n",
        "* **Idea:** Learn **dense, low-dimensional vectors** where similar words are close in vector space.\n",
        "* **Example:**\n",
        "  Vector(‚Äúking‚Äù) ‚Äì Vector(‚Äúman‚Äù) + Vector(‚Äúwoman‚Äù) ‚âà Vector(‚Äúqueen‚Äù)\n",
        "* **Benefit:** Captures **semantic meaning** and **relationships** between words.\n",
        "* **Limitation:** Fixed for each word ‚Äî context-independent.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "GoxwwPuwy9q0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.cs.cmu.edu/~dst/WordEmbeddingDemo/tutorial.html"
      ],
      "metadata": {
        "id": "1Lt9MoY6cI1B"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dq7UiDq1zB0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† **1. Word2Vec**\n",
        "\n",
        "### üìò **Definition**\n",
        "\n",
        "**Word2Vec** (by Google, 2013) is a **neural embedding model** that learns to represent words as dense vectors.\n",
        "It uses two main architectures:\n",
        "\n",
        "* **CBOW (Continuous Bag of Words):** Predicts a word based on its context.\n",
        "* **Skip-Gram:** Predicts context words given a target word.\n",
        "\n",
        "These embeddings capture **semantic and syntactic relationships** between words ‚Äî e.g.,\n",
        "`vector(\"king\") - vector(\"man\") + vector(\"woman\") ‚âà vector(\"queen\")`.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Advantages**\n",
        "\n",
        "* Captures both **semantic** and **syntactic** relationships.\n",
        "* Trains efficiently on large datasets.\n",
        "* Performs well in many NLP downstream tasks.\n",
        "\n",
        "### ‚ùå **Disadvantages**\n",
        "\n",
        "* Doesn‚Äôt handle **out-of-vocabulary (OOV)** words.\n",
        "* Ignores **subword (morphological)** information.\n",
        "* Embeddings depend heavily on training data quality.\n"
      ],
      "metadata": {
        "id": "Qnwa635aGiNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### üíª **Code Example**\n",
        "!pip install gensim"
      ],
      "metadata": {
        "id": "38czyPwudfEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# Sample corpus\n",
        "sentences = [\n",
        "    \"I love deep learning and natural language processing\",\n",
        "    \"Word embeddings are useful for NLP tasks\",\n",
        "    \"Word2Vec is a great model for learning word vectors\"\n",
        "]\n",
        "\n",
        "# Preprocessing\n",
        "tokenized_sentences = [simple_preprocess(sentence) for sentence in sentences]\n",
        "\n",
        "# Train Word2Vec model\n",
        "model_w2v = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
        "\n",
        "# Check vector and similar words\n",
        "print(model_w2v.wv[\"learning\"])\n",
        "print(model_w2v.wv.most_similar(\"learning\"))\n"
      ],
      "metadata": {
        "id": "cHmO9JYzESBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìó **2. GloVe (Global Vectors for Word Representation)**\n",
        "\n",
        "### üìò **Definition**\n",
        "\n",
        "**GloVe** (by Stanford, 2014) learns word embeddings by analyzing **global word co-occurrence statistics** across the entire corpus.\n",
        "It focuses on how often words appear together ‚Äî building a **co-occurrence matrix**, then factorizing it to learn embeddings.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Advantages**\n",
        "\n",
        "* Captures **global context** better than Word2Vec (which is local).\n",
        "* Produces **consistent** embeddings using statistical information.\n",
        "* Pretrained models available (trained on huge corpora like Wikipedia).\n",
        "\n",
        "### ‚ùå **Disadvantages**\n",
        "\n",
        "* Doesn‚Äôt handle **OOV** words.\n",
        "* Cannot learn new embeddings once pretrained.\n",
        "* Needs large memory for co-occurrence matrix.\n",
        "\n",
        "---\n",
        "\n",
        "### üíª **Code Example**\n",
        "\n",
        "```python\n",
        "# !pip install torchtext torch\n",
        "\n",
        "from torchtext.vocab import GloVe\n",
        "import torch\n",
        "\n",
        "# Load pretrained GloVe embeddings (50 dimensions)\n",
        "glove = GloVe(name=\"6B\", dim=50)\n",
        "\n",
        "# Get vector for a word\n",
        "word_vec = glove[\"computer\"]\n",
        "print(word_vec)\n",
        "\n",
        "# Compute similarity\n",
        "sim = torch.cosine_similarity(glove[\"king\"].unsqueeze(0), glove[\"queen\"].unsqueeze(0))\n",
        "print(f\"Similarity(king, queen): {sim.item():.4f}\")\n",
        "```"
      ],
      "metadata": {
        "id": "VKHxVOSvGoIn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BgCv5BZ-G4uA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# üìò **3. FastText**\n",
        "\n",
        "### üìò **Definition**\n",
        "\n",
        "**FastText** (by Facebook, 2016) extends Word2Vec by representing each word as a **bag of character n-grams**.\n",
        "This allows it to understand **morphology** and generate embeddings for unseen words.\n",
        "\n",
        "Example:\n",
        "`\"playing\"` ‚Üí `[\"pla\", \"lay\", \"ayi\", \"yin\", \"ing\"]`\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Advantages**\n",
        "\n",
        "* Handles **out-of-vocabulary** (OOV) words.\n",
        "* Captures **subword information** (prefixes, suffixes).\n",
        "* Works well for **morphologically rich languages** (like German or Turkish).\n",
        "\n",
        "### ‚ùå **Disadvantages**\n",
        "\n",
        "* Slightly **slower** to train than Word2Vec.\n",
        "* Requires more memory.\n",
        "* Subword info might not always improve performance (for small datasets).\n",
        "\n",
        "---\n",
        "\n",
        "### üíª **Code Example**\n",
        "\n",
        "```python\n",
        "from gensim.models import FastText\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# Corpus\n",
        "sentences = [\n",
        "    \"I love machine learning and data science\",\n",
        "    \"FastText creates embeddings using subwords\",\n",
        "    \"It can handle unseen words like learnings\"\n",
        "]\n",
        "\n",
        "tokenized_sentences = [simple_preprocess(s) for s in sentences]\n",
        "\n",
        "# Train FastText model\n",
        "model_ft = FastText(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Vector for a known word\n",
        "print(model_ft.wv[\"learning\"])\n",
        "\n",
        "# Vector for an unseen (OOV) word\n",
        "print(model_ft.wv[\"learnings\"])  # Works!\n",
        "```"
      ],
      "metadata": {
        "id": "s38A17AbG5Ut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# üìä **üîç Summary Table**\n",
        "\n",
        "| Model        | Year | Creator  | Key Idea                    | Handles OOV | Pros                              | Cons                       |\n",
        "| ------------ | ---- | -------- | --------------------------- | ----------- | --------------------------------- | -------------------------- |\n",
        "| **Word2Vec** | 2013 | Google   | Predictive (CBOW/Skip-Gram) | ‚ùå           | Semantic relationships, efficient | No OOV, ignores morphology |\n",
        "| **GloVe**    | 2014 | Stanford | Global co-occurrence matrix | ‚ùå           | Global context, pretrained models | Static, no OOV             |\n",
        "| **FastText** | 2016 | Facebook | Subword (character n-grams) | ‚úÖ           | Handles OOV, morphological info   | Slower, more memory        |\n",
        "\n",
        "---\n",
        "\n",
        "Would you like me to add a **visualization (PCA or t-SNE)** to compare how similar words cluster across these models?\n"
      ],
      "metadata": {
        "id": "JUPhp2wZESnl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lapO0s2hER58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x-bqQv1EERvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VnQOlQS7ERhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### **6. Contextual Word Embeddings (BERT, GPT, etc.) (2018 onward)**\n",
        "\n",
        "* **Idea:** Represent words **in context**, so ‚Äúbank‚Äù in ‚Äúriver bank‚Äù ‚â† ‚Äúbank‚Äù in ‚Äúmoney bank‚Äù.\n",
        "* **Examples:** ELMo (2018), BERT (2018), GPT series (2018+)\n",
        "* **Benefit:** State-of-the-art performance across NLP tasks.\n",
        "* **Limitation:** Computationally expensive.\n"
      ],
      "metadata": {
        "id": "UtW5b4QLz4T4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "enhYmeduQTJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kchei7NfRcWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† **Contextual Word Embeddings | NLP Lecture 5**\n",
        "\n",
        "---\n",
        "\n",
        "## I. **Introduction**\n",
        "\n",
        "Traditional techniques like **One-Hot**, **BoW**, **N-grams**, and **TF‚ÄìIDF** treat each word as **independent** and **context-free**.\n",
        "They fail to capture:\n",
        "\n",
        "* **Meaning differences** based on context (e.g., *bank* = river bank vs. money bank)\n",
        "* **Semantic similarity** between related words (*good*, *great*, *excellent*)\n",
        "\n",
        "‚û°Ô∏è **Contextual Word Embeddings** solve these issues using **deep learning**.\n",
        "\n",
        "---\n",
        "\n",
        "## II. **From Static to Contextual Embeddings**\n",
        "\n",
        "### 1. **Static Embeddings (Word2Vec, GloVe, FastText)**\n",
        "\n",
        "* Each word has **one fixed vector**, no matter where it appears.\n",
        "* ‚Äúbank‚Äù ‚Üí same vector in both ‚Äúriver bank‚Äù and ‚Äúbank loan‚Äù.\n",
        "* Captures general *semantic similarity*, but **no context**.\n",
        "\n",
        "### 2. **Contextual Embeddings (ELMo, BERT, GPT, etc.)**\n",
        "\n",
        "* Word meaning changes **depending on its sentence context**.\n",
        "* ‚Äúbank‚Äù in ‚Äúriver bank‚Äù and ‚Äúbank loan‚Äù ‚Üí **different embeddings**.\n",
        "* Generated dynamically by **transformer-based** neural networks.\n",
        "\n",
        "---\n",
        "\n",
        "## III. **How Contextual Embeddings Work**\n",
        "\n",
        "### üîπ **Architecture**\n",
        "\n",
        "Most modern embeddings are based on the **Transformer architecture**, introduced in *‚ÄúAttention is All You Need‚Äù (Vaswani et al., 2017)*.\n",
        "\n",
        "Key idea:\n",
        "üëâ Use **Self-Attention** to learn relationships between all words in a sentence.\n",
        "\n",
        "### üîπ **Self-Attention**\n",
        "\n",
        "* Every word looks at all other words to understand its context.\n",
        "* Example:\n",
        "\n",
        "  * Sentence: *‚ÄúThe bank of the river was flooded.‚Äù*\n",
        "  * The model looks at ‚Äúriver‚Äù ‚Üí understands that *bank* refers to a geographical feature.\n",
        "\n",
        "---\n",
        "\n",
        "## IV. **Popular Contextual Embedding Models**\n",
        "\n",
        "| Model               | Type                  | Key Idea                              | Notes                        |\n",
        "| ------------------- | --------------------- | ------------------------------------- | ---------------------------- |\n",
        "| **ELMo (2018)**     | BiLSTM                | Contextual from both directions       | First major contextual model |\n",
        "| **BERT (2019)**     | Transformer           | Bidirectional context using attention | Widely used in NLP tasks     |\n",
        "| **GPT (2018‚Äì2024)** | Transformer (decoder) | Predicts next word (left-to-right)    | Used for generation tasks    |\n",
        "| **RoBERTa**         | BERT variant          | More training, better performance     |                              |\n",
        "| **DistilBERT**      | Lightweight BERT      | 40% smaller, similar accuracy         |                              |\n",
        "\n",
        "---\n",
        "\n",
        "## V. **Example: Using BERT Embeddings**\n",
        "\n",
        "### üíª **Code Example (Using `transformers` library)**\n"
      ],
      "metadata": {
        "id": "Xfhpa5kVRc6L"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6XQPGyBfRdYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Example 1 ‚Äî BERT (Bidirectional Encoder Representations from Transformers)\n"
      ],
      "metadata": {
        "id": "-1zv78a1SWEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers torch\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Load pretrained model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Example sentences\n",
        "sentence = \"I went to the bank to withdraw money.\"\n",
        "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "\n",
        "# Get embeddings\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Extract last hidden states (contextual embeddings)\n",
        "last_hidden_states = outputs.last_hidden_state\n",
        "print(last_hidden_states.shape)  # [batch_size, sequence_length, hidden_size]\n",
        "\n",
        "# Example: get embedding for \"bank\"\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "print(tokens)\n",
        "bank_index = tokens.index(\"bank\")\n",
        "print(f\"Embedding for 'bank':\\n\", last_hidden_states[0, bank_index, :5])  # first 5 dims\n"
      ],
      "metadata": {
        "id": "dJg7VqdHSVL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wbqf242kSb39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Example 2 ‚Äî GPT (Generative Pre-trained Transformer)"
      ],
      "metadata": {
        "id": "ynh-520DScVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2Model.from_pretrained(\"gpt2\")\n",
        "\n",
        "sentence = \"The river bank was full of trees.\"\n",
        "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Get last hidden states\n",
        "embeddings = outputs.last_hidden_state\n",
        "print(embeddings.shape)\n",
        "\n",
        "# Inspect token-level embeddings\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "rRF7qk53Se2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Example 3 ‚Äî Compare Contexts for ‚Äúbank‚Äù"
      ],
      "metadata": {
        "id": "5VqwsOlPSmuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"I deposited money in the bank.\",\n",
        "    \"We sat by the river bank and watched the water.\"\n",
        "]\n",
        "\n",
        "for sentence in sentences:\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "    bank_index = tokens.index(\"bank\")\n",
        "    embedding = outputs.last_hidden_state[0, bank_index, :]\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Bank embedding (first 5 dims): {embedding[:5]}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "3cG8eZm3SVmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Model    | Year       | Architecture        | Context Type  | Pros                         | Cons                       |\n",
        "| -------- | ---------- | ------------------- | ------------- | ---------------------------- | -------------------------- |\n",
        "| **ELMo** | 2018       | Bi-LSTM             | Bidirectional | Context-aware, simple        | Slower, older architecture |\n",
        "| **BERT** | 2018       | Transformer Encoder | Bidirectional | Best for understanding tasks | Heavy, non-generative      |\n",
        "| **GPT**  | 2018‚Äì2023+ | Transformer Decoder | Left-to-right | Generative power, flexible   | Unidirectional context     |\n"
      ],
      "metadata": {
        "id": "ga5Xr8mZSxD1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z6k9dnlCSqit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kLu_rslqSrAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```python\n",
        "!pip install transformers torch --quiet\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Load pretrained BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"The bank of the river was flooded.\"\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(sentence, return_tensors='pt')\n",
        "\n",
        "# Get embeddings\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# outputs contains:\n",
        "# - last_hidden_state: embeddings for each token\n",
        "# - pooler_output: embedding for entire sentence (CLS token)\n",
        "word_embeddings = outputs.last_hidden_state\n",
        "sentence_embedding = outputs.pooler_output\n",
        "\n",
        "print(\"Word Embeddings Shape:\", word_embeddings.shape)\n",
        "print(\"Sentence Embedding Shape:\", sentence_embedding.shape)\n",
        "```\n",
        "\n",
        "### üß© **Output Explanation**\n",
        "\n",
        "* `word_embeddings`: Tensor of shape `[1, num_tokens, 768]` ‚Üí one 768-dim vector per token.\n",
        "* `sentence_embedding`: `[1, 768]` ‚Üí condensed representation of entire sentence.\n",
        "\n",
        "---\n",
        "\n",
        "## VI. **Understanding the Embedding Dimensions**\n",
        "\n",
        "| Model           | Vector Size | Layers | Context Type         |\n",
        "| --------------- | ----------- | ------ | -------------------- |\n",
        "| **BERT-base**   | 768         | 12     | Bidirectional        |\n",
        "| **BERT-large**  | 1024        | 24     | Bidirectional        |\n",
        "| **GPT-2 small** | 768         | 12     | Unidirectional       |\n",
        "| **ELMo**        | 1024        | 2      | Bidirectional (LSTM) |\n",
        "\n",
        "Each embedding dimension encodes complex linguistic patterns such as syntax, semantics, and relationships.\n",
        "\n",
        "---\n",
        "\n",
        "## VII. **Sentence-Level Embeddings**\n",
        "\n",
        "Instead of token-level vectors, sometimes we need **whole-sentence embeddings** (for tasks like similarity or classification).\n",
        "\n",
        "### üîπ Options:\n",
        "\n",
        "1. **[CLS] Token (BERT):**\n",
        "   Use the first token embedding (sentence representation).\n",
        "2. **Mean Pooling:**\n",
        "   Average over all token embeddings.\n",
        "3. **Sentence-BERT (SBERT):**\n",
        "   Fine-tuned version of BERT for **semantic similarity**.\n",
        "\n",
        "### üíª Example (SBERT)\n",
        "\n",
        "```python\n",
        "!pip install sentence-transformers --quiet\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "sentences = [\"The cat sat on the mat.\", \"A dog lay on the rug.\"]\n",
        "\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "print(\"Embedding Shape:\", embeddings.shape)\n",
        "```\n",
        "\n",
        "‚û°Ô∏è Output: `(2, 384)` ‚Äî Each sentence = 384-dimensional dense vector.\n",
        "You can then compute **cosine similarity** to measure semantic closeness.\n",
        "\n",
        "---\n",
        "\n",
        "## VIII. **Applications of Contextual Embeddings**\n",
        "\n",
        "| Task                               | Use                                         |\n",
        "| ---------------------------------- | ------------------------------------------- |\n",
        "| **Text Classification**            | Sentiment, topic, emotion detection         |\n",
        "| **Named Entity Recognition (NER)** | Extracting entities like names, dates, etc. |\n",
        "| **Question Answering**             | Understanding context in questions          |\n",
        "| **Semantic Search**                | Finding meaning-based matches               |\n",
        "| **Machine Translation**            | Context-aware language translation          |\n",
        "| **Chatbots & Summarization**       | Contextual understanding of input text      |\n",
        "\n",
        "---\n",
        "\n",
        "## IX. **Advantages vs. Traditional Methods**\n",
        "\n",
        "| Feature                | Traditional (TF-IDF / BoW) | Contextual (BERT / GPT)                 |\n",
        "| ---------------------- | -------------------------- | --------------------------------------- |\n",
        "| Word meaning           | Fixed                      | Context-aware                           |\n",
        "| Vocabulary handling    | OOV issue                  | Handles unseen words via subword tokens |\n",
        "| Sparsity               | High                       | Dense                                   |\n",
        "| Training need          | None (handcrafted)         | Pretrained deep models                  |\n",
        "| Computation            | Lightweight                | Heavy but powerful                      |\n",
        "| Semantic understanding | Weak                       | Strong                                  |\n",
        "\n",
        "---\n",
        "\n",
        "## X. **Key Takeaways**\n",
        "\n",
        "* **Contextual embeddings** revolutionized NLP by enabling models to *understand meaning in context*.\n",
        "* They are **dense, continuous, and dynamic** representations.\n",
        "* Models like **BERT, GPT, RoBERTa, and SBERT** form the backbone of modern NLP systems.\n",
        "* These embeddings power applications like ChatGPT, semantic search, and summarization.\n",
        "\n",
        "---\n",
        "\n",
        "## XI. **Practice Exercise**\n",
        "\n",
        "1. Load a small text dataset (e.g., IMDb reviews).\n",
        "2. Generate:\n",
        "\n",
        "   * TF-IDF vectors\n",
        "   * BERT sentence embeddings\n",
        "3. Compare similarity between sentences using cosine similarity.\n",
        "4. Visualize embeddings with **PCA or t-SNE**.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "gq2tmO5QQT07"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jgD4a_00QUqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### üß≠ **Summary Timeline**\n",
        "\n",
        "| Era         | Technique                         | Type            | Key Idea                   |\n",
        "| ----------- | --------------------------------- | --------------- | -------------------------- |\n",
        "| 1950s‚Äì1960s | One-Hot Encoding                  | Sparse          | Binary identity vectors    |\n",
        "| 1980s‚Äì1990s | Bag of Words (BoW)                | Sparse          | Count-based, ignores order |\n",
        "| 1990s‚Äì2000s | N-grams                           | Sparse          | Adds local context         |\n",
        "| 1990s‚Äì2000s | TF‚ÄìIDF                            | Weighted Sparse | Importance weighting       |\n",
        "| 2013+       | Word Embeddings (Word2Vec, GloVe) | Dense           | Semantic similarity        |\n",
        "| 2018+       | Contextual Embeddings (BERT, GPT) | Dense           | Meaning depends on context |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Jq02jb9cuWy9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNyoed0EsJeM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TwQWC-K1sSep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BeTrzYXOsS6m"
      }
    }
  ]
}