{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mflDZtv5sUXj"
      },
      "source": [
        "# üìò **Text Representation | NLP Lecture 4**\n",
        "\n",
        "### Topics: Bag of Words | TF-IDF | N-grams (Uni-grams, Bi-grams)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lK2urnEshri"
      },
      "source": [
        "## I. **Introduction to Text Representation**\n",
        "\n",
        "**Goal:** Convert text data into numerical form (a process also known as *Text Vectorisation* or *Feature Extraction from Text*) so that ML algorithms can process it.\n",
        "\n",
        "### üîπ Importance\n",
        "\n",
        "* **Feature Quality:** The effectiveness of ML models heavily depends on feature quality ‚Äî *‚ÄúGarbage in, garbage out.‚Äù*\n",
        "* **NLP Pipeline:** Follows data acquisition and pre-processing; crucial for ML-based NLP.\n",
        "* **Objective:** Numerical representation should capture the *semantic meaning* of text.\n",
        "\n",
        "### üîπ Challenges\n",
        "\n",
        "* Text ‚Üí Numbers is hard (unlike image or audio data).\n",
        "* Requires intelligent mapping of linguistic meaning to mathematical form.\n",
        "\n",
        "### üîπ Techniques Covered\n",
        "\n",
        "1. One-Hot Encoding\n",
        "2. Bag of Words (BoW)\n",
        "3. N-grams (Uni, Bi, Tri-grams)\n",
        "4. TF‚ÄìIDF (Term Frequency‚ÄìInverse Document Frequency)\n",
        "5. Custom Features\n",
        "   *(Future: Word Embeddings ‚Äì Word2Vec, GloVe, etc.)*\n",
        "\n",
        "---\n",
        "\n",
        "## II. **Key Terminology**\n",
        "\n",
        "| Term                     | Description                                      |\n",
        "| ------------------------ | ------------------------------------------------ |\n",
        "| **Corpus (C)**           | All words from all documents (including repeats) |\n",
        "| **Vocabulary (V)**       | Set of *unique* words from the corpus            |\n",
        "| **Document (D)**         | A single text unit (sentence, paragraph, etc.)   |\n",
        "| **Word (W) or Term (T)** | A single token in a document                     |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ3HoX_CNPUw"
      },
      "source": [
        "#### **1. One-Hot Encoding (1950s‚Äì1960s, very early NLP)**\n",
        "\n",
        "* **Idea:** Represent each word as a binary vector ‚Äî all 0s except for a single 1 in the position corresponding to that word in the vocabulary.\n",
        "* **Example:**\n",
        "  Vocabulary: [\"cat\", \"dog\", \"fish\"]\n",
        "  ‚Äúdog‚Äù ‚Üí [0, **1**, 0]\n",
        "* **Limitation:**\n",
        "\n",
        "  * Ignores context and meaning\n",
        "  * High dimensional and sparse\n",
        "- ‚ö†Ô∏è Drawbacks\n",
        "\n",
        "üîπ Disadvantages\n",
        "\n",
        "- ‚ùå Sparsity: Huge, mostly-zero matrices.\n",
        "- ‚ùå Non-fixed size: Different document lengths = different input sizes.\n",
        "- ‚ùå OOV(Out-Of-Vocabulary) problem: New words can‚Äôt be represented.\n",
        "- ‚ùå No semantics: ‚Äúwalk‚Äù and ‚Äúrun‚Äù are equally distant from ‚Äúbottle‚Äù.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsVHTOGCuf9v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BM3QwCNXusGl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "corpus = [\"dog barks\", \"cat meows\", \"dog runs\"]\n",
        "words = list(set(\" \".join(corpus).split()))  # unique vocab\n",
        "print(\"Vocabulary:\", words)\n",
        "\n",
        "# Manual one-hot encoding\n",
        "import numpy as np\n",
        "V = len(words)\n",
        "encoding = {word: np.eye(V)[i] for i, word in enumerate(words)}\n",
        "encoding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uykPTSJ1uyUK"
      },
      "source": [
        "---\n",
        "\n",
        "#### **2. Bag of Words (BoW) (1980s‚Äì1990s)**\n",
        "\n",
        "* **Idea:** Represent a document by word **counts** (or frequencies) ‚Äî order of words is ignored.\n",
        "\n",
        "- Vocabulary from corpus ‚Üí Each document = vector of word frequencies.\n",
        "- Ignores order ‚Üí treats text as a bag of words.\n",
        "- Used widely for text classification.\n",
        "\n",
        "* **Example:**\n",
        "  ‚ÄúDog bites man‚Äù and ‚ÄúMan bites dog‚Äù ‚Üí same vector (same counts).\n",
        "* **Limitation:**\n",
        "\n",
        "#### üîπ Advantages\n",
        "\n",
        "- ‚úÖ Fixed-size vector for any document\n",
        "- ‚úÖ Tolerates unseen words (ignored at inference)/OOV words ignored gracefully\n",
        "\n",
        "#### üîπ Disadvantages\n",
        "\n",
        "- ‚ùå Sparse vectors/Large vocabulary ‚Üí sparse vectors\n",
        "- ‚ùå Loses word order/Ignores word order/context (syntax)\n",
        "- ‚ùå Fails with negation ‚Äî ‚Äúgood‚Äù vs. ‚Äúnot good‚Äù appear similar\n",
        "\n",
        "\n",
        "* üîπ Scikit-learn:\n",
        "- CountVectorizer()\n",
        "\n",
        "* üîπ Key Parameters\n",
        "  - binary=True: presence (1/0) instead of count.\n",
        "  - max_features: limit vocabulary to top-N frequent words.\n",
        "  - stop_words: remove common stopwords.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9pPYxdB_wdWU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary: ['cat' 'chased' 'dog' 'log' 'mat' 'mouse' 'on' 'sat' 'the']\n",
            "   cat  chased  dog  log  mat  mouse  on  sat  the\n",
            "0    1       0    0    0    1      0   1    1    2\n",
            "1    0       0    1    1    0      0   1    1    2\n",
            "2    1       1    0    0    0      1   0    0    2\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "corpus = [\n",
        "    \"The cat sat on the mat\",\n",
        "    \"The dog sat on the log\",\n",
        "    \"The cat chased the mouse\"\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ERt8O60rw2_4"
      },
      "outputs": [],
      "source": [
        "# ‚öôÔ∏è Key Parameters\n",
        "vectorizer1 = CountVectorizer(\n",
        "    binary=True,             # presence/absence instead of counts\n",
        "    max_features=10,         # limit vocabulary size\n",
        "    stop_words='english',    # remove stopwords\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "_NWwKX8lF1ZU"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 0, 0, 0, 1, 0, 1],\n",
              "       [0, 0, 1, 1, 0, 0, 1],\n",
              "       [1, 1, 0, 0, 0, 1, 0]])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectorizer1.fit_transform(corpus).toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REc6sR6hw3aR"
      },
      "source": [
        "#### **3. N-grams (Unigram, Bigram, Trigram) (1990s‚Äì2000s)**\n",
        "\n",
        "* **Idea:** Capture **local word order** by looking at sequences of *n* words.\n",
        "## * üìò Concept\n",
        "\n",
        "- Extends BoW by including sequences of N words.\n",
        "- Captures local word order and context.\n",
        "\n",
        "* **Examples:**\n",
        "\n",
        "  * Unigrams: ‚Äúdog‚Äù, ‚Äúbites‚Äù, ‚Äúman‚Äù\n",
        "  * Bigrams: ‚Äúdog bites‚Äù, ‚Äúbites man‚Äù\n",
        "  * Trigrams: ‚Äúdog bites man‚Äù\n",
        "* **Benefit:** Adds some context awareness.\n",
        "- Captures context and short phrases\n",
        "\n",
        "- Helps handle negations and idioms\n",
        "\n",
        "* **Limitation:** Still sparse, grows combinatorially with *n*.\n",
        "  - Vocabulary size grows fast (computational cost)\n",
        "  - Still sparse and OOV issues remain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dfYVQNLw4vW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bi-gram Vocabulary: ['cat' 'cat chased' 'cat chased the' 'cat sat' 'cat sat on' 'chased'\n",
            " 'chased the' 'chased the mouse' 'dog' 'dog sat' 'dog sat on' 'log' 'mat'\n",
            " 'mouse' 'on' 'on the' 'on the log' 'on the mat' 'sat' 'sat on'\n",
            " 'sat on the' 'the' 'the cat' 'the cat chased' 'the cat sat' 'the dog'\n",
            " 'the dog sat' 'the log' 'the mat' 'the mouse']\n",
            "   cat  cat chased  cat chased the  cat sat  cat sat on  chased  chased the  \\\n",
            "0    1           0               0        1           1       0           0   \n",
            "1    0           0               0        0           0       0           0   \n",
            "2    1           1               1        0           0       1           1   \n",
            "\n",
            "   chased the mouse  dog  dog sat  ...  sat on the  the  the cat  \\\n",
            "0                 0    0        0  ...           1    2        1   \n",
            "1                 0    1        1  ...           1    2        0   \n",
            "2                 1    0        0  ...           0    2        1   \n",
            "\n",
            "   the cat chased  the cat sat  the dog  the dog sat  the log  the mat  \\\n",
            "0               0            1        0            0        0        1   \n",
            "1               0            0        1            1        1        0   \n",
            "2               1            0        0            0        0        0   \n",
            "\n",
            "   the mouse  \n",
            "0          0  \n",
            "1          0  \n",
            "2          1  \n",
            "\n",
            "[3 rows x 30 columns]\n",
            "==============================\n",
            "30\n"
          ]
        }
      ],
      "source": [
        "# Bi-gram example\n",
        "bi_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
        "# (2, 2): Setting the range from 2 (minimum n-value) to 2 (maximum n-value)\n",
        "# so (2,2 means the vectorizer will only consider sequences of exactly two words, known as bigrams.\n",
        "X_bi = bi_vectorizer.fit_transform(corpus)\n",
        "corpus = [\n",
        "    \"The cat sat on the mat\",\n",
        "    \"The dog sat on the log\",\n",
        "    \"The cat chased the mouse\"\n",
        "]\n",
        "print(\"Bi-gram Vocabulary:\", bi_vectorizer.get_feature_names_out())\n",
        "print(pd.DataFrame(X_bi.toarray(), columns=bi_vectorizer.get_feature_names_out()))\n",
        "print(\"==============================\")\n",
        "print(len(bi_vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_LQa7TAT-nR"
      },
      "outputs": [],
      "source": [
        "# Uni + Bi + Tri-grams\n",
        "combo_vectorizer = CountVectorizer(ngram_range=(1,3))\n",
        "X_combo = combo_vectorizer.fit_transform(corpus)\n",
        "print(\"Combined Vocabulary Size:\", len(combo_vectorizer.get_feature_names_out()))\n",
        "\n",
        "print(X_combo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boJffXGbUOcY"
      },
      "source": [
        "---\n",
        "\n",
        "#### **4. TF‚ÄìIDF (Term Frequency‚ÄìInverse Document Frequency) (1990s‚Äì2000s)**\n",
        "\n",
        "* **Idea:** Weigh words by importance ‚Äî frequent in a document but rare across the corpus.\n",
        "- Assigns importance weights to words instead of raw counts.\n",
        "* **Formula:**\n",
        "  TF-IDF(T,D)=TF(T,D)√óIDF(T)\n",
        "\n",
        "- TF= Word occurrences in doc/Total words in doc'\n",
        "\n",
        "- Inverse Document Frequency (IDF):\n",
        "- IDF=log(N/n_T)\n",
        "\n",
        "Weight = TF √ó IDF\n",
        "* **Benefit:** Reduces impact of common words like ‚Äúthe‚Äù, ‚Äúand‚Äù.\n",
        "* **Limitation:** Still based on counts, no semantic understanding.\n",
        "\n",
        "* ‚úÖ Advantages\n",
        "\n",
        "Reduces weight of common words\n",
        "Useful for information retrieval (e.g., search engines)\n",
        "\n",
        "* ‚ùå Disadvantages\n",
        "\n",
        "- Sparse matrix\n",
        "- OOV issue\n",
        "- Still no deep semantic relation captured\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAwXJGMhL54e"
      },
      "source": [
        "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XeYB3-C2cD3KIi975ql4_g.png)\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*9tecGbVZZ8wxniSPoiXcYA.png)\n",
        "\n",
        "https://medium.com/@abhishekjainindore24/tf-idf-in-nlp-term-frequency-inverse-document-frequency-e05b65932f1d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVZP2Xr0w7Ya"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHzzJvUky5Zx",
        "outputId": "d576f3af-8c3f-4f7e-a295-724dc9ddb781"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary: ['cat' 'chased' 'dog' 'log' 'mat' 'mouse' 'on' 'sat' 'the']\n",
            "        cat    chased       dog       log       mat     mouse        on  \\\n",
            "0  0.374207  0.000000  0.000000  0.000000  0.492038  0.000000  0.374207   \n",
            "1  0.000000  0.000000  0.468699  0.468699  0.000000  0.000000  0.356457   \n",
            "2  0.381519  0.501651  0.000000  0.000000  0.000000  0.501651  0.000000   \n",
            "\n",
            "        sat       the  \n",
            "0  0.374207  0.581211  \n",
            "1  0.356457  0.553642  \n",
            "2  0.000000  0.592567  \n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"The cat sat on the mat\",\n",
        "    \"The dog sat on the log\",\n",
        "    \"The cat chased the mouse\"\n",
        "]\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "X_tfidf = tfidf.fit_transform(corpus)\n",
        "\n",
        "print(\"Vocabulary:\", tfidf.get_feature_names_out())\n",
        "print(pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoxwwPuwy9q0"
      },
      "source": [
        "#### **5. Word Embeddings (Word2Vec, GloVe, FastText, etc.) (2013 onward)**\n",
        "\n",
        "* **Idea:** Learn **dense, low-dimensional vectors** where similar words are close in vector space.\n",
        "* **Example:**\n",
        "  Vector(‚Äúking‚Äù) ‚Äì Vector(‚Äúman‚Äù) + Vector(‚Äúwoman‚Äù) ‚âà Vector(‚Äúqueen‚Äù)\n",
        "* **Benefit:** Captures **semantic meaning** and **relationships** between words.\n",
        "* **Limitation:** Fixed for each word ‚Äî context-independent.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lt9MoY6cI1B"
      },
      "source": [
        "https://www.cs.cmu.edu/~dst/WordEmbeddingDemo/tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dq7UiDq1zB0p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qnwa635aGiNz"
      },
      "source": [
        "# üß† **1. Word2Vec**\n",
        "\n",
        "### üìò **Definition**\n",
        "\n",
        "**Word2Vec** (by Google, 2013) is a **neural embedding model** that learns to represent words as dense vectors.\n",
        "It uses two main architectures:\n",
        "\n",
        "* **CBOW (Continuous Bag of Words):** Predicts a word based on its context.\n",
        "* **Skip-Gram:** Predicts context words given a target word.\n",
        "\n",
        "These embeddings capture **semantic and syntactic relationships** between words ‚Äî e.g.,\n",
        "`vector(\"king\") - vector(\"man\") + vector(\"woman\") ‚âà vector(\"queen\")`.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Advantages**\n",
        "\n",
        "* Captures both **semantic** and **syntactic** relationships.\n",
        "* Trains efficiently on large datasets.\n",
        "* Performs well in many NLP downstream tasks.\n",
        "\n",
        "### ‚ùå **Disadvantages**\n",
        "\n",
        "* Doesn‚Äôt handle **out-of-vocabulary (OOV)** words.\n",
        "* Ignores **subword (morphological)** information.\n",
        "* Embeddings depend heavily on training data quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38czyPwudfEj",
        "outputId": "58683d59-8a2e-4418-ab06-b19983b261eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp310-cp310-win_amd64.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\hp\\anaconda3\\envs\\aienv310\\lib\\site-packages (from gensim) (2.2.6)\n",
            "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\hp\\anaconda3\\envs\\aienv310\\lib\\site-packages (from gensim) (1.15.3)\n",
            "Collecting smart_open>=1.8.1 (from gensim)\n",
            "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting wrapt (from smart_open>=1.8.1->gensim)\n",
            "  Using cached wrapt-2.0.1-cp310-cp310-win_amd64.whl.metadata (9.2 kB)\n",
            "Downloading gensim-4.4.0-cp310-cp310-win_amd64.whl (24.4 MB)\n",
            "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
            "   - -------------------------------------- 1.0/24.4 MB 12.7 MB/s eta 0:00:02\n",
            "   - -------------------------------------- 1.0/24.4 MB 12.7 MB/s eta 0:00:02\n",
            "   ------ --------------------------------- 3.7/24.4 MB 6.4 MB/s eta 0:00:04\n",
            "   ------------ --------------------------- 7.6/24.4 MB 9.6 MB/s eta 0:00:02\n",
            "   --------------- ------------------------ 9.4/24.4 MB 10.9 MB/s eta 0:00:02\n",
            "   --------------- ------------------------ 9.4/24.4 MB 10.9 MB/s eta 0:00:02\n",
            "   --------------- ------------------------ 9.4/24.4 MB 10.9 MB/s eta 0:00:02\n",
            "   ---------------- ----------------------- 10.2/24.4 MB 6.0 MB/s eta 0:00:03\n",
            "   ----------------------- ---------------- 14.4/24.4 MB 7.4 MB/s eta 0:00:02\n",
            "   -------------------------------- ------- 19.9/24.4 MB 9.3 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 23.1/24.4 MB 10.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------  23.9/24.4 MB 9.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 24.4/24.4 MB 8.9 MB/s  0:00:03\n",
            "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
            "Using cached wrapt-2.0.1-cp310-cp310-win_amd64.whl (60 kB)\n",
            "Installing collected packages: wrapt, smart_open, gensim\n",
            "\n",
            "   ---------------------------------------- 0/3 [wrapt]\n",
            "   ------------- -------------------------- 1/3 [smart_open]\n",
            "   ------------- -------------------------- 1/3 [smart_open]\n",
            "   ------------- -------------------------- 1/3 [smart_open]\n",
            "   ------------- -------------------------- 1/3 [smart_open]\n",
            "   ------------- -------------------------- 1/3 [smart_open]\n",
            "   -------------------------- ------------- 2/3 [gensim]\n",
            "   -------------------------- ------------- 2/3 [gensim]\n",
            "   -------------------------- ------------- 2/3 [gensim]\n",
            "   -------------------------- ------------- 2/3 [gensim]\n",
            "   -------------------------- ------------- 2/3 [gensim]\n",
            "   -------------------------- ------------- 2/3 [gensim]\n",
            "   -------------------------- ------------- 2/3 [gensim]\n",
            "   -------------------------- ------------- 2/3 [gensim]\n",
            "   -------------------------- ------------- 2/3 [gensim]\n",
            "   -------------------------- ------------- 2/3 [gensim]\n",
            "   -------------------------- ------------- 2/3 [gensim]\n",
            "   -------------------------- ------------- 2/3 [gensim]\n",
            "   -------------------------- ------------- 2/3 [gensim]\n",
            "   -------------------------- ------------- 2/3 [gensim]\n",
            "   -------------------------- ------------- 2/3 [gensim]\n",
            "   -------------------------- ------------- 2/3 [gensim]\n",
            "   -------------------------- ------------- 2/3 [gensim]\n",
            "   -------------------------- ------------- 2/3 [gensim]\n",
            "   -------------------------- ------------- 2/3 [gensim]\n",
            "   -------------------------- ------------- 2/3 [gensim]\n",
            "   -------------------------- ------------- 2/3 [gensim]\n",
            "   -------------------------- ------------- 2/3 [gensim]\n",
            "   ---------------------------------------- 3/3 [gensim]\n",
            "\n",
            "Successfully installed gensim-4.4.0 smart_open-7.5.0 wrapt-2.0.1\n"
          ]
        }
      ],
      "source": [
        "### üíª **Code Example**\n",
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHmO9JYzESBq",
        "outputId": "71fc677c-bf92-4c28-fa98-a2b03f63cf7d"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gensim'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m simple_preprocess\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Sample corpus\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# Sample corpus\n",
        "sentences = [\n",
        "    \"I love deep learning and natural language processing\",\n",
        "    \"Word embeddings are useful for NLP tasks\",\n",
        "    \"Word2Vec is a great model for learning word vectors\"\n",
        "]\n",
        "# Preprocessing\n",
        "tokenized_sentences = [simple_preprocess(sentence) for sentence in sentences]\n",
        "\n",
        "tokenized_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFbDUfP9V3kk",
        "outputId": "650cf1e6-a2eb-4568-aac0-92176284204e"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Word2Vec' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train Word2Vec model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model_w2v \u001b[38;5;241m=\u001b[39m \u001b[43mWord2Vec\u001b[49m(sentences\u001b[38;5;241m=\u001b[39mtokenized_sentences, vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, sg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m  \u001b[38;5;66;03m# vector_size=100: The size of the dense vector (embedding) for each word, determining its features.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# window=5: The maximum distance between the target word and context words (5 words to the left and 5 to the right).\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# min_count=1: Ignores all words with a total frequency lower than 1 (i.e., includes every word that appears at least once).\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# sg=1: Specifies the Skip-Gram training algorithm, which predicts context words from a target word (better for infrequent words). (Use sg=0 for CBOW - Continuous Bag of Words).\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Check vector and similar words\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_w2v\u001b[38;5;241m.\u001b[39mwv[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
            "\u001b[1;31mNameError\u001b[0m: name 'Word2Vec' is not defined"
          ]
        }
      ],
      "source": [
        "# Train Word2Vec model\n",
        "model_w2v = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
        " # vector_size=100: The size of the dense vector (embedding) for each word, determining its features.\n",
        "# window=5: The maximum distance between the target word and context words (5 words to the left and 5 to the right).\n",
        "# min_count=1: Ignores all words with a total frequency lower than 1 (i.e., includes every word that appears at least once).\n",
        "# sg=1: Specifies the Skip-Gram training algorithm, which predicts context words from a target word (better for infrequent words). (Use sg=0 for CBOW - Continuous Bag of Words).\n",
        "\n",
        "# Check vector and similar words\n",
        "print(model_w2v.wv[\"learning\"])\n",
        "print(model_w2v.wv.most_similar(\"learning\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKHxVOSvGoIn"
      },
      "source": [
        "# üìó **2. GloVe (Global Vectors for Word Representation)**\n",
        "\n",
        "### üìò **Definition**\n",
        "\n",
        "**GloVe** (by Stanford, 2014) learns word embeddings by analyzing **global word co-occurrence statistics** across the entire corpus.\n",
        "It focuses on how often words appear together ‚Äî building a **co-occurrence matrix**, then factorizing it to learn embeddings.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Advantages**\n",
        "\n",
        "* Captures **global context** better than Word2Vec (which is local).\n",
        "* Produces **consistent** embeddings using statistical information.\n",
        "* Pretrained models available (trained on huge corpora like Wikipedia).\n",
        "\n",
        "### ‚ùå **Disadvantages**\n",
        "\n",
        "* Doesn‚Äôt handle **OOV** words.\n",
        "* Cannot learn new embeddings once pretrained.\n",
        "* Needs large memory for co-occurrence matrix.\n",
        "\n",
        "---\n",
        "\n",
        "### üíª **Code Example**\n",
        "\n",
        "```python\n",
        "# !pip install torchtext torch\n",
        "\n",
        "from torchtext.vocab import GloVe\n",
        "import torch\n",
        "\n",
        "# Load pretrained GloVe embeddings (50 dimensions)\n",
        "glove = GloVe(name=\"6B\", dim=50)\n",
        "\n",
        "# Get vector for a word\n",
        "word_vec = glove[\"computer\"]\n",
        "print(word_vec)\n",
        "\n",
        "# Compute similarity\n",
        "sim = torch.cosine_similarity(glove[\"king\"].unsqueeze(0), glove[\"queen\"].unsqueeze(0))\n",
        "print(f\"Similarity(king, queen): {sim.item():.4f}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgCv5BZ-G4uA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s38A17AbG5Ut"
      },
      "source": [
        "---\n",
        "\n",
        "# üìò **3. FastText**\n",
        "\n",
        "### üìò **Definition**\n",
        "\n",
        "**FastText** (by Facebook, 2016) extends Word2Vec by representing each word as a **bag of character n-grams**.\n",
        "This allows it to understand **morphology** and generate embeddings for unseen words.\n",
        "\n",
        "Example:\n",
        "`\"playing\"` ‚Üí `[\"pla\", \"lay\", \"ayi\", \"yin\", \"ing\"]`\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Advantages**\n",
        "\n",
        "* Handles **out-of-vocabulary** (OOV) words.\n",
        "* Captures **subword information** (prefixes, suffixes).\n",
        "* Works well for **morphologically rich languages** (like German or Turkish).\n",
        "\n",
        "### ‚ùå **Disadvantages**\n",
        "\n",
        "* Slightly **slower** to train than Word2Vec.\n",
        "* Requires more memory.\n",
        "* Subword info might not always improve performance (for small datasets).\n",
        "\n",
        "---\n",
        "\n",
        "### üíª **Code Example**\n",
        "\n",
        "```python\n",
        "from gensim.models import FastText\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# Corpus\n",
        "sentences = [\n",
        "    \"I love machine learning and data science\",\n",
        "    \"FastText creates embeddings using subwords\",\n",
        "    \"It can handle unseen words like learnings\"\n",
        "]\n",
        "\n",
        "tokenized_sentences = [simple_preprocess(s) for s in sentences]\n",
        "\n",
        "# Train FastText model\n",
        "model_ft = FastText(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Vector for a known word\n",
        "print(model_ft.wv[\"learning\"])\n",
        "\n",
        "# Vector for an unseen (OOV) word\n",
        "print(model_ft.wv[\"learnings\"])  # Works!\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUPhp2wZESnl"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# üìä **üîç Summary Table**\n",
        "\n",
        "| Model        | Year | Creator  | Key Idea                    | Handles OOV | Pros                              | Cons                       |\n",
        "| ------------ | ---- | -------- | --------------------------- | ----------- | --------------------------------- | -------------------------- |\n",
        "| **Word2Vec** | 2013 | Google   | Predictive (CBOW/Skip-Gram) | ‚ùå           | Semantic relationships, efficient | No OOV, ignores morphology |\n",
        "| **GloVe**    | 2014 | Stanford | Global co-occurrence matrix | ‚ùå           | Global context, pretrained models | Static, no OOV             |\n",
        "| **FastText** | 2016 | Facebook | Subword (character n-grams) | ‚úÖ           | Handles OOV, morphological info   | Slower, more memory        |\n",
        "\n",
        "---\n",
        "\n",
        "Would you like me to add a **visualization (PCA or t-SNE)** to compare how similar words cluster across these models?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lapO0s2hER58"
      },
      "outputs": [],
      "source": [
        "# https://projector.tensorflow.org/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-bqQv1EERvh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnQOlQS7ERhj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtW5b4QLz4T4"
      },
      "source": [
        "\n",
        "\n",
        "#### **6. Contextual Word Embeddings (BERT, GPT, etc.) (2018 onward)**\n",
        "\n",
        "* **Idea:** Represent words **in context**, so ‚Äúbank‚Äù in ‚Äúriver bank‚Äù ‚â† ‚Äúbank‚Äù in ‚Äúmoney bank‚Äù.\n",
        "* **Examples:** ELMo (2018), BERT (2018), GPT series (2018+)\n",
        "* **Benefit:** State-of-the-art performance across NLP tasks.\n",
        "* **Limitation:** Computationally expensive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kchei7NfRcWy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xfhpa5kVRc6L"
      },
      "source": [
        "# üß† **Contextual Word Embeddings | NLP Lecture 5**\n",
        "\n",
        "---\n",
        "\n",
        "## I. **Introduction**\n",
        "\n",
        "Traditional techniques like **One-Hot**, **BoW**, **N-grams**, and **TF‚ÄìIDF** treat each word as **independent** and **context-free**.\n",
        "They fail to capture:\n",
        "\n",
        "* **Meaning differences** based on context (e.g., *bank* = river bank vs. money bank)\n",
        "* **Semantic similarity** between related words (*good*, *great*, *excellent*)\n",
        "\n",
        "‚û°Ô∏è **Contextual Word Embeddings** solve these issues using **deep learning**.\n",
        "\n",
        "---\n",
        "\n",
        "## II. **From Static to Contextual Embeddings**\n",
        "\n",
        "### 1. **Static Embeddings (Word2Vec, GloVe, FastText)**\n",
        "\n",
        "* Each word has **one fixed vector**, no matter where it appears.\n",
        "* ‚Äúbank‚Äù ‚Üí same vector in both ‚Äúriver bank‚Äù and ‚Äúbank loan‚Äù.\n",
        "* Captures general *semantic similarity*, but **no context**.\n",
        "\n",
        "### 2. **Contextual Embeddings (ELMo, BERT, GPT, etc.)**\n",
        "\n",
        "* Word meaning changes **depending on its sentence context**.\n",
        "* ‚Äúbank‚Äù in ‚Äúriver bank‚Äù and ‚Äúbank loan‚Äù ‚Üí **different embeddings**.\n",
        "* Generated dynamically by **transformer-based** neural networks.\n",
        "\n",
        "---\n",
        "\n",
        "## III. **How Contextual Embeddings Work**\n",
        "\n",
        "### üîπ **Architecture**\n",
        "\n",
        "Most modern embeddings are based on the **Transformer architecture**, introduced in *‚ÄúAttention is All You Need‚Äù (Vaswani et al., 2017)*.\n",
        "\n",
        "Key idea:\n",
        "üëâ Use **Self-Attention** to learn relationships between all words in a sentence.\n",
        "\n",
        "### üîπ **Self-Attention**\n",
        "\n",
        "* Every word looks at all other words to understand its context.\n",
        "* Example:\n",
        "\n",
        "  * Sentence: *‚ÄúThe bank of the river was flooded.‚Äù*\n",
        "  * The model looks at ‚Äúriver‚Äù ‚Üí understands that *bank* refers to a geographical feature.\n",
        "\n",
        "---\n",
        "\n",
        "## IV. **Popular Contextual Embedding Models**\n",
        "\n",
        "| Model               | Type                  | Key Idea                              | Notes                        |\n",
        "| ------------------- | --------------------- | ------------------------------------- | ---------------------------- |\n",
        "| **ELMo (2018)**     | BiLSTM                | Contextual from both directions       | First major contextual model |\n",
        "| **BERT (2019)**     | Transformer           | Bidirectional context using attention | Widely used in NLP tasks     |\n",
        "| **GPT (2018‚Äì2024)** | Transformer (decoder) | Predicts next word (left-to-right)    | Used for generation tasks    |\n",
        "| **RoBERTa**         | BERT variant          | More training, better performance     |                              |\n",
        "| **DistilBERT**      | Lightweight BERT      | 40% smaller, similar accuracy         |                              |\n",
        "\n",
        "---\n",
        "\n",
        "## V. **Example: Using BERT Embeddings**\n",
        "\n",
        "### üíª **Code Example (Using `transformers` library)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJwqKvPbZzZR"
      },
      "source": [
        "![](https://miro.medium.com/v2/resize:fit:1200/1*4bg3WZRcZMYFfler0StxXQ.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XQPGyBfRdYr"
      },
      "outputs": [],
      "source": [
        "# https://medium.com/@lmpo/a-brief-history-of-lmms-from-transformers-2017-to-deepseek-r1-2025-dae75dd3f59a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1zv78a1SWEJ"
      },
      "source": [
        "üîπ Example 1 ‚Äî BERT (Bidirectional Encoder Representations from Transformers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJg7VqdHSVL8"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers torch\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Load pretrained model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Example sentences\n",
        "sentence = \"I went to the bank to withdraw money.\"\n",
        "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "\n",
        "# Get embeddings\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Extract last hidden states (contextual embeddings)\n",
        "last_hidden_states = outputs.last_hidden_state\n",
        "print(last_hidden_states.shape)  # [batch_size, sequence_length, hidden_size]\n",
        "\n",
        "# Example: get embedding for \"bank\"\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "print(tokens)\n",
        "bank_index = tokens.index(\"bank\")\n",
        "print(f\"Embedding for 'bank':\\n\", last_hidden_states[0, bank_index, :5])  # first 5 dims\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbqf242kSb39"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynh-520DScVT"
      },
      "source": [
        "üîπ Example 2 ‚Äî GPT (Generative Pre-trained Transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRF7qk53Se2D"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2Model.from_pretrained(\"gpt2\")\n",
        "\n",
        "sentence = \"The river bank was full of trees.\"\n",
        "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Get last hidden states\n",
        "embeddings = outputs.last_hidden_state\n",
        "print(embeddings.shape)\n",
        "\n",
        "# Inspect token-level embeddings\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VqwsOlPSmuk"
      },
      "source": [
        "üîπ Example 3 ‚Äî Compare Contexts for ‚Äúbank‚Äù"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cG8eZm3SVmm"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    \"I deposited money in the bank.\",\n",
        "    \"We sat by the river bank and watched the water.\"\n",
        "]\n",
        "\n",
        "for sentence in sentences:\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "    bank_index = tokens.index(\"bank\")\n",
        "    embedding = outputs.last_hidden_state[0, bank_index, :]\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Bank embedding (first 5 dims): {embedding[:5]}\")\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga5Xr8mZSxD1"
      },
      "source": [
        "| Model    | Year       | Architecture        | Context Type  | Pros                         | Cons                       |\n",
        "| -------- | ---------- | ------------------- | ------------- | ---------------------------- | -------------------------- |\n",
        "| **ELMo** | 2018       | Bi-LSTM             | Bidirectional | Context-aware, simple        | Slower, older architecture |\n",
        "| **BERT** | 2018       | Transformer Encoder | Bidirectional | Best for understanding tasks | Heavy, non-generative      |\n",
        "| **GPT**  | 2018‚Äì2023+ | Transformer Decoder | Left-to-right | Generative power, flexible   | Unidirectional context     |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6k9dnlCSqit"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLu_rslqSrAG"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq2tmO5QQT07"
      },
      "source": [
        "\n",
        "\n",
        "```python\n",
        "!pip install transformers torch --quiet\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Load pretrained BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"The bank of the river was flooded.\"\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(sentence, return_tensors='pt')\n",
        "\n",
        "# Get embeddings\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# outputs contains:\n",
        "# - last_hidden_state: embeddings for each token\n",
        "# - pooler_output: embedding for entire sentence (CLS token)\n",
        "word_embeddings = outputs.last_hidden_state\n",
        "sentence_embedding = outputs.pooler_output\n",
        "\n",
        "print(\"Word Embeddings Shape:\", word_embeddings.shape)\n",
        "print(\"Sentence Embedding Shape:\", sentence_embedding.shape)\n",
        "```\n",
        "\n",
        "### üß© **Output Explanation**\n",
        "\n",
        "* `word_embeddings`: Tensor of shape `[1, num_tokens, 768]` ‚Üí one 768-dim vector per token.\n",
        "* `sentence_embedding`: `[1, 768]` ‚Üí condensed representation of entire sentence.\n",
        "\n",
        "---\n",
        "\n",
        "## VI. **Understanding the Embedding Dimensions**\n",
        "\n",
        "| Model           | Vector Size | Layers | Context Type         |\n",
        "| --------------- | ----------- | ------ | -------------------- |\n",
        "| **BERT-base**   | 768         | 12     | Bidirectional        |\n",
        "| **BERT-large**  | 1024        | 24     | Bidirectional        |\n",
        "| **GPT-2 small** | 768         | 12     | Unidirectional       |\n",
        "| **ELMo**        | 1024        | 2      | Bidirectional (LSTM) |\n",
        "\n",
        "Each embedding dimension encodes complex linguistic patterns such as syntax, semantics, and relationships.\n",
        "\n",
        "---\n",
        "\n",
        "## VII. **Sentence-Level Embeddings**\n",
        "\n",
        "Instead of token-level vectors, sometimes we need **whole-sentence embeddings** (for tasks like similarity or classification).\n",
        "\n",
        "### üîπ Options:\n",
        "\n",
        "1. **[CLS] Token (BERT):**\n",
        "   Use the first token embedding (sentence representation).\n",
        "2. **Mean Pooling:**\n",
        "   Average over all token embeddings.\n",
        "3. **Sentence-BERT (SBERT):**\n",
        "   Fine-tuned version of BERT for **semantic similarity**.\n",
        "\n",
        "### üíª Example (SBERT)\n",
        "\n",
        "```python\n",
        "!pip install sentence-transformers --quiet\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "sentences = [\"The cat sat on the mat.\", \"A dog lay on the rug.\"]\n",
        "\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "print(\"Embedding Shape:\", embeddings.shape)\n",
        "```\n",
        "\n",
        "‚û°Ô∏è Output: `(2, 384)` ‚Äî Each sentence = 384-dimensional dense vector.\n",
        "You can then compute **cosine similarity** to measure semantic closeness.\n",
        "\n",
        "---\n",
        "\n",
        "## VIII. **Applications of Contextual Embeddings**\n",
        "\n",
        "| Task                               | Use                                         |\n",
        "| ---------------------------------- | ------------------------------------------- |\n",
        "| **Text Classification**            | Sentiment, topic, emotion detection         |\n",
        "| **Named Entity Recognition (NER)** | Extracting entities like names, dates, etc. |\n",
        "| **Question Answering**             | Understanding context in questions          |\n",
        "| **Semantic Search**                | Finding meaning-based matches               |\n",
        "| **Machine Translation**            | Context-aware language translation          |\n",
        "| **Chatbots & Summarization**       | Contextual understanding of input text      |\n",
        "\n",
        "---\n",
        "\n",
        "## IX. **Advantages vs. Traditional Methods**\n",
        "\n",
        "| Feature                | Traditional (TF-IDF / BoW) | Contextual (BERT / GPT)                 |\n",
        "| ---------------------- | -------------------------- | --------------------------------------- |\n",
        "| Word meaning           | Fixed                      | Context-aware                           |\n",
        "| Vocabulary handling    | OOV issue                  | Handles unseen words via subword tokens |\n",
        "| Sparsity               | High                       | Dense                                   |\n",
        "| Training need          | None (handcrafted)         | Pretrained deep models                  |\n",
        "| Computation            | Lightweight                | Heavy but powerful                      |\n",
        "| Semantic understanding | Weak                       | Strong                                  |\n",
        "\n",
        "---\n",
        "\n",
        "## X. **Key Takeaways**\n",
        "\n",
        "* **Contextual embeddings** revolutionized NLP by enabling models to *understand meaning in context*.\n",
        "* They are **dense, continuous, and dynamic** representations.\n",
        "* Models like **BERT, GPT, RoBERTa, and SBERT** form the backbone of modern NLP systems.\n",
        "* These embeddings power applications like ChatGPT, semantic search, and summarization.\n",
        "\n",
        "---\n",
        "\n",
        "## XI. **Practice Exercise**\n",
        "\n",
        "1. Load a small text dataset (e.g., IMDb reviews).\n",
        "2. Generate:\n",
        "\n",
        "   * TF-IDF vectors\n",
        "   * BERT sentence embeddings\n",
        "3. Compare similarity between sentences using cosine similarity.\n",
        "4. Visualize embeddings with **PCA or t-SNE**.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgD4a_00QUqe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq02jb9cuWy9"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### üß≠ **Summary Timeline**\n",
        "\n",
        "| Era         | Technique                         | Type            | Key Idea                   |\n",
        "| ----------- | --------------------------------- | --------------- | -------------------------- |\n",
        "| 1950s‚Äì1960s | One-Hot Encoding                  | Sparse          | Binary identity vectors    |\n",
        "| 1980s‚Äì1990s | Bag of Words (BoW)                | Sparse          | Count-based, ignores order |\n",
        "| 1990s‚Äì2000s | N-grams                           | Sparse          | Adds local context         |\n",
        "| 1990s‚Äì2000s | TF‚ÄìIDF                            | Weighted Sparse | Importance weighting       |\n",
        "| 2013+       | Word Embeddings (Word2Vec, GloVe) | Dense           | Semantic similarity        |\n",
        "| 2018+       | Contextual Embeddings (BERT, GPT) | Dense           | Meaning depends on context |\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNyoed0EsJeM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwQWC-K1sSep"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeTrzYXOsS6m"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO852toOD1okEX4NQ4T3/4t",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aienv310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
