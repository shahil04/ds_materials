{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdjurCnBebJ-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect üëç\n",
    "Below is a **FROM-SCRATCH Transformer implementation** that you can **teach line-by-line** and **run live in class**.\n",
    "It works for **both non-technical (concept first)** and **technical (code + math)** students.\n",
    "\n",
    "I‚Äôll follow this flow (very important for teaching):\n",
    "\n",
    "> **Attention ‚Üí Transformer Block ‚Üí Mini Transformer ‚Üí Example Run**\n",
    "\n",
    "---\n",
    "\n",
    "# üß† Step 0: What We Are Building (Explain to Students)\n",
    "\n",
    "![Image](https://sebastianraschka.com/images/blog/2023/self-attention-from-scratch/summary.png?utm_source=chatgpt.com)\n",
    "\n",
    "![Image](https://daxg39y63pxwu.cloudfront.net/images/blog/transformers-architecture/Components_of_Transformer_Architecture.png?utm_source=chatgpt.com)\n",
    "\n",
    "![Image](https://www.researchgate.net/publication/342774739/figure/fig5/AS%3A941464695623704%401601474083378/An-example-of-multi-head-attention-visualization-for-the-forward-utterances-in-the.png?utm_source=chatgpt.com)\n",
    "\n",
    "### üéØ Goal\n",
    "\n",
    "We will build a **tiny Transformer** that:\n",
    "\n",
    "* Reads a sentence\n",
    "* Learns word relationships\n",
    "* Outputs transformed vectors\n",
    "\n",
    "‚ö†Ô∏è This is **NOT BERT or GPT**\n",
    "This is the **ENGINE inside them**\n",
    "\n",
    "---\n",
    "\n",
    "# üß© Step 1: Self-Attention (CORE IDEA)\n",
    "\n",
    "## üß† Non-Technical Explanation\n",
    "\n",
    "> ‚ÄúEach word looks at other words and decides **who is important**.‚Äù\n",
    "\n",
    "Example:\n",
    "\n",
    "> **‚ÄúVirat hit a century because he was confident‚Äù**\n",
    "> ‚Üí ‚Äúhe‚Äù attends strongly to ‚ÄúVirat‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Technical Code: Self-Attention from Scratch\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "```\n",
    "\n",
    "```python\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key   = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "\n",
    "        # Attention score\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.embed_dim)\n",
    "        attention = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        output = torch.matmul(attention, V)\n",
    "        return output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Explain This to Students\n",
    "\n",
    "| Code    | Meaning               |\n",
    "| ------- | --------------------- |\n",
    "| Q       | What am I looking for |\n",
    "| K       | What do I offer       |\n",
    "| V       | Actual information    |\n",
    "| softmax | Importance score      |\n",
    "\n",
    "---\n",
    "\n",
    "# üß© Step 2: Multi-Head Attention (Parallel Thinking)\n",
    "\n",
    "## üß† Non-Technical\n",
    "\n",
    "> ‚ÄúInstead of **one brain**, Transformer uses **many brains in parallel**.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Code: Multi-Head Attention\n",
    "\n",
    "```python\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, heads):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_dim // heads\n",
    "        \n",
    "        assert self.head_dim * heads == embed_dim\n",
    "        \n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys   = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries= nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, seq_len, _ = x.shape\n",
    "        x = x.reshape(N, seq_len, self.heads, self.head_dim)\n",
    "\n",
    "        values  = self.values(x)\n",
    "        keys    = self.keys(x)\n",
    "        queries = self.queries(x)\n",
    "\n",
    "        scores = torch.einsum(\"nqhd,nkhd->nhqk\", queries, keys)\n",
    "        attention = torch.softmax(scores / math.sqrt(self.head_dim), dim=-1)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", attention, values)\n",
    "        out = out.reshape(N, seq_len, self.embed_dim)\n",
    "        return self.fc_out(out)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üß© Step 3: Transformer Block (Real Transformer)\n",
    "\n",
    "![Image](https://res.cloudinary.com/edlitera/image/upload/c_fill%2Cf_auto/v1680253949/blog/tggrmtbkds6pbnqzt782?utm_source=chatgpt.com)\n",
    "\n",
    "![Image](https://i.sstatic.net/eAKQu.png?utm_source=chatgpt.com)\n",
    "\n",
    "![Image](https://miro.medium.com/1%2A9M945AONYTHLFOtrTLz45Q.png?utm_source=chatgpt.com)\n",
    "\n",
    "### Contains:\n",
    "\n",
    "* Multi-Head Attention\n",
    "* Feed Forward Network\n",
    "* Residual + LayerNorm\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Code: Transformer Block\n",
    "\n",
    "```python\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(embed_dim, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention = self.attention(x)\n",
    "        x = self.norm1(attention + x)\n",
    "        \n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.norm2(forward + x)\n",
    "        return out\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üß© Step 4: Positional Encoding (Word Order)\n",
    "\n",
    "## üß† Non-Technical\n",
    "\n",
    "> Transformer does not know word order ‚Üí we **inject position info**\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Code: Positional Encoding\n",
    "\n",
    "```python\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=100):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim)\n",
    "        )\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üß© Step 5: Full Mini Transformer Model\n",
    "\n",
    "```python\n",
    "class MiniTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, heads):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position = PositionalEncoding(embed_dim)\n",
    "        self.transformer = TransformerBlock(embed_dim, heads)\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.position(x)\n",
    "        x = self.transformer(x)\n",
    "        return self.fc_out(x)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üß™ Step 6: Example Run (LIVE DEMO)\n",
    "\n",
    "```python\n",
    "# Vocabulary\n",
    "vocab = {\n",
    "    \"i\":0, \"love\":1, \"ai\":2, \"transformers\":3\n",
    "}\n",
    "\n",
    "sentence = [\"i\", \"love\", \"ai\"]\n",
    "input_ids = torch.tensor([[vocab[word] for word in sentence]])\n",
    "\n",
    "model = MiniTransformer(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=32,\n",
    "    heads=4\n",
    ")\n",
    "\n",
    "output = model(input_ids)\n",
    "print(output.shape)\n",
    "```\n",
    "\n",
    "### Output:\n",
    "\n",
    "```\n",
    "torch.Size([1, 3, 4])\n",
    "```\n",
    "\n",
    "‚úî Each word now has learned contextual meaning\n",
    "\n",
    "---\n",
    "\n",
    "# üéì How You Explain This in Class (Very Important)\n",
    "\n",
    "### For Non-Technical Students\n",
    "\n",
    "* ‚ÄúWords become numbers‚Äù\n",
    "* ‚ÄúNumbers talk to each other‚Äù\n",
    "* ‚ÄúImportant words get more focus‚Äù\n",
    "\n",
    "### For Technical Students\n",
    "\n",
    "* QKV attention\n",
    "* Softmax weighting\n",
    "* Residual connections\n",
    "* Layer normalization\n",
    "\n",
    "---\n",
    "\n",
    "# üß† Bridge to BERT & GPT (Final Teaching Hook)\n",
    "\n",
    "| Model | Built Using         |\n",
    "| ----- | ------------------- |\n",
    "| BERT  | Transformer Encoder |\n",
    "| GPT   | Transformer Decoder |\n",
    "| T5    | Encoder + Decoder   |\n",
    "\n",
    "> ‚ÄúIf you understand this code,\n",
    "> you understand **90% of modern AI models**.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Next (Highly Recommended)\n",
    "\n",
    "I can now:\n",
    "1Ô∏è‚É£ Convert this into **BERT from scratch**\n",
    "2Ô∏è‚É£ Convert into **GPT (text generation)**\n",
    "3Ô∏è‚É£ Create **Jupyter notebook for teaching**\n",
    "4Ô∏è‚É£ Add **training loop + loss**\n",
    "5Ô∏è‚É£ Create **student assignment version**\n",
    "\n",
    "Just tell me üëâ **what‚Äôs next** üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect üëç\n",
    "Below is a **FROM-SCRATCH BERT implementation (Encoder-only Transformer)** that you can **teach step-by-step**, with **clear explanations for both non-technical and technical students**.\n",
    "\n",
    "This is a **teaching version of BERT**, not the Hugging Face shortcut.\n",
    "\n",
    "---\n",
    "\n",
    "# üß† What We Are Building (Teacher Framing)\n",
    "\n",
    "![Image](https://towardsdatascience.com/wp-content/uploads/2024/05/1Qww2aaIdqrWVeNmo3AS0ZQ.png?utm_source=chatgpt.com)\n",
    "\n",
    "![Image](https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/MLM.png?utm_source=chatgpt.com)\n",
    "\n",
    "![Image](https://www.researchgate.net/publication/352642338/figure/fig1/AS%3A1037413736542211%401624350117816/BERT-Encoder-N-Transformer-Blocks.png?utm_source=chatgpt.com)\n",
    "\n",
    "![Image](https://www.researchgate.net/publication/349546860/figure/fig2/AS%3A994573320994818%401614136166736/The-Transformer-based-BERT-base-architecture-with-twelve-encoder-blocks.ppm?utm_source=chatgpt.com)\n",
    "\n",
    "### üéØ Goal\n",
    "\n",
    "We will build a **Mini-BERT** that:\n",
    "\n",
    "* Reads text **from both left and right**\n",
    "* Learns **deep meaning**\n",
    "* Solves **Masked Language Modeling (MLM)**\n",
    "\n",
    "> ‚ö†Ô∏è BERT **does NOT generate stories**\n",
    "> It **UNDERSTANDS text**\n",
    "\n",
    "---\n",
    "\n",
    "# üß© BERT High-Level Architecture\n",
    "\n",
    "### Non-Technical View\n",
    "\n",
    "* Input sentence\n",
    "* Hide some words\n",
    "* BERT guesses missing words\n",
    "* Learns language deeply\n",
    "\n",
    "### Technical View\n",
    "\n",
    "* Token Embedding\n",
    "* Positional Embedding\n",
    "* Segment Embedding\n",
    "* Transformer **Encoder Blocks**\n",
    "* MLM Head\n",
    "\n",
    "---\n",
    "\n",
    "# ü™ú STEP-BY-STEP BUILD (FROM SCRATCH)\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Step 1: Imports\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Step 2: Token + Position + Segment Embeddings\n",
    "\n",
    "### üß† Explain to Students\n",
    "\n",
    "* **Token embedding** ‚Üí word meaning\n",
    "* **Position embedding** ‚Üí word order\n",
    "* **Segment embedding** ‚Üí sentence A or B\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Code: BERT Embedding Layer\n",
    "\n",
    "```python\n",
    "class BERTEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, max_len=512):\n",
    "        super().__init__()\n",
    "        self.token = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position = nn.Embedding(max_len, embed_size)\n",
    "        self.segment = nn.Embedding(2, embed_size)\n",
    "\n",
    "    def forward(self, input_ids, segment_ids):\n",
    "        seq_len = input_ids.size(1)\n",
    "        positions = torch.arange(seq_len).unsqueeze(0)\n",
    "\n",
    "        token_emb = self.token(input_ids)\n",
    "        pos_emb = self.position(positions)\n",
    "        seg_emb = self.segment(segment_ids)\n",
    "\n",
    "        return token_emb + pos_emb + seg_emb\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Step 3: Self-Attention (BERT Core)\n",
    "\n",
    "> Same attention as Transformer, but **bidirectional**\n",
    "\n",
    "```python\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.q(x)\n",
    "        K = self.k(x)\n",
    "        V = self.v(x)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(x.size(-1))\n",
    "        attention = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        return torch.matmul(attention, V)\n",
    "```\n",
    "\n",
    "### üß† Teaching Line\n",
    "\n",
    "> ‚ÄúEvery word looks at **every other word** in both directions.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Step 4: Transformer Encoder Block (BERT Block)\n",
    "\n",
    "![Image](https://res.cloudinary.com/edlitera/image/upload/c_fill%2Cf_auto/v1680253949/blog/tggrmtbkds6pbnqzt782?utm_source=chatgpt.com)\n",
    "\n",
    "![Image](https://www.baeldung.com/wp-content/uploads/sites/4/2024/07/residuals300.drawio-1024x693.png?utm_source=chatgpt.com)\n",
    "\n",
    "```python\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.attention = SelfAttention(embed_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn = self.attention(x)\n",
    "        x = self.norm1(x + attn)\n",
    "\n",
    "        ffn = self.ffn(x)\n",
    "        x = self.norm2(x + ffn)\n",
    "\n",
    "        return x\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Step 5: Mini-BERT Model\n",
    "\n",
    "```python\n",
    "class MiniBERT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = BERTEmbedding(vocab_size, embed_dim)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderBlock(embed_dim) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, segment_ids):\n",
    "        x = self.embedding(input_ids, segment_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üß™ STEP 6: Masked Language Model (MLM Head)\n",
    "\n",
    "### üß† Non-Technical\n",
    "\n",
    "> ‚ÄúSome words are hidden ‚Üí BERT guesses them‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "class MLMHead(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üß™ STEP 7: FULL BERT + MLM\n",
    "\n",
    "```python\n",
    "class BERTForMaskedLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, layers=2):\n",
    "        super().__init__()\n",
    "        self.bert = MiniBERT(vocab_size, embed_dim, layers)\n",
    "        self.mlm = MLMHead(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, segment_ids):\n",
    "        x = self.bert(input_ids, segment_ids)\n",
    "        return self.mlm(x)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üß™ STEP 8: Example Run (Live Classroom Demo)\n",
    "\n",
    "```python\n",
    "# Vocabulary\n",
    "vocab = {\n",
    "    \"[PAD]\":0, \"[MASK]\":1,\n",
    "    \"i\":2, \"love\":3, \"ai\":4\n",
    "}\n",
    "\n",
    "sentence = [\"i\", \"love\", \"[MASK]\"]\n",
    "input_ids = torch.tensor([[vocab[word] for word in sentence]])\n",
    "segment_ids = torch.tensor([[0,0,0]])\n",
    "\n",
    "model = BERTForMaskedLM(vocab_size=len(vocab))\n",
    "output = model(input_ids, segment_ids)\n",
    "\n",
    "print(output.shape)\n",
    "```\n",
    "\n",
    "### Output\n",
    "\n",
    "```\n",
    "torch.Size([1, 3, 5])\n",
    "```\n",
    "\n",
    "‚úî Model predicts **masked word probabilities**\n",
    "\n",
    "---\n",
    "\n",
    "# üß† How You Explain This in Class\n",
    "\n",
    "### For Non-Technical Students\n",
    "\n",
    "* ‚ÄúWords become vectors‚Äù\n",
    "* ‚ÄúHidden word is guessed‚Äù\n",
    "* ‚ÄúModel learns meaning‚Äù\n",
    "\n",
    "### For Technical Students\n",
    "\n",
    "* Encoder-only Transformer\n",
    "* Bidirectional self-attention\n",
    "* MLM loss (CrossEntropy)\n",
    "* Pretraining + Fine-tuning\n",
    "\n",
    "---\n",
    "\n",
    "# üìä BERT vs GPT (Quick Recap for Students)\n",
    "\n",
    "| Feature   | BERT            | GPT           |\n",
    "| --------- | --------------- | ------------- |\n",
    "| Reads     | Both directions | Left ‚Üí Right  |\n",
    "| Main job  | Understand      | Generate      |\n",
    "| Training  | Masked words    | Next word     |\n",
    "| Use cases | Search, QA      | Chat, writing |\n",
    "\n",
    "---\n",
    "\n",
    "# üéì What Students Should Remember (EXAM GOLD)\n",
    "\n",
    "* BERT = **Encoder-only Transformer**\n",
    "* Bidirectional context\n",
    "* Trained using **Masked Language Model**\n",
    "* Used for **classification, QA, NER**\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ NEXT OPTIONS (Highly Recommended)\n",
    "\n",
    "I can now:\n",
    "1Ô∏è‚É£ Add **training loop + loss function**\n",
    "2Ô∏è‚É£ Convert this to **Sentence Classification (Spam/Review)**\n",
    "3Ô∏è‚É£ Compare **BERT vs GPT code-wise**\n",
    "4Ô∏è‚É£ Create **Jupyter Notebook for students**\n",
    "5Ô∏è‚É£ Build **Resume Classifier / QA System using this BERT**\n",
    "\n",
    "üëâ Just tell me **what to do next**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect üëç\n",
    "Below is a **FROM-SCRATCH GPT (Decoder-only Transformer)** implementation that you can **teach live**, **run end-to-end**, and **extend later** (training, prompts, chat).\n",
    "\n",
    "This is a **teaching GPT** (mini-GPT), not a Hugging Face shortcut.\n",
    "\n",
    "---\n",
    "\n",
    "# üß† What We Are Building (Teacher Framing)\n",
    "\n",
    "![Image](https://www.lavivienpost.com/wp-content/uploads/2023/04/decoder-only-architecture-768.jpg?utm_source=chatgpt.com)\n",
    "\n",
    "![Image](https://files.mastodon.social/media_attachments/files/111/820/570/310/327/483/original/9c619019f8f9a286.webp?utm_source=chatgpt.com)\n",
    "\n",
    "![Image](https://www.researchgate.net/publication/329121939/figure/fig1/AS%3A695748987469826%401542890893045/A-language-model-based-on-an-autoregressive-HMM-that-emits-sequentially-dependent-binary.png?utm_source=chatgpt.com)\n",
    "\n",
    "![Image](https://www.georgeho.org/assets/images/rnn-unrolled.png?utm_source=chatgpt.com)\n",
    "\n",
    "### üéØ Goal\n",
    "\n",
    "We will build a **Mini-GPT** that:\n",
    "\n",
    "* Reads text **left ‚Üí right**\n",
    "* Uses **causal (masked) self-attention**\n",
    "* Predicts the **next word**\n",
    "* Generates text **autoregressively**\n",
    "\n",
    "> ‚ö†Ô∏è GPT = **Writer**\n",
    "> It does **NOT** read both sides like BERT.\n",
    "\n",
    "---\n",
    "\n",
    "# üß© GPT High-Level Architecture\n",
    "\n",
    "### Non-Technical View\n",
    "\n",
    "* Read previous words\n",
    "* Hide future words\n",
    "* Predict next word\n",
    "* Repeat again and again\n",
    "\n",
    "### Technical View\n",
    "\n",
    "* Token Embedding\n",
    "* Positional Embedding\n",
    "* **Masked Self-Attention**\n",
    "* Feed-Forward Network\n",
    "* Linear + Softmax\n",
    "* Autoregressive generation loop\n",
    "\n",
    "---\n",
    "\n",
    "# ü™ú STEP-BY-STEP GPT (FROM SCRATCH)\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Step 1: Imports\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Step 2: Token + Positional Embeddings\n",
    "\n",
    "```python\n",
    "class GPTEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, max_len=100):\n",
    "        super().__init__()\n",
    "        self.token = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position = nn.Embedding(max_len, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        positions = torch.arange(seq_len).unsqueeze(0)\n",
    "        return self.token(x) + self.position(positions)\n",
    "```\n",
    "\n",
    "### üß† Teaching Line\n",
    "\n",
    "> ‚ÄúWords get meaning + position ‚Üí now the model knows order.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Step 3: Causal (Masked) Self-Attention\n",
    "\n",
    "### üß† Non-Technical\n",
    "\n",
    "> ‚ÄúGPT is **not allowed to see the future**.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.q(x)\n",
    "        K = self.k(x)\n",
    "        V = self.v(x)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(x.size(-1))\n",
    "\n",
    "        # üîí Causal mask (lower triangle)\n",
    "        mask = torch.tril(torch.ones(scores.size()))\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attention = torch.softmax(scores, dim=-1)\n",
    "        return torch.matmul(attention, V)\n",
    "```\n",
    "\n",
    "### üß† Explain This Clearly\n",
    "\n",
    "| Concept        | Meaning                 |\n",
    "| -------------- | ----------------------- |\n",
    "| Mask           | Hide future words       |\n",
    "| tril           | Lower-triangle matrix   |\n",
    "| autoregressive | Predict next token only |\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Step 4: GPT Decoder Block\n",
    "\n",
    "![Image](https://res.cloudinary.com/edlitera/image/upload/c_fill%2Cf_auto/v1680629118/blog/gz5ccspg3yvq4eo6xhrr?utm_source=chatgpt.com)\n",
    "\n",
    "![Image](https://i.sstatic.net/eAKQu.png?utm_source=chatgpt.com)\n",
    "\n",
    "```python\n",
    "class GPTBlock(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.attn = CausalSelfAttention(embed_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x + self.attn(x))\n",
    "        x = self.norm2(x + self.ffn(x))\n",
    "        return x\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Step 5: Mini-GPT Model\n",
    "\n",
    "```python\n",
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = GPTEmbedding(vocab_size, embed_dim)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [GPTBlock(embed_dim) for _ in range(layers)]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return self.fc_out(x)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üß™ STEP 6: Example Forward Pass (Class Demo)\n",
    "\n",
    "```python\n",
    "# Vocabulary\n",
    "vocab = {\n",
    "    \"i\":0, \"love\":1, \"ai\":2, \"and\":3, \"data\":4\n",
    "}\n",
    "\n",
    "sentence = [\"i\", \"love\", \"ai\"]\n",
    "input_ids = torch.tensor([[vocab[w] for w in sentence]])\n",
    "\n",
    "model = MiniGPT(vocab_size=len(vocab))\n",
    "logits = model(input_ids)\n",
    "\n",
    "print(logits.shape)\n",
    "```\n",
    "\n",
    "### Output\n",
    "\n",
    "```\n",
    "torch.Size([1, 3, 5])\n",
    "```\n",
    "\n",
    "‚úî Each word predicts **next-word probabilities**\n",
    "\n",
    "---\n",
    "\n",
    "# üß† STEP 7: Text Generation (AUTOREGRESSIVE)\n",
    "\n",
    "```python\n",
    "def generate(model, input_ids, max_new_tokens):\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(input_ids)\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "    return input_ids\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Demo Generation\n",
    "\n",
    "```python\n",
    "start = torch.tensor([[vocab[\"i\"]]])\n",
    "output = generate(model, start, max_new_tokens=5)\n",
    "print(output)\n",
    "```\n",
    "\n",
    "> (Random output now ‚Äî makes sense only after training)\n",
    "\n",
    "---\n",
    "\n",
    "# üß† How You Explain GPT in Class\n",
    "\n",
    "### For Non-Technical Students\n",
    "\n",
    "* ‚ÄúGPT writes word by word‚Äù\n",
    "* ‚ÄúIt never peeks ahead‚Äù\n",
    "* ‚ÄúIt learns patterns from text‚Äù\n",
    "\n",
    "### For Technical Students\n",
    "\n",
    "* Decoder-only Transformer\n",
    "* Causal self-attention\n",
    "* Autoregressive language modeling\n",
    "* Cross-entropy loss (next token)\n",
    "\n",
    "---\n",
    "\n",
    "# üìä GPT vs BERT (Code-Level)\n",
    "\n",
    "| Feature      | BERT          | GPT        |\n",
    "| ------------ | ------------- | ---------- |\n",
    "| Architecture | Encoder       | Decoder    |\n",
    "| Attention    | Bidirectional | Causal     |\n",
    "| Training     | Masked LM     | Next Token |\n",
    "| Output       | Understanding | Generation |\n",
    "\n",
    "---\n",
    "\n",
    "# üéì EXAM + INTERVIEW GOLD\n",
    "\n",
    "* GPT = **Decoder-only Transformer**\n",
    "* Uses **causal mask**\n",
    "* Trained using **next-token prediction**\n",
    "* Generates text **autoregressively**\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ NEXT (Highly Recommended)\n",
    "\n",
    "I can now:\n",
    "1Ô∏è‚É£ Add **training loop + loss**\n",
    "2Ô∏è‚É£ Train GPT on **tiny text dataset**\n",
    "3Ô∏è‚É£ Convert to **chat-style GPT**\n",
    "4Ô∏è‚É£ Compare **GPT vs LLaMA code**\n",
    "5Ô∏è‚É£ Create **Jupyter notebook for teaching**\n",
    "\n",
    "üëâ Just tell me **what‚Äôs next**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hugging face pre-train models "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMq6srv0u2PckjjCVeB2cK3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
