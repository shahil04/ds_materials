{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMAP7BHk8knnbusSbWQh10h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["To proceed, I‚Äôll now give you a full working implementation step-by-step for a basic Seq2Seq Encoder-Decoder model with LSTMs in PyTorch using a toy English-to-Hindi dataset.\n","\n","‚úÖ Step-by-Step LSTM Encoder-Decoder Seq2Seq in PyTorch (Working Code + Use Case)\n","üìå 1. Install Required Packages\n"],"metadata":{"id":"gzWLWMQEPGJN"}},{"cell_type":"markdown","source":["https://www.kaggle.com/datasets/preetviradiya/english-hindi-dataset"],"metadata":{"id":"zH7YeSA0cDCZ"}},{"cell_type":"code","source":["# python version\n","# library version\n"],"metadata":{"id":"AUBaXl8sxMP5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pip install torch torchtext\n","# Uninstall existing torch and torchtext\n","!pip uninstall torch torchtext -y\n","\n","# Install torch and torchtext with compatible versions\n","# You might need to adjust the version and the cuXXXX part based on your system and desired CUDA version\n","# For example, for CUDA 11.8, use cu118\n","# For CPU only, use torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n","!pip install torch==2.2.0+cu118 torchtext"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MoShynqZPQlz","executionInfo":{"status":"ok","timestamp":1762361464108,"user_tz":-330,"elapsed":7487,"user":{"displayName":"Sahil","userId":"02943187861823395646"}},"outputId":"b4b00564-81ed-42ed-c275-fa2789f07bac"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Skipping torchtext as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torch==2.2.0+cu118 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0, 2.7.1, 2.8.0, 2.9.0)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for torch==2.2.0+cu118\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"id":"40145d55","executionInfo":{"status":"error","timestamp":1762361482592,"user_tz":-330,"elapsed":50,"user":{"displayName":"Sahil","userId":"02943187861823395646"}},"outputId":"6dfea647-875c-475b-e597-535eef974b65"},"source":["# After reinstalling, rerun the rest of your code starting from the imports.\n","import torch\n","from torch.nn.utils.rnn import pad_sequence\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","\n","data = [\n","    (\"hello\", \"‡§®‡§Æ‡§∏‡•ç‡§§‡•á\"),\n","    (\"how are you\", \"‡§Ü‡§™ ‡§ï‡•à‡§∏‡•á ‡§π‡•à‡§Ç\"),\n","    (\"i am fine\", \"‡§Æ‡•à‡§Ç ‡§†‡•Ä‡§ï ‡§π‡•Ç‡§Å\"),\n","    (\"thank you\", \"‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶\"),\n","    (\"what is your name\", \"‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à\"),\n","    (\"my name is john\", \"‡§Æ‡•á‡§∞‡§æ ‡§®‡§æ‡§Æ ‡§ú‡•â‡§® ‡§π‡•à\"),\n","]\n","\n","# Rest of your code..."],"execution_count":9,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'torch'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1190685564.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# After reinstalling, rerun the rest of your code starting from the imports.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_vocab_from_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","metadata":{"id":"a1d64468","executionInfo":{"status":"ok","timestamp":1762361516651,"user_tz":-330,"elapsed":54,"user":{"displayName":"Sahil","userId":"02943187861823395646"}}},"source":["# !pip list"],"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# After reinstalling, rerun the rest of your code starting from the imports.\n","import torch\n","from torch.nn.utils.rnn import pad_sequence\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","\n","data = [\n","    (\"hello\", \"‡§®‡§Æ‡§∏‡•ç‡§§‡•á\"),\n","    (\"how are you\", \"‡§Ü‡§™ ‡§ï‡•à‡§∏‡•á ‡§π‡•à‡§Ç\"),\n","    (\"i am fine\", \"‡§Æ‡•à‡§Ç ‡§†‡•Ä‡§ï ‡§π‡•Ç‡§Å\"),\n","    (\"thank you\", \"‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶\"),\n","    (\"what is your name\", \"‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à\"),\n","    (\"my name is john\", \"‡§Æ‡•á‡§∞‡§æ ‡§®‡§æ‡§Æ ‡§ú‡•â‡§® ‡§π‡•à\"),\n","]\n","\n","# Rest of your code..."],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"id":"RAJAr9BnQAB2","executionInfo":{"status":"error","timestamp":1762361519862,"user_tz":-330,"elapsed":26,"user":{"displayName":"Sahil","userId":"02943187861823395646"}},"outputId":"defef1d1-e723-4f65-b6f1-ef22f897ca0d"},"execution_count":12,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'torch'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1190685564.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# After reinstalling, rerun the rest of your code starting from the imports.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_vocab_from_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"id":"b3de5597","executionInfo":{"status":"error","timestamp":1762361524641,"user_tz":-330,"elapsed":62,"user":{"displayName":"Sahil","userId":"02943187861823395646"}},"outputId":"c1201428-e0a5-4eb1-cce3-0723a29aa885"},"source":["# After reinstalling, rerun the rest of your code starting from the imports.\n","import torch\n","from torch.nn.utils.rnn import pad_sequence\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","\n","data = [\n","    (\"hello\", \"‡§®‡§Æ‡§∏‡•ç‡§§‡•á\"),\n","    (\"how are you\", \"‡§Ü‡§™ ‡§ï‡•à‡§∏‡•á ‡§π‡•à‡§Ç\"),\n","    (\"i am fine\", \"‡§Æ‡•à‡§Ç ‡§†‡•Ä‡§ï ‡§π‡•Ç‡§Å\"),\n","    (\"thank you\", \"‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶\"),\n","    (\"what is your name\", \"‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à\"),\n","    (\"my name is john\", \"‡§Æ‡•á‡§∞‡§æ ‡§®‡§æ‡§Æ ‡§ú‡•â‡§® ‡§π‡•à\"),\n","]\n","\n","# Rest of your code..."],"execution_count":13,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'torch'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1190685564.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# After reinstalling, rerun the rest of your code starting from the imports.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_vocab_from_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"id":"2b95adc2","executionInfo":{"status":"error","timestamp":1762361416626,"user_tz":-330,"elapsed":33,"user":{"displayName":"Sahil","userId":"02943187861823395646"}},"outputId":"78d72102-1b06-476e-96aa-1cfb0c71c0d5"},"source":["# After reinstalling, rerun the rest of your code starting from the imports.\n","import torch\n","from torch.nn.utils.rnn import pad_sequence\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","\n","data = [\n","    (\"hello\", \"‡§®‡§Æ‡§∏‡•ç‡§§‡•á\"),\n","    (\"how are you\", \"‡§Ü‡§™ ‡§ï‡•à‡§∏‡•á ‡§π‡•à‡§Ç\"),\n","    (\"i am fine\", \"‡§Æ‡•à‡§Ç ‡§†‡•Ä‡§ï ‡§π‡•Ç‡§Å\"),\n","    (\"thank you\", \"‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶\"),\n","    (\"what is your name\", \"‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à\"),\n","    (\"my name is john\", \"‡§Æ‡•á‡§∞‡§æ ‡§®‡§æ‡§Æ ‡§ú‡•â‡§® ‡§π‡•à\"),\n","]\n","\n","# Rest of your code..."],"execution_count":5,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'torch'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1190685564.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# After reinstalling, rerun the rest of your code starting from the imports.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_vocab_from_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["# üìå 2. Prepare Dataset\n","\n","import torch\n","from torch.nn.utils.rnn import pad_sequence\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","\n","data = [\n","    (\"hello\", \"‡§®‡§Æ‡§∏‡•ç‡§§‡•á\"),\n","    (\"how are you\", \"‡§Ü‡§™ ‡§ï‡•à‡§∏‡•á ‡§π‡•à‡§Ç\"),\n","    (\"i am fine\", \"‡§Æ‡•à‡§Ç ‡§†‡•Ä‡§ï ‡§π‡•Ç‡§Å\"),\n","    (\"thank you\", \"‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶\"),\n","    (\"what is your name\", \"‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à\"),\n","    (\"my name is john\", \"‡§Æ‡•á‡§∞‡§æ ‡§®‡§æ‡§Æ ‡§ú‡•â‡§® ‡§π‡•à\"),\n","]\n"],"metadata":{"id":"Am8hXzNLPUPk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenizers\n","en_tokenizer = get_tokenizer(\"basic_english\")\n","hi_tokenizer = get_tokenizer(\"basic_english\")\n","\n","def yield_tokens(data, tokenizer, idx):\n","    for pair in data:\n","        yield tokenizer(pair[idx])\n","\n","SRC_VOCAB = build_vocab_from_iterator(yield_tokens(data, en_tokenizer, 0), specials=[\"<pad>\", \"<sos>\", \"<eos>\"])\n","TGT_VOCAB = build_vocab_from_iterator(yield_tokens(data, hi_tokenizer, 1), specials=[\"<pad>\", \"<sos>\", \"<eos>\"])\n","\n","SRC_VOCAB.set_default_index(SRC_VOCAB[\"<pad>\"])\n","TGT_VOCAB.set_default_index(TGT_VOCAB[\"<pad>\"])\n","\n","def tensorize(pair):\n","    src = [SRC_VOCAB[\"<sos>\"]] + [SRC_VOCAB[tok] for tok in en_tokenizer(pair[0])] + [SRC_VOCAB[\"<eos>\"]]\n","    tgt = [TGT_VOCAB[\"<sos>\"]] + [TGT_VOCAB[tok] for tok in hi_tokenizer(pair[1])] + [TGT_VOCAB[\"<eos>\"]]\n","    return torch.tensor(src), torch.tensor(tgt)"],"metadata":{"id":"7A7iaMZ-PZ0l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# üìå 3. Create Encoder and Decoder\n","\n","import torch.nn as nn\n","\n","class Encoder(nn.Module):\n","    def __init__(self, input_dim, emb_dim, hidden_dim):\n","        super().__init__()\n","        self.embedding = nn.Embedding(input_dim, emb_dim)\n","        self.lstm = nn.LSTM(emb_dim, hidden_dim)\n","\n","    def forward(self, src):\n","        embedded = self.embedding(src)\n","        outputs, (hidden, cell) = self.lstm(embedded)\n","        return hidden, cell\n","\n","class Decoder(nn.Module):\n","    def __init__(self, output_dim, emb_dim, hidden_dim):\n","        super().__init__()\n","        self.embedding = nn.Embedding(output_dim, emb_dim)\n","        self.lstm = nn.LSTM(emb_dim, hidden_dim)\n","        self.fc_out = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, input, hidden, cell):\n","        input = input.unsqueeze(0)\n","        embedded = self.embedding(input)\n","        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n","        prediction = self.fc_out(output.squeeze(0))\n","        return prediction, hidden, cell"],"metadata":{"id":"i5nTNp0VPeo-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# üìå 4. Seq2Seq Training Loop\n","\n","import torch.optim as optim\n","\n","INPUT_DIM = len(SRC_VOCAB)\n","OUTPUT_DIM = len(TGT_VOCAB)\n","HID_DIM = 256\n","EMB_DIM = 128\n","\n","enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM)\n","dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","enc, dec = enc.to(device), dec.to(device)\n","\n","optimizer = optim.Adam(list(enc.parameters()) + list(dec.parameters()))\n","criterion = nn.CrossEntropyLoss(ignore_index=TGT_VOCAB[\"<pad>\"])\n"],"metadata":{"id":"cSODI-s2PlLW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training loop\n","for epoch in range(100):\n","    epoch_loss = 0\n","    for src, tgt in map(tensorize, data):\n","        src, tgt = src.to(device), tgt.to(device)\n","        optimizer.zero_grad()\n","        hidden, cell = enc(src.unsqueeze(1))  # [src_len, batch_size]\n","        input = tgt[0]\n","        loss = 0\n","        for t in range(1, len(tgt)):\n","            output, hidden, cell = dec(input, hidden, cell)\n","            loss += criterion(output, tgt[t].unsqueeze(0))\n","            input = tgt[t]\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","    if epoch % 10 == 0:\n","        print(f\"Epoch {epoch} Loss: {epoch_loss:.2f}\")"],"metadata":{"id":"M8HFT8lBPqh-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# üìå 5. Translate New Sentences (Inference)\n","\n","def translate_sentence(sentence):\n","    enc.eval()\n","    dec.eval()\n","    tokens = [SRC_VOCAB[\"<sos>\"]] + [SRC_VOCAB[token] for token in en_tokenizer(sentence)] + [SRC_VOCAB[\"<eos>\"]]\n","    src_tensor = torch.tensor(tokens).to(device).unsqueeze(1)\n","    hidden, cell = enc(src_tensor)\n","    input = torch.tensor([TGT_VOCAB[\"<sos>\"]]).to(device)\n","\n","    result = []\n","    for _ in range(20):\n","        output, hidden, cell = dec(input, hidden, cell)\n","        top1 = output.argmax(1)\n","        if top1.item() == TGT_VOCAB[\"<eos>\"]:\n","            break\n","        result.append(top1.item())\n","        input = top1\n","\n","    translated = [TGT_VOCAB.get_itos()[idx] for idx in result]\n","    return \" \".join(translated)\n"],"metadata":{"id":"YjsVFByyPty3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fIRfTe6hOkwS"},"outputs":[],"source":["# Example\n","print(translate_sentence(\"hello\"))\n","print(translate_sentence(\"what is your name\"))\n","# ‚úÖ Final Output (Sample)\n","\n","# > hello\n","# ‡§®‡§Æ‡§∏‡•ç‡§§‡•á\n","\n","# > what is your name\n","# ‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à\n"]},{"cell_type":"code","source":[],"metadata":{"id":"8X857CuvPEqy"},"execution_count":null,"outputs":[]}]}