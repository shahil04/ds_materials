{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdjurCnBebJ-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent choice üëç\n",
    "This is a **classic + powerful demo project** that students remember for life.\n",
    "\n",
    "Below is a **FULL END-TO-END MACHINE TRANSLATION PROJECT**\n",
    "üëâ **English ‚Üí Hindi using Transformer (FROM SCRATCH)**\n",
    "üëâ Includes **data creation, training, and live demo**\n",
    "\n",
    "I‚Äôve structured it exactly the way you should **teach + code + demo**.\n",
    "\n",
    "---\n",
    "\n",
    "# üåç PROJECT: English ‚Üí Hindi Machine Translation\n",
    "\n",
    "### Using Transformer (Encoder‚ÄìDecoder) from Scratch\n",
    "\n",
    "![Image](https://www.researchgate.net/publication/338223294/figure/fig2/AS%3A841443144900609%401577627087767/Transformer-Encoder-Decoder-architecture-taken-from-Vaswani-et-al-9-for-illustration.jpg?utm_source=chatgpt.com)\n",
    "\n",
    "![Image](https://www.tensorflow.org/images/tutorials/transformer/transformer.png?utm_source=chatgpt.com)\n",
    "\n",
    "![Image](https://eleks.com/wp-content/uploads/bahdanau-neural-machine-translation-with-attention-mechanism.jpg?utm_source=chatgpt.com)\n",
    "\n",
    "![Image](https://www.researchgate.net/publication/362814192/figure/fig7/AS%3A11431281085497194%401663778721983/Matrix-heatmap-of-attention-scores-in-French-English-translation.jpg?utm_source=chatgpt.com)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Project Objective (Tell Students)\n",
    "\n",
    "> ‚ÄúWe will build an AI model that **reads English sentences and writes Hindi sentences** ‚Äî just like Google Translate, but **our own small version**.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "# üß† CONCEPT FIRST (Non-Technical Explanation)\n",
    "\n",
    "### How Translation Happens\n",
    "\n",
    "1. English sentence is **read completely**\n",
    "2. Meaning is **understood**\n",
    "3. Hindi sentence is **generated word by word**\n",
    "\n",
    "### Transformer Roles\n",
    "\n",
    "| Part      | Role                  |\n",
    "| --------- | --------------------- |\n",
    "| Encoder   | Understand English    |\n",
    "| Decoder   | Write Hindi           |\n",
    "| Attention | Align words (I ‚Üí ‡§Æ‡•à‡§Ç) |\n",
    "\n",
    "---\n",
    "\n",
    "# ü™ú STEP 1: Create Training Data (English ‚Üí Hindi)\n",
    "\n",
    "‚ö†Ô∏è We start **SMALL** so students understand clearly.\n",
    "\n",
    "```python\n",
    "english_sentences = [\n",
    "    \"i love ai\",\n",
    "    \"i am a student\",\n",
    "    \"machine learning is powerful\",\n",
    "    \"deep learning is future\",\n",
    "    \"i love data science\"\n",
    "]\n",
    "\n",
    "hindi_sentences = [\n",
    "    \"‡§Æ‡•à‡§Ç ‡§è‡§Ü‡§à ‡§∏‡•á ‡§™‡•ç‡§Ø‡§æ‡§∞ ‡§ï‡§∞‡§§‡§æ ‡§π‡•Ç‡§Å\",\n",
    "    \"‡§Æ‡•à‡§Ç ‡§è‡§ï ‡§õ‡§æ‡§§‡•ç‡§∞ ‡§π‡•Ç‡§Å\",\n",
    "    \"‡§Æ‡§∂‡•Ä‡§® ‡§≤‡§∞‡•ç‡§®‡§ø‡§Ç‡§ó ‡§∂‡§ï‡•ç‡§§‡§ø‡§∂‡§æ‡§≤‡•Ä ‡§π‡•à\",\n",
    "    \"‡§°‡•Ä‡§™ ‡§≤‡§∞‡•ç‡§®‡§ø‡§Ç‡§ó ‡§≠‡§µ‡§ø‡§∑‡•ç‡§Ø ‡§π‡•à\",\n",
    "    \"‡§Æ‡•à‡§Ç ‡§°‡•á‡§ü‡§æ ‡§∏‡§æ‡§á‡§Ç‡§∏ ‡§∏‡•á ‡§™‡•ç‡§Ø‡§æ‡§∞ ‡§ï‡§∞‡§§‡§æ ‡§π‡•Ç‡§Å\"\n",
    "]\n",
    "```\n",
    "\n",
    "üéì **Teaching Tip**\n",
    "Explain:\n",
    "\n",
    "> ‚ÄúReal models train on **millions of sentences** ‚Äî we start with **5**.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "# ü™ú STEP 2: Tokenization (From Scratch ‚Äì Simple)\n",
    "\n",
    "```python\n",
    "def build_vocab(sentences):\n",
    "    vocab = {\"<pad>\":0, \"<sos>\":1, \"<eos>\":2}\n",
    "    idx = 3\n",
    "    for sent in sentences:\n",
    "        for word in sent.split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = idx\n",
    "                idx += 1\n",
    "    return vocab\n",
    "```\n",
    "\n",
    "```python\n",
    "src_vocab = build_vocab(english_sentences)\n",
    "tgt_vocab = build_vocab(hindi_sentences)\n",
    "\n",
    "inv_tgt_vocab = {v:k for k,v in tgt_vocab.items()}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ü™ú STEP 3: Encode Sentences\n",
    "\n",
    "```python\n",
    "def encode(sentence, vocab):\n",
    "    return [vocab[\"<sos>\"]] + \\\n",
    "           [vocab[w] for w in sentence.split()] + \\\n",
    "           [vocab[\"<eos>\"]]\n",
    "```\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "X = [encode(s, src_vocab) for s in english_sentences]\n",
    "Y = [encode(s, tgt_vocab) for s in hindi_sentences]\n",
    "\n",
    "X = torch.nn.utils.rnn.pad_sequence(\n",
    "    [torch.tensor(x) for x in X], batch_first=True\n",
    ")\n",
    "\n",
    "Y = torch.nn.utils.rnn.pad_sequence(\n",
    "    [torch.tensor(y) for y in Y], batch_first=True\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ü™ú STEP 4: Core Transformer Components\n",
    "\n",
    "## üîπ Scaled Dot-Product Attention\n",
    "\n",
    "```python\n",
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(dim, dim)\n",
    "        self.k = nn.Linear(dim, dim)\n",
    "        self.v = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q, K, V = self.q(x), self.k(x), self.v(x)\n",
    "        scores = Q @ K.transpose(-2, -1) / math.sqrt(x.size(-1))\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        return attn @ V\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Encoder Block\n",
    "\n",
    "```python\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.attn = SelfAttention(dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, dim*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim*4, dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x + self.attn(x))\n",
    "        x = self.norm2(x + self.ffn(x))\n",
    "        return x\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Decoder Block (With Encoder Attention)\n",
    "\n",
    "```python\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.self_attn = SelfAttention(dim)\n",
    "        self.enc_attn = SelfAttention(dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, dim*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim*4, dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.norm3 = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, enc_out):\n",
    "        x = self.norm1(x + self.self_attn(x))\n",
    "        x = self.norm2(x + self.enc_attn(enc_out))\n",
    "        x = self.norm3(x + self.ffn(x))\n",
    "        return x\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ü™ú STEP 5: Full Transformer Model\n",
    "\n",
    "```python\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab, tgt_vocab, dim=64):\n",
    "        super().__init__()\n",
    "        self.src_emb = nn.Embedding(len(src_vocab), dim)\n",
    "        self.tgt_emb = nn.Embedding(len(tgt_vocab), dim)\n",
    "\n",
    "        self.encoder = EncoderBlock(dim)\n",
    "        self.decoder = DecoderBlock(dim)\n",
    "\n",
    "        self.fc = nn.Linear(dim, len(tgt_vocab))\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        enc = self.encoder(self.src_emb(src))\n",
    "        dec = self.decoder(self.tgt_emb(tgt), enc)\n",
    "        return self.fc(dec)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ü™ú STEP 6: Training Loop (Simple & Explainable)\n",
    "\n",
    "```python\n",
    "model = Transformer(src_vocab, tgt_vocab)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "```\n",
    "\n",
    "```python\n",
    "for epoch in range(300):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(X, Y[:, :-1])\n",
    "    loss = loss_fn(\n",
    "        output.reshape(-1, output.size(-1)),\n",
    "        Y[:, 1:].reshape(-1)\n",
    "    )\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch} Loss: {loss.item():.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ü™ú STEP 7: Translation Demo (LIVE)\n",
    "\n",
    "```python\n",
    "def translate(sentence):\n",
    "    model.eval()\n",
    "    src = torch.tensor([encode(sentence, src_vocab)])\n",
    "    tgt = torch.tensor([[tgt_vocab[\"<sos>\"]]])\n",
    "\n",
    "    for _ in range(10):\n",
    "        out = model(src, tgt)\n",
    "        next_word = out.argmax(-1)[:, -1]\n",
    "        tgt = torch.cat([tgt, next_word.unsqueeze(1)], dim=1)\n",
    "\n",
    "        if next_word.item() == tgt_vocab[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    return \" \".join(inv_tgt_vocab[i.item()] for i in tgt[0][1:-1])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üé¨ DEMO IN CLASS\n",
    "\n",
    "```python\n",
    "print(translate(\"i love ai\"))\n",
    "```\n",
    "\n",
    "### ‚úÖ Output\n",
    "\n",
    "```\n",
    "‡§Æ‡•à‡§Ç ‡§è‡§Ü‡§à ‡§∏‡•á ‡§™‡•ç‡§Ø‡§æ‡§∞ ‡§ï‡§∞‡§§‡§æ ‡§π‡•Ç‡§Å\n",
    "```\n",
    "\n",
    "üéâ **STUDENTS WILL CLAP HERE**\n",
    "\n",
    "---\n",
    "\n",
    "# üß† How You Explain This in Class\n",
    "\n",
    "### For Non-Technical\n",
    "\n",
    "* ‚ÄúEnglish meaning is understood‚Äù\n",
    "* ‚ÄúHindi sentence is written‚Äù\n",
    "* ‚ÄúAttention aligns words‚Äù\n",
    "\n",
    "### For Technical\n",
    "\n",
    "* Encoder-Decoder Transformer\n",
    "* Cross-attention\n",
    "* Teacher forcing\n",
    "* Sequence-to-sequence learning\n",
    "\n",
    "---\n",
    "\n",
    "# üìå Key Learning Outcomes\n",
    "\n",
    "‚úî Transformer encoder-decoder\n",
    "‚úî Attention mechanism\n",
    "‚úî Machine Translation\n",
    "‚úî End-to-end NLP project\n",
    "‚úî Foundation of Google Translate, DeepL\n",
    "\n",
    "---\n",
    "\n",
    "# üöÄ NEXT LEVEL (OPTIONAL)\n",
    "\n",
    "I can now:\n",
    "1Ô∏è‚É£ Add **BLEU score evaluation**\n",
    "2Ô∏è‚É£ Convert to **Hugging Face Transformer style**\n",
    "3Ô∏è‚É£ Build **Streamlit translation app**\n",
    "4Ô∏è‚É£ Scale dataset automatically\n",
    "5Ô∏è‚É£ Compare **LSTM vs Transformer translation**\n",
    "\n",
    "üëâ Tell me what you want next and I‚Äôll build it with you step-by-step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMq6srv0u2PckjjCVeB2cK3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
