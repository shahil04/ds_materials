{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "CZr_k-KaCK7r",
      "metadata": {
        "id": "CZr_k-KaCK7r"
      },
      "source": [
        "\n",
        "# 5. **History of Hugging Face**\n",
        "\n",
        "### Timeline:\n",
        "\n",
        "* **2016**: Launched as a chatbot company\n",
        "* **2018**: Released Transformers library â†’ huge adoption\n",
        "* **2020**: Released Datasets & Tokenizers\n",
        "* **2021**: Introduced Spaces\n",
        "* **2022**: Diffusers for image generation\n",
        "* **2023â€“2025**: Expanded into open LLMs, inference solutions, serverless, and multimodal AI\n",
        "\n",
        "Hugging Face = **OpenAI + GitHub + Model Zoo + AI deployment platform combined.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59fff49e",
      "metadata": {
        "id": "59fff49e"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\"Hugging Face makes AI simple!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S9imCsoZKjfY",
      "metadata": {
        "id": "S9imCsoZKjfY"
      },
      "outputs": [],
      "source": [
        "# **Batch Inference (Technical)**\n",
        "\n",
        "texts = [\"I like AI\", \"I hate bugs\"]\n",
        "\n",
        "pipeline(\"sentiment-analysis\")(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faabc706",
      "metadata": {
        "id": "faabc706"
      },
      "source": [
        "<!--\n",
        "\n",
        "Here are detailed notes with code examples for various tasks using Hugging Face, drawing upon the provided YouTube source.\n",
        "\n",
        "***\n",
        "\n",
        "### 1. Introduction to Hugging Face and its Core Libraries\n",
        "\n",
        "Hugging Face is a company and open-source community focused on Natural Language Processing (NLP) and Artificial Intelligence (AI). It is best known for its **Transformers library**, which offers tools and pre-trained models for a wide range of NLP tasks. Beyond Transformers, Hugging Face also provides other widely used libraries, including **Datasets** and **Tokenizers**. It also features a **Model Hub** for sharing and downloading pre-trained models, datasets, and other resources, and **Spaces** for hosting and sharing machine learning demos and applications.\n",
        "\n",
        "**Key Libraries:**\n",
        "*   **Transformers Library**: The core library for pre-trained models and pipelines, providing access to thousands of pre-trained models for NLP tasks like translation, text summarization, and text classification. It is simple to use with complex NLP models, offers cutting-edge models, supports customization, and has a large, active community.\n",
        "*   **Datasets Library**: Provides easy access to a wide variety of datasets for NLP and other machine learning tasks. It enables efficient work with large datasets through lazy loading and streaming, offers a unified API for processing datasets, and integrates well with the Transformers library and other ML frameworks. Hugging Face provides over 350,000 datasets on its platform.\n",
        "*   **Tokenizers Library**: A fast, efficient, and flexible library designed for tokenizing text data, a crucial step in NLP. Tokenization involves splitting text into smaller units (words, subwords, characters) and converting them into numerical representations for ML models. It is optimised for fast tokenization, supports custom tokenizers, and integrates with other Hugging Face libraries like Transformers.\n",
        "\n",
        "### 2. Hugging Face Access Token\n",
        "\n",
        "An access token (also referred to as an API key) is a secure string of characters used to access Hugging Face services and resources.\n",
        "\n",
        "**When an Access Token is Needed:**\n",
        "*   When using a private or \"gated\" model (e.g., Meta's LLaMA) or an Inference API.\n",
        "*   When uploading models, datasets, or Spaces to the Hugging Face Hub.\n",
        "\n",
        "**When an Access Token is NOT Needed:**\n",
        "*   When accessing public models (e.g., GPT-2) which are freely available to download and use without authentication.\n",
        "*   When using models via the Transformers library, as many are publicly available and downloadable without an API key.\n",
        "\n",
        "To create an access token, you need to create an account on the official Hugging Face website (`huggingface.co/join`), then navigate to your profile, select \"Access Tokens,\" and create a new token. It is crucial **not to share** your access tokens with anyone.\n",
        "\n",
        "### 3. Installing Hugging Face Libraries on Google Colab\n",
        "\n",
        "Google Colab is a free web application that can be used to run Python notebooks. You can easily install Hugging Face libraries there.\n",
        "\n",
        "**General Installation Command:**\n",
        "The general command to install libraries using `pip` (a Python package manager) on Google Colab includes an exclamation mark (`!`) at the beginning.\n",
        "\n",
        "```python\n",
        "# General command to install a library on Google Colab\n",
        "!pip install <library_name>\n",
        "```\n",
        "\n",
        "**Specific Installation Commands:**\n",
        "*   **Transformers Library**: `!pip install transformers`\n",
        "*   **Datasets Library**: `!pip install datasets`\n",
        "*   **Tokenizers Library**: `!pip install tokenizers`\n",
        "*   **PyTorch (often required for models)**: `!pip install torch`\n",
        "*   **Diffusers Library (for text-to-image/video)**: `!pip install diffusers`\n",
        "\n",
        "You can change the runtime type on Google Colab (e.g., to T4 GPU or V2-8 TPU for complex projects) for better efficiency.\n",
        "\n",
        "### 4. Downloading a Dataset\n",
        "\n",
        "The `datasets` library allows easy access to a wide variety of datasets for machine learning.\n",
        "\n",
        "**Example: Downloading the IMDB Dataset**\n",
        "\n",
        "```python\n",
        "# 1. Install the datasets library (if not already installed)\n",
        "!pip install datasets\n",
        "\n",
        "# 2. Import the necessary function\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 3. Load the IMDB dataset\n",
        "# The load_dataset function downloads datasets from the Hugging Face Hub or loads from local files.\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# 4. Print the dataset to see its structure\n",
        "# This will display an overview, including the number of samples in each split (train and test).\n",
        "print(imdb_dataset)\n",
        "```\n",
        "\n",
        "**Output Explanation:**\n",
        "The output shows a `DatasetDict` structure, typically with 'train' and 'test' splits.\n",
        "*   **`train`**: Contains 25,000 rows with features like 'text' (movie reviews) and 'label' (sentiment: positive or negative).\n",
        "*   **`test`**: Contains 25,000 rows for testing, with the same features.\n",
        "*   The 'unsupervised' split (50,000 rows) is often used for pre-training or semi-supervised learning and typically lacks labels.\n",
        "\n",
        "### 5. Downloading a Model\n",
        "\n",
        "Models can be downloaded using the `transformers` library, specifically using the `from_pretrained` method.\n",
        "\n",
        "**Example: Downloading a pre-trained BERT Model**\n",
        "\n",
        "```python\n",
        "# 1. Install the transformers library (if not already installed)\n",
        "!pip install transformers\n",
        "\n",
        "# 2. Import necessary modules\n",
        "from transformers import AutoModel # Assuming AutoModel is used, similar to the source's implied usage for BERT.\n",
        "                                      # The source mentions from_pretrained for model weights, config, and tokenizer.\n",
        "                                      # For a full example, AutoTokenizer and AutoModel would typically be used together.\n",
        "\n",
        "# 3. Download a pre-trained BERT model\n",
        "# The from_pretrained method downloads the model weights, configuration, and tokenizer from the Hugging Face Hub.\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# 4. (Optional) Pass an input and check output shape (as shown in source for BERT)\n",
        "# This requires a tokenizer first to convert text to input IDs.\n",
        "# from transformers import AutoTokenizer\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "# inputs = tokenizer(\"hello hugging face\", return_tensors=\"pt\")\n",
        "# outputs = model(**inputs)\n",
        "# print(outputs.last_hidden_state.shape)\n",
        "\n",
        "# Expected output shape for BERT-base-uncased with \"hello hugging face\":\n",
        "# (1, 7, 768)\n",
        "# - 1: Batch size (one input sentence)\n",
        "# - 7: Sequence length (tokenized \"hello hugging face\" including special tokens)\n",
        "# - 768: Hidden size (each token is a 768-dimensional vector, standard for BERT's base architecture)\n",
        "```\n",
        "\n",
        "### 6. Sentiment Analysis\n",
        "\n",
        "Sentiment analysis determines the sentiment (positive, negative, or neutral) expressed in a piece of text.\n",
        "\n",
        "**Types of Sentiment Analysis:**\n",
        "*   **Polarity Detection**: Classifies sentiment as positive, negative, or neutral.\n",
        "    *   *Examples:* \"I love this product\" (positive), \"The service is terrible\" (negative), \"The package arrived on time\" (neutral).\n",
        "*   **Emotion Detection**: Identifies specific emotions like anger, joy, frustration, etc..\n",
        "    *   *Examples:* \"This is pathetic, so frustrating\" (anger), \"I'm thrilled about the results\" (joy).\n",
        "*   **Aspect-Based Sentiment Analysis**: Analyses sentiment towards specific aspects of a product or service.\n",
        "    *   *Example:* \"The food was great but the service was slow\" (positive for food, negative for service).\n",
        "*   **Intent Analysis**: Detects the user's intention, e.g., to purchase or complain.\n",
        "    *   *Example:* \"Where can I buy this product?\" (purchase intent).\n",
        "\n",
        "**Example: Performing Sentiment Analysis**\n",
        "\n",
        "```python\n",
        "# 1. Install required libraries\n",
        "!pip install transformers torch\n",
        "\n",
        "# 2. Import necessary modules and load the sentiment analysis pipeline\n",
        "# The pipeline function provides a simple way to perform various NLP tasks.\n",
        "from transformers import pipeline\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "# Note: For public models, an access token is not needed.\n",
        "\n",
        "# 3. Prepare input text\n",
        "texts_to_analyze = [\n",
        "    \"I love playing and watching cricket.\",\n",
        "    \"I hate when Virat Kohli misses a century.\"\n",
        "]\n",
        "\n",
        "# 4. Perform sentiment analysis\n",
        "results = sentiment_analyzer(texts_to_analyze)\n",
        "\n",
        "# 5. Display the output\n",
        "# The output is a list of dictionaries, with each dictionary containing the sentiment 'label' and 'score' (confidence).\n",
        "for text, result in zip(texts_to_analyze, results):\n",
        "    print(f\"Text: \\\"{text}\\\"\")\n",
        "    print(f\"  Label: {result['label']}, Score: {result['score']:.4f}\\n\")\n",
        "\n",
        "# Output Explanation:\n",
        "# The 'score' is a confidence level (probability) between 0 and 1. Closer to 1 means higher confidence.\n",
        "# The 'label' indicates the predicted sentiment (e.g., 'positive', 'negative').\n",
        "# High scores (close to 1) often indicate strong, unambiguous language in the input text.\n",
        "```\n",
        "\n",
        "### 7. Text Classification\n",
        "\n",
        "Text classification categorises text into predefined classes, such as spam detection, news article classification, or intent detection.\n",
        "\n",
        "**Difference from Sentiment Analysis:**\n",
        "*   **Sentiment Analysis**: Narrow and specific to sentiment (positive, negative, neutral).\n",
        "*   **Text Classification**: Broader, labels depend on the specific task (e.g., spam/not spam, different topics like sports/technology).\n",
        "\n",
        "**Example: Detecting Spam (Text Classification)**\n",
        "\n",
        "```python\n",
        "# 1. Install required libraries\n",
        "!pip install transformers torch\n",
        "\n",
        "# 2. Import necessary modules and load a pre-trained spam detection model\n",
        "from transformers import pipeline\n",
        "# Using a model fine-tuned for sentiment analysis but adapted for spam detection.\n",
        "spam_classifier = pipeline(\"sentiment-analysis\", model=\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
        "# No access token is needed for this publicly available model.\n",
        "\n",
        "# 3. Prepare input text (list of strings to classify)\n",
        "texts_to_classify = [\n",
        "    \"Congratulations! You have won a 500 INR Amazon gift card! Click here to claim.\",\n",
        "    \"Hi Amit, Let's have a meeting tomorrow at 12 p.m.\",\n",
        "    \"Your Gmail account has been compromised. Click here to verify immediately.\"\n",
        "]\n",
        "\n",
        "# 4. Map labels (as the model is sentiment-based)\n",
        "# Negative sentiment can map to 'spam', neutral and positive to 'not spam'.\n",
        "label_mapping = {\n",
        "    \"NEGATIVE\": \"spam\",\n",
        "    \"NEUTRAL\": \"not spam\",\n",
        "    \"POSITIVE\": \"not spam\"\n",
        "}\n",
        "\n",
        "# 5. Perform spam detection and display results\n",
        "for text in texts_to_classify:\n",
        "    result = spam_classifier(text) # The pipeline returns a list, take the first element.\n",
        "    predicted_label = label_mapping.get(result['label'], \"unknown\") # Use .get() for safer access.\n",
        "    print(f\"Text: \\\"{text}\\\"\")\n",
        "    print(f\"  Predicted Label: {predicted_label}, Confidence Score: {result['score']:.4f}\\n\")\n",
        "\n",
        "# Output Explanation:\n",
        "# The output includes a 'label' (e.g., \"NEGATIVE\", \"POSITIVE\") and a 'score' (confidence).\n",
        "# Low confidence scores (e.g., < 0.7) indicate uncertainty in the model's prediction.\n",
        "# The model might be fine-tuned for general sentiment, so it might not be perfectly accurate for spam detection out-of-the-box.\n",
        "```\n",
        "\n",
        "### 8. Text Summarization\n",
        "\n",
        "Text summarization is used to condense long articles, documents, or research papers into shorter snippets or extract key points.\n",
        "\n",
        "**Example: Summarizing Text**\n",
        "\n",
        "```python\n",
        "# 1. Install required libraries\n",
        "!pip install transformers torch\n",
        "\n",
        "# 2. Import necessary modules and load model and tokenizer\n",
        "# AutoModelForSeq2SeqLM is used for sequence-to-sequence tasks like summarization.\n",
        "# AutoTokenizer for tokenizing text.\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
        "\n",
        "# Load a pre-trained model for summarization (publicly available, no access token needed).\n",
        "model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Alternatively, use pipeline for simplicity:\n",
        "# summarizer = pipeline(\"summarization\", model=model_name)\n",
        "\n",
        "# 3. Set input text to summarize\n",
        "input_text = \"\"\"\n",
        "Hugging Face is a company and open-source community that focuses on Natural Language Processing and Artificial Intelligence.\n",
        "It is best known for its transformers library which provides tools and pre-trained models for a wide range of NLP tasks\n",
        "such as text classification, sentiment analysis, machine translation, and more. Hugging Face also includes a model hub\n",
        "that is a platform where users can share and download pre-trained models, datasets, and other resources.\n",
        "Additionally, it provides a library for a variety of datasets called the datasets library, and a platform for hosting\n",
        "and sharing machine learning demos and applications called Spaces. With Hugging Face, users can easily deploy and\n",
        "use models in production environments. The community is strong, with developers and AI enthusiasts contributing to the ecosystem.\n",
        "\"\"\"\n",
        "\n",
        "# 4. Tokenize the input text\n",
        "# return_tensors=\"pt\" ensures PyTorch tensors, max_length ensures truncation if needed.\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "\n",
        "# 5. Generate the summary\n",
        "# Parameters like max_length, min_length, length_penalty, num_beams control summary length and quality.\n",
        "summary_ids = model.generate(\n",
        "    input_ids,\n",
        "    max_length=150,  # Maximum number of tokens in the summary\n",
        "    min_length=30,   # Minimum number of tokens in the summary\n",
        "    length_penalty=2.0, # Encourages longer summaries (value > 1.0)\n",
        "    num_beams=4,     # Controls beam search width; higher values improve quality but slow down inference\n",
        "    early_stopping=True # Stop beam search when all beams have reached a certain stage.\n",
        ")\n",
        "\n",
        "# 6. Decode the generated tokens back to text and print the summary\n",
        "summary = tokenizer.decode(summary_ids, skip_special_tokens=True)\n",
        "print(f\"Original Text:\\n{input_text}\\n\")\n",
        "print(f\"Generated Summary:\\n{summary}\\n\")\n",
        "```\n",
        "\n",
        "### 9. Machine Translation (Text-to-Text Generation)\n",
        "\n",
        "Machine translation involves translating text from one language to another. This falls under **text-to-text generation**, which encompasses tasks where the model takes an input sequence and generates an output sequence, including summarization, paraphrasing, and question answering.\n",
        "\n",
        "**Difference from Text Generation:**\n",
        "*   **Text Generation**: Used for auto-regressive generation (one token at a time), e.g., dialogue systems.\n",
        "*   **Text-to-Text Generation**: Used for sequence-to-sequence tasks, taking an input sequence to generate an output sequence (e.g., translation, summarization).\n",
        "\n",
        "**Example: Translating English to Spanish**\n",
        "\n",
        "```python\n",
        "# 1. Install required libraries\n",
        "!pip install transformers torch\n",
        "\n",
        "# 2. Import necessary modules and load a pre-trained translation model\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "# Load the T5 model (a versatile text-to-text model).\n",
        "model_name = \"t5-small\" # A smaller version of T5 model.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Alternatively, use pipeline for simplicity:\n",
        "# translator = pipeline(\"translation_en_to_es\", model=model_name)\n",
        "\n",
        "# 3. Prepare input text with a task-specific prefix\n",
        "# T5 models often require a task prefix like \"translate English to Spanish:\".\n",
        "input_text = \"translate English to Spanish: My name is Amit Diwan and I love cricket.\"\n",
        "\n",
        "# 4. Tokenize the input text\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# 5. Generate the translated text\n",
        "# max_length and num_beams can be customised.\n",
        "translated_ids = model.generate(input_ids, max_length=50, num_beams=4)\n",
        "\n",
        "# 6. Decode output tokens and print the translated text\n",
        "translated_text = tokenizer.decode(translated_ids, skip_special_tokens=True)\n",
        "print(f\"Original Text: {input_text}\\n\")\n",
        "print(f\"Translated Text: {translated_text}\\n\")\n",
        "```\n",
        "\n",
        "### 10. Question Answering\n",
        "\n",
        "Question answering involves finding the answer to a question within a given \"context\" (a paragraph or text).\n",
        "\n",
        "**Example: Performing Question Answering**\n",
        "\n",
        "```python\n",
        "# 1. Install required libraries\n",
        "!pip install transformers torch\n",
        "\n",
        "# 2. Import necessary modules and load a pre-trained QA model and tokenizer\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load a publicly available QA model (no access token needed).\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
        "\n",
        "# 3. Prepare the context and question\n",
        "context = \"Amit Diwan is a software engineer based in Delhi. He works for a tech company.\"\n",
        "question = \"Where is Amit Diwan based?\"\n",
        "\n",
        "# 4. Get the model's prediction\n",
        "# The pipeline handles tokenization, model prediction, and answer extraction internally.\n",
        "result = qa_pipeline(question=question, context=context)\n",
        "\n",
        "# 5. Display the answer\n",
        "print(f\"Context: {context}\\n\")\n",
        "print(f\"Question: {question}\\n\")\n",
        "print(f\"Answer: {result['answer']}\")\n",
        "print(f\"Confidence Score: {result['score']:.4f}\\n\")\n",
        "\n",
        "# Output Explanation:\n",
        "# The output directly provides the 'answer' extracted from the context and a 'score' indicating confidence.\n",
        "```\n",
        "\n",
        "### 11. Text to Image\n",
        "\n",
        "Text-to-image generation creates an image from a textual description. This often utilises diffusion models, which are a class of generative models. The Hugging Face **Diffusers library** is an open-source Python library focusing on diffusion models for generating images, audio, and other data types. **Stable Diffusion** is a popular latent diffusion model within the Diffusers library for high-quality image generation from text prompts.\n",
        "\n",
        "**Example: Generating an Image from Text**\n",
        "\n",
        "```python\n",
        "# 1. Install required libraries\n",
        "!pip install diffusers transformers torch accelerate\n",
        "# 'accelerate' is often needed for optimisations when running diffusers models.\n",
        "\n",
        "# 2. Import necessary modules and load the Stable Diffusion pipeline\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "# Load a publicly available Stable Diffusion model (no access token needed).\n",
        "# Ensure you accept the model's terms if prompted, which might require logging in to Hugging Face Hub (but not an access token for public models).\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "pipeline = StableDiffusionPipeline.from_pretrained(model_id)\n",
        "\n",
        "# If running on Colab GPU, move pipeline to CUDA for faster inference\n",
        "# pipeline.to(\"cuda\")\n",
        "\n",
        "# 3. Define the text prompt\n",
        "prompt = \"Flying cars soar over a futuristic cityscape at sunset.\"\n",
        "\n",
        "# 4. Generate the image\n",
        "# The pipeline generates an image by passing the text prompt.\n",
        "image = pipeline(prompt).images\n",
        "\n",
        "# 5. Save and display the image\n",
        "image_path = \"generated_image.png\"\n",
        "image.save(image_path)\n",
        "print(f\"Image saved as {image_path}\")\n",
        "\n",
        "# To display the image in Colab (requires PIL/Pillow):\n",
        "from IPython.display import Image, display\n",
        "display(Image(filename=image_path))\n",
        "\n",
        "# The image will be saved in the Google Colab file system.\n",
        "```\n",
        "\n",
        "### 12. Text to Video\n",
        "\n",
        "Text-to-video synthesis involves generating a video from textual descriptions. This is a complex task that combines NLP models with generative models or diffusion models. Hugging Face provides models and tools for this, often leveraging the **Diffusers library**.\n",
        "\n",
        "**Example: Generating a Video from Text (Stitching Frames)**\n",
        "\n",
        "This example shows how to generate individual frames using a text-to-image model and then stitch them into a video using OpenCV.\n",
        "\n",
        "```python\n",
        "# 1. Install required libraries\n",
        "# OpenCV (cv2) and NumPy are needed for video stitching.\n",
        "!pip install diffusers transformers torch accelerate opencv-python numpy\n",
        "\n",
        "# 2. Import necessary modules and load a text-to-image model\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Load a publicly available text-to-image model (Stable Diffusion).\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "pipeline = StableDiffusionPipeline.from_pretrained(model_id)\n",
        "\n",
        "# If running on Colab GPU, move pipeline to CUDA for faster inference\n",
        "# pipeline.to(\"cuda\")\n",
        "\n",
        "# 3. Define the text prompt\n",
        "prompt = \"A futuristic cityscape at night with flying cars.\"\n",
        "\n",
        "# 4. Generate individual frames based on the text description\n",
        "num_frames = 10\n",
        "frames = []\n",
        "for i in range(num_frames):\n",
        "    # Generating slightly different images for each frame based on the same prompt.\n",
        "    # In a real text-to-video model, the model itself would handle temporal consistency.\n",
        "    # Here, we're simulating by generating distinct images.\n",
        "    image = pipeline(prompt).images\n",
        "    frames.append(np.array(image)) # Convert PIL Image to NumPy array for OpenCV\n",
        "    image.save(f\"frame_{i}.png\") # Save individual frames\n",
        "    print(f\"Generated frame_{i}.png\")\n",
        "\n",
        "# 5. Stitch the frames into a video using OpenCV\n",
        "if frames:\n",
        "    height, width, layers = frames.shape\n",
        "    video_name = 'output_video.mp4'\n",
        "    # Define the codec and create VideoWriter object\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Codec for .mp4 files\n",
        "    video = cv2.VideoWriter(video_name, fourcc, 1, (width, height)) # 1 FPS for demonstration\n",
        "\n",
        "    for frame in frames:\n",
        "        video.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)) # Convert RGB to BGR for OpenCV\n",
        "\n",
        "    video.release() # Release the video writer object\n",
        "    print(f\"\\nVideo saved as {video_name}\")\n",
        "\n",
        "# You can download the generated frames (frame_0.png to frame_9.png) and output_video.mp4 from Colab's file browser.\n",
        "```\n",
        "\n",
        " -->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5liXG1XLJWbC",
      "metadata": {
        "id": "5liXG1XLJWbC"
      },
      "source": [
        "# 1ï¸âƒ£4ï¸âƒ£ **Image Captioning (Multimodal)**\n",
        "\n",
        "Model describes what is in the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JxXhGYMqJW1N",
      "metadata": {
        "id": "JxXhGYMqJW1N"
      },
      "outputs": [],
      "source": [
        "captioner = pipeline(\"image-to-text\")\n",
        "captioner(\"dog.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v6Jb5KmRNHmO",
      "metadata": {
        "id": "v6Jb5KmRNHmO"
      },
      "source": [
        "# ðŸŒŸ **MODULE 6 â€“ FINE-TUNING (TRAINER API + CUSTOM DATASETS)**\n",
        "Fine-tuning means **teaching a pre-trained AI model a new skill** using your own data.\n",
        "\n",
        "---\n",
        "\n",
        "# 2ï¸âƒ£ **Why Do We Fine-Tune?**\n",
        "\n",
        "### â­ Non-Technical:\n",
        "\n",
        "* Saves time (model already knows language)\n",
        "* Needs less data\n",
        "* Cheaper than training from scratch\n",
        "* Produces accurate results\n",
        "\n",
        "### â­ Technical:\n",
        "\n",
        "* Improves model performance on downstream tasks\n",
        "* Requires small datasets (1kâ€“50k)\n",
        "* Backpropagation updates parameters\n",
        "* Works for text, images, audio\n",
        "\n",
        "---\n",
        "\n",
        "# 3ï¸âƒ£ **Fine-Tuning Workflow (Beginner Diagram)**\n",
        "\n",
        "```\n",
        "      Pretrained Model (BERT, DistilBERT, GPT)\n",
        "                        â†“\n",
        "            Add small labeled dataset\n",
        "                        â†“\n",
        "                Train (few minutes)\n",
        "                       â†“\n",
        "           Model learns your specific task\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# 4ï¸âƒ£ **Hugging Face Tools for Fine-Tuning**\n",
        "\n",
        "```\n",
        "Transformers â†’ Models + Tokenizers\n",
        "Datasets     â†’ Load your dataset\n",
        "Trainer API  â†’ Training loop\n",
        "PEFT         â†’ Efficient training (LoRA)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# 5ï¸âƒ£ **Key Concepts Before Fine-Tuning**\n",
        "\n",
        "| Concept           | Beginner-Friendly Meaning            |\n",
        "| ----------------- | ------------------------------------ |\n",
        "| **Epoch**         | One full pass over the dataset       |\n",
        "| **Batch Size**    | Number of samples processed together |\n",
        "| **Learning Rate** | How fast the model learns            |\n",
        "| **Loss**          | Model mistake level (lower = better) |\n",
        "| **Evaluation**    | Checking model accuracy              |\n",
        "| **Metrics**       | Accuracy, F1, Precision, Recall      |\n",
        "\n",
        "---\n",
        "\n",
        "# 6ï¸âƒ£ **Trainer API â€” Easiest Way to Fine-Tune**\n",
        "\n",
        "### â­ Non-Technical Explanation:\n",
        "\n",
        "Trainer is a **ready-made training engine** that trains models for you.\n",
        "\n",
        "### â­ Technical Explanation:\n",
        "\n",
        "Trainer manages:\n",
        "\n",
        "* Data loaders\n",
        "* Optimizers\n",
        "* Schedulers\n",
        "* Logging\n",
        "* Mixed precision (fp16)\n",
        "* Evaluation loops\n",
        "* Saving checkpoints\n",
        "\n",
        "---\n",
        "\n",
        "# 7ï¸âƒ£ **Steps for Fine-Tuning Using Trainer API**\n",
        "\n",
        "```\n",
        "1. Load dataset\n",
        "2. Load tokenizer\n",
        "3. Tokenize dataset\n",
        "4. Load pretrained model\n",
        "5. Define training settings\n",
        "6. Train with Trainer()\n",
        "7. Evaluate\n",
        "8. Save/push model\n",
        "```\n",
        "\n",
        "# 5ï¸âƒ£ **Key Concepts Before Fine-Tuning**\n",
        "\n",
        "| Concept           | Beginner-Friendly Meaning            |\n",
        "| ----------------- | ------------------------------------ |\n",
        "| **Epoch**         | One full pass over the dataset       |\n",
        "| **Batch Size**    | Number of samples processed together |\n",
        "| **Learning Rate** | How fast the model learns            |\n",
        "| **Loss**          | Model mistake level (lower = better) |\n",
        "| **Evaluation**    | Checking model accuracy              |\n",
        "| **Metrics**       | Accuracy, F1, Precision, Recall      |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bmYDvORwOJCx",
      "metadata": {
        "id": "bmYDvORwOJCx"
      },
      "outputs": [],
      "source": [
        "# 6ï¸âƒ£ **Trainer API â€” Easiest Way to Fine-Tune**\n",
        "# 8ï¸âƒ£ **Beginner Code: Text Classification Fine-Tuning**\n",
        "\n",
        "# pip install transformers datasets\n",
        "\n",
        "# from datasets import load_dataset\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "## Step 2 â€” Load Tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "## Step 3 â€” Tokenize the Data\n",
        "def tok_fn(batch):\n",
        "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized = dataset.map(tok_fn, batched=True)\n",
        "\n",
        "## Step 4 â€” Load Pretrained Model\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\", num_labels=2\n",
        ")\n",
        "\n",
        "## Step 5 â€” Training Arguments\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"output\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "## Step 6 â€” Trainer Object\n",
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    eval_dataset=tokenized[\"test\"]\n",
        ")\n",
        "\n",
        "## Step 7 â€” Train\n",
        "trainer.train()\n",
        "\n",
        "## Step 8 â€” Evaluate\n",
        "trainer.evaluate()\n",
        "\n",
        "## Step 9 â€” Save Model\n",
        "trainer.save_model(\"sentiment_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ITfrXmdUQIpP",
      "metadata": {
        "id": "ITfrXmdUQIpP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "Ul9PwTEXQJVx",
      "metadata": {
        "id": "Ul9PwTEXQJVx"
      },
      "source": [
        "### 9ï¸âƒ£ **Fine-Tuning Custom Datasets (CSV, JSON, Excel)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2lS7iLbJQKOn",
      "metadata": {
        "id": "2lS7iLbJQKOn"
      },
      "outputs": [],
      "source": [
        "### Load CSV:\n",
        "dataset = load_dataset(\"csv\", data_files=\"mydata.csv\")\n",
        "### Columns should include:\n",
        "# * `text`\n",
        "# * `label`\n",
        "\n",
        "# If names differ, rename:\n",
        "dataset = dataset.rename_column(\"review\", \"text\")\n",
        "\n",
        "\n",
        "# ðŸ”Ÿ **Evaluation Metrics**\n",
        "# Metrics tell how good the model is.\n",
        "import evaluate\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    predictions, labels = pred\n",
        "    predictions = predictions.argmax(axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# Add to Trainer:\n",
        "Trainer(... compute_metrics=compute_metrics)\n",
        "\n",
        "# 1ï¸âƒ£1ï¸âƒ£ **Saving & Uploading to Hugging Face Hub**\n",
        "trainer.push_to_hub(\"my-finetuned-model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kQxYCFz9QsGc",
      "metadata": {
        "id": "kQxYCFz9QsGc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "aTTbzEm7Qu19",
      "metadata": {
        "id": "aTTbzEm7Qu19"
      },
      "source": [
        "# **PEFT: Parameter Efficient Fine-Tuning (Beginner + Technical)**\n",
        "\n",
        "# PEFT = Fine-tune **only small parts** of the model â†’ saves memory.\n",
        "\n",
        "### Technical:\n",
        "Use LoRA, QLoRA, Prefix Tuning.\n",
        "\n",
        "### Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PKVqmcWtQvtr",
      "metadata": {
        "id": "PKVqmcWtQvtr"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    task_type=\"SEQ_CLS\"\n",
        ")\n",
        "model = get_peft_model(model, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v0jYBsFURj9O",
      "metadata": {
        "id": "v0jYBsFURj9O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "9JkfZu_1Rkny",
      "metadata": {
        "id": "9JkfZu_1Rkny"
      },
      "source": [
        "\n",
        "## ðŸŒŸ **MODULE 8 â€” ACCELERATE & PEFT (Efficient Training for Large Models)**\n",
        "\n",
        "# 1ï¸âƒ£ **What is Efficient Training?**\n",
        "\n",
        "Efficient training means **training big AI models using less memory, less cost, and faster speed**.\n",
        "\n",
        "Techniques like:\n",
        "\n",
        "* Distributed training\n",
        "* Mixed precision (fp16/bf16)\n",
        "* Parameter-efficient fine-tuning\n",
        "* Quantization\n",
        "\n",
        "\n",
        "### 2ï¸âƒ£ **Why Do We Need Efficient Training?**\n",
        "\n",
        "### â­ Beginners:\n",
        "\n",
        "* Many models are huge (billions of parameters)\n",
        "* Normal computers cannot train them\n",
        "* Efficient methods make training possible\n",
        "\n",
        "### â­ Technical Users:\n",
        "\n",
        "* Reduce VRAM usage (40â€“70%)\n",
        "* Reduce training time\n",
        "* Enable multi-GPU training\n",
        "* Allow fine-tuning LLMs (7Bâ€“70B) on a single GPU\n",
        "\n",
        "---\n",
        "\n",
        "### 3ï¸âƒ£ **Two Major Tools in Hugging Face:**\n",
        "\n",
        "```\n",
        "1. Accelerate  â†’ Efficient training on any hardware\n",
        "2. PEFT        â†’ Train only small parts of model (LoRA, QLoRA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l39x869MRjrZ",
      "metadata": {
        "id": "l39x869MRjrZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ”µ **PART A â€” ACCELERATE**\n",
        "\n",
        "---\n",
        "\n",
        "# 4ï¸âƒ£ **What is Accelerate?**\n",
        "\n",
        "Accelerate helps you **train models on CPU or GPU easily**, without writing complex code.\n",
        "\n",
        "### â­ Technical Explanation:\n",
        "\n",
        "* Supports distributed training\n",
        "* Mixed precision (fp16, bf16)\n",
        "* TPU support\n",
        "* Multi-GPU handling\n",
        "* Device mapping\n",
        "* Zero-code-scale training\n",
        "\n",
        "---\n",
        "\n",
        "The config command helps you choose:\n",
        "\n",
        "* CPU\n",
        "* Single GPU\n",
        "* Multiple GPUs\n",
        "* Mixed precision\n",
        "\n",
        "---\n",
        "\n",
        "### 6ï¸âƒ£ **Accelerate Workflow Diagram**\n",
        "\n",
        "```\n",
        "Your Model & Training Code\n",
        "             â†“\n",
        "     accelerate.prepare()\n",
        "             â†“\n",
        " Multi-GPU / TPU / CPU Auto Handling\n",
        "             â†“\n",
        "         Efficient Training\n",
        "```\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "-L7Rdvv9V3wP"
      },
      "id": "-L7Rdvv9V3wP"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install accelerate\n",
        "accelerate config"
      ],
      "metadata": {
        "id": "veAhMH-QV3XV"
      },
      "id": "veAhMH-QV3XV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7ï¸âƒ£ **Basic Accelerate Example (Technical)**\n",
        "from accelerate import Accelerator\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "accelerator = Accelerator()\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "model, optimizer, train_loader = accelerator.prepare(\n",
        "    model, optimizer, train_loader\n",
        ")\n",
        "\n",
        "# This automatically:\n",
        "\n",
        "# * moves model to GPU\n",
        "# * handles mixed precision\n",
        "# * handles distributed training"
      ],
      "metadata": {
        "id": "uR6LsAS0Wljm"
      },
      "id": "uR6LsAS0Wljm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8ï¸âƒ£ **Accelerate with Trainer**\n",
        "\n",
        "The Trainer API **already integrates** Accelerate internally.\n",
        "\n",
        "No change needed â€” Accelerate is used automatically."
      ],
      "metadata": {
        "id": "Z1S9HFLCXEct"
      },
      "id": "Z1S9HFLCXEct"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NFecY-KwWw0K"
      },
      "id": "NFecY-KwWw0K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VRM4uEiAYq0v"
      },
      "id": "VRM4uEiAYq0v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŸ¢ **PART B â€” PEFT (Parameter-Efficient Fine-Tuning)**\n",
        "\n",
        "---\n",
        "\n",
        "# 9ï¸âƒ£ **What is PEFT?**\n",
        "\n",
        "### â­ Beginner-Friendly:\n",
        "\n",
        "PEFT means **fine-tuning only small parts of a large model**, instead of updating all weights.\n",
        "\n",
        "This makes training:\n",
        "\n",
        "* Cheaper\n",
        "* Faster\n",
        "* Possible on normal GPUs\n",
        "\n",
        "### â­ Technical:\n",
        "\n",
        "PEFT updates **1â€“5%** of total parameters.\n",
        "\n",
        "Supported methods:\n",
        "\n",
        "* LoRA\n",
        "* QLoRA\n",
        "* Prefix Tuning\n",
        "* P-Tuning v2\n",
        "* Adapters\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸ”Ÿ **Why PEFT is Important?**\n",
        "\n",
        "### â­ Beginners:\n",
        "\n",
        "Big models (like Llama, Mistral, GPT-J) are too heavy.\n",
        "PEFT lets you fine-tune them on a laptop or Google Colab.\n",
        "\n",
        "### â­ Technical Users:\n",
        "\n",
        "* Huge memory savings (50â€“80%)\n",
        "* Enables 4-bit training\n",
        "* Supports LLMs (7Bâ€“70B)\n",
        "\n",
        "---\n",
        "\n",
        "# 1ï¸âƒ£1ï¸âƒ£ **PEFT Diagram (Simple)**\n",
        "\n",
        "```\n",
        "Full Model (7 Billion Params)\n",
        " â†“\n",
        "Freeze 99% of weights\n",
        " â†“\n",
        "Train only small LoRA layers\n",
        " â†“\n",
        "Small, fast fine-tuning\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# 1ï¸âƒ£2ï¸âƒ£ **PEFT Techniques Explained Simply**\n",
        "\n",
        "| Technique         | Simple Meaning             | Technical Meaning             |\n",
        "| ----------------- | -------------------------- | ----------------------------- |\n",
        "| **LoRA**          | Train small extra layers   | Low-rank matrix decomposition |\n",
        "| **QLoRA**         | Train LoRA in 4-bit        | Uses NF4 quantization         |\n",
        "| **Prefix Tuning** | Add extra learnable tokens | Learnable prefix embeddings   |\n",
        "| **Adapters**      | Insert small modules       | Residual adapter layers       |\n",
        "\n",
        "---\n",
        "\n",
        "# 1ï¸âƒ£3ï¸âƒ£ **LoRA Example (Beginner-Friendly)**\n",
        "\n",
        "LoRA adds small layers to the model like â€œplug-insâ€.\n",
        "\n",
        "Instead of updating the whole model, LoRA updates only the added plug-in layers.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "8B-lkEGZYrVY"
      },
      "id": "8B-lkEGZYrVY"
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", num_labels=2\n",
        ")\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"query\", \"value\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_CLS\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# This prints:\n",
        "# Trainable params: 1% (LoRA only)"
      ],
      "metadata": {
        "id": "PxS9UeObYqrz"
      },
      "id": "PxS9UeObYqrz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VsQwJBFRYqkj"
      },
      "id": "VsQwJBFRYqkj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1ï¸âƒ£5ï¸âƒ£ **QLoRA (The Most Popular Technique)**\n",
        "\n",
        "QLoRA lets you fine-tune **very big models** using **very low memory**.\n",
        "\n",
        "### â­ Technical Explanation:\n",
        "\n",
        "* Uses 4-bit quantization (NF4)\n",
        "* Keeps base model frozen\n",
        "* Trains LoRA adapters\n",
        "---\n",
        "# **QLoRA Code Example**"
      ],
      "metadata": {
        "id": "DuV7BxgrZS-K"
      },
      "id": "DuV7BxgrZS-K"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=\"float16\"\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"facebook/opt-1.3b\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "WHEA6oa8ZSiF",
        "outputId": "03a216c0-1dba-406b-e405-cb435ebf7048"
      },
      "id": "WHEA6oa8ZSiF",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PackageNotFoundError",
          "evalue": "No package metadata was found for bitsandbytes",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mfrom_name\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mStopIteration\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-773809284.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoraConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_peft_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m bnb_config = BitsAndBytesConfig(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mload_in_4bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mbnb_4bit_compute_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float16\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/quantization_config.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, load_in_8bit, load_in_4bit, llm_int8_threshold, llm_int8_skip_modules, llm_int8_enable_fp32_cpu_offload, llm_int8_has_fp16_weight, bnb_4bit_compute_dtype, bnb_4bit_quant_type, bnb_4bit_use_double_quant, bnb_4bit_quant_storage, **kwargs)\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unused kwargs: {list(kwargs.keys())}. These kwargs are not used in {self.__class__}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/quantization_config.py\u001b[0m in \u001b[0;36mpost_init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bnb_4bit_use_double_quant must be a boolean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         if self.load_in_4bit and not version.parse(importlib.metadata.version(\"bitsandbytes\")) >= version.parse(\n\u001b[0m\u001b[1;32m    569\u001b[0m             \u001b[0;34m\"0.39.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         ):\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mversion\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;34m\"Version\"\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m     \"\"\"\n\u001b[0;32m--> 889\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistribution_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mdistribution\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDistribution\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mor\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mthereof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m     \"\"\"\n\u001b[0;32m--> 862\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistribution_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mfrom_name\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPackageNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPackageNotFoundError\u001b[0m: No package metadata was found for bitsandbytes",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZI4d0iymbbR6"
      },
      "id": "ZI4d0iymbbR6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 1ï¸âƒ£7ï¸âƒ£ **Advantages of Accelerate + PEFT**\n",
        "\n",
        "### â­ Beginners:\n",
        "\n",
        "* Train faster\n",
        "* Use cheap hardware\n",
        "* Learn large models easily\n",
        "\n",
        "### â­ Technical:\n",
        "\n",
        "* Minimal VRAM usage\n",
        "* Supports LLM fine-tuning\n",
        "* Multi-GPU distributed training\n",
        "* Mixed precision FP16/BF16\n",
        "\n",
        "---\n",
        "\n",
        "# 1ï¸âƒ£8ï¸âƒ£ **When to Use Accelerate?**\n",
        "\n",
        "âœ” When training on multiple GPUs\n",
        "âœ” When training big models\n",
        "âœ” When you want mixed precision\n",
        "âœ” When you need distributed training\n",
        "\n",
        "---\n",
        "\n",
        "# 1ï¸âƒ£9ï¸âƒ£ **When to Use PEFT?**\n",
        "\n",
        "âœ” You want to fine-tune LLMs (7Bâ€“70B)\n",
        "âœ” You have 1 GPU with 8â€“16 GB VRAM\n",
        "âœ” You want lightweight models for deployment\n",
        "âœ” You want low training cost\n",
        "\n",
        "---\n",
        "\n",
        "# 2ï¸âƒ£0ï¸âƒ£ **Full Workflow Diagram**\n",
        "\n",
        "```\n",
        "Load Dataset\n",
        "    â†“\n",
        "Load Pretrained Model\n",
        "    â†“\n",
        "Apply PEFT (LoRA / QLoRA / Adapters)\n",
        "    â†“\n",
        "Prepare Model using Accelerate\n",
        "    â†“\n",
        "Train using Trainer or custom loop\n",
        "    â†“\n",
        "Save & Push to Hugging Face Hub\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# 2ï¸âƒ£1ï¸âƒ£ **Real-World Use Cases**\n",
        "\n",
        "### NLP\n",
        "\n",
        "âœ” Customer-specific chatbot\n",
        "âœ” Legal domain Q&A\n",
        "âœ” Medical text classifier\n",
        "âœ” Email classification\n",
        "\n",
        "### Vision\n",
        "\n",
        "âœ” Fine-tuning ViT with LoRA\n",
        "âœ” Product defect classification\n",
        "\n",
        "### Audio\n",
        "\n",
        "âœ” Language-specific Whisper fine-tuning\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "EXiTwZjEbb3e"
      },
      "id": "EXiTwZjEbb3e"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "95wEKvhQbbO8"
      },
      "id": "95wEKvhQbbO8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IDiPsfZZbbK3"
      },
      "id": "IDiPsfZZbbK3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HBSw963vbbGX"
      },
      "id": "HBSw963vbbGX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U3xZOSWWbbAT"
      },
      "id": "U3xZOSWWbbAT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DzVMarlbZSXV"
      },
      "id": "DzVMarlbZSXV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "BffM54NoQtBQ",
      "metadata": {
        "id": "BffM54NoQtBQ"
      },
      "source": [
        "# 1ï¸âƒ£3ï¸âƒ£ **Fine-Tuning Tips for Students**\n",
        "* Use small models (DistilBERT)\n",
        "* Use small batch sizes (8â€“16)\n",
        "* Train 2â€“3 epochs\n",
        "* Use mixed precision\n",
        "* Use weight decay\n",
        "* Use gradient checkpointing\n",
        "* Perform hyperparameter tuning\n",
        "\n",
        "---\n",
        "\n",
        "# 1ï¸âƒ£4ï¸âƒ£ **Common Training Errors**\n",
        "\n",
        "| Error              | Cause            | Fix                  |\n",
        "| ------------------ | ---------------- | -------------------- |\n",
        "| CUDA Out of Memory | Large batch size | Reduce batch size    |\n",
        "| Slow training      | Large model      | Use DistilBERT, PEFT |\n",
        "| Wrong labels       | Dataset mismatch | Check label mapping  |\n",
        "| Token mismatch     | Wrong tokenizer  | Use same model-name  |\n",
        "\n",
        "---\n",
        "\n",
        "# 1ï¸âƒ£6ï¸âƒ£ **Real-World Use Cases of Fine-Tuning**\n",
        "\n",
        "### NLP:\n",
        "\n",
        "âœ” Sentiment analysis\n",
        "âœ” Email classification\n",
        "âœ” Chatbot for a company\n",
        "âœ” Resume screening\n",
        "âœ” Domain-specific QA\n",
        "\n",
        "### Vision:\n",
        "\n",
        "âœ” Medical image classification\n",
        "âœ” Product defect detection\n",
        "\n",
        "### Audio:\n",
        "\n",
        "âœ” Accent-specific speech recognition"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aienv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}