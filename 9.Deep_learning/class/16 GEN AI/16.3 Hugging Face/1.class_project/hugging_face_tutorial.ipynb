{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750cc198",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 11. Text to Image\n",
    "\n",
    "# Text-to-image generation creates an image from a textual description. This often utilises diffusion models, which are a class of generative models. The Hugging Face **Diffusers library** is an open-source Python library focusing on diffusion models for generating images, audio, and other data types. **Stable Diffusion** is a popular latent diffusion model within the Diffusers library for high-quality image generation from text prompts.\n",
    "\n",
    "# **Example: Generating an Image from Text**\n",
    "\n",
    "\n",
    "# 1. Install required libraries\n",
    "!pip install diffusers transformers torch accelerate\n",
    "# 'accelerate' is often needed for optimisations when running diffusers models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea8597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import necessary modules and load the Stable Diffusion pipeline\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# Load a publicly available Stable Diffusion model (no access token needed).\n",
    "# Ensure you accept the model's terms if prompted, which might require logging in to Hugging Face Hub (but not an access token for public models).\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(model_id)\n",
    "\n",
    "# If running on Colab GPU, move pipeline to CUDA for faster inference\n",
    "# pipeline.to(\"cuda\")\n",
    "\n",
    "# 3. Define the text prompt\n",
    "prompt = \"Flying cars soar over a futuristic cityscape at sunset.\"\n",
    "\n",
    "# 4. Generate the image\n",
    "# The pipeline generates an image by passing the text prompt.\n",
    "image = pipeline(prompt).images\n",
    "\n",
    "# 5. Save and display the image\n",
    "image_path = \"generated_image.png\"\n",
    "image.save(image_path)\n",
    "print(f\"Image saved as {image_path}\")\n",
    "\n",
    "# To display the image in Colab (requires PIL/Pillow):\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=image_path))\n",
    "\n",
    "# The image will be saved in the Google Colab file system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fff49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Here are detailed notes with code examples for various tasks using Hugging Face, drawing upon the provided YouTube source.\n",
    "\n",
    "***\n",
    "\n",
    "### 1. Introduction to Hugging Face and its Core Libraries\n",
    "\n",
    "Hugging Face is a company and open-source community focused on Natural Language Processing (NLP) and Artificial Intelligence (AI). It is best known for its **Transformers library**, which offers tools and pre-trained models for a wide range of NLP tasks. Beyond Transformers, Hugging Face also provides other widely used libraries, including **Datasets** and **Tokenizers**. It also features a **Model Hub** for sharing and downloading pre-trained models, datasets, and other resources, and **Spaces** for hosting and sharing machine learning demos and applications.\n",
    "\n",
    "**Key Libraries:**\n",
    "*   **Transformers Library**: The core library for pre-trained models and pipelines, providing access to thousands of pre-trained models for NLP tasks like translation, text summarization, and text classification. It is simple to use with complex NLP models, offers cutting-edge models, supports customization, and has a large, active community.\n",
    "*   **Datasets Library**: Provides easy access to a wide variety of datasets for NLP and other machine learning tasks. It enables efficient work with large datasets through lazy loading and streaming, offers a unified API for processing datasets, and integrates well with the Transformers library and other ML frameworks. Hugging Face provides over 350,000 datasets on its platform.\n",
    "*   **Tokenizers Library**: A fast, efficient, and flexible library designed for tokenizing text data, a crucial step in NLP. Tokenization involves splitting text into smaller units (words, subwords, characters) and converting them into numerical representations for ML models. It is optimised for fast tokenization, supports custom tokenizers, and integrates with other Hugging Face libraries like Transformers.\n",
    "\n",
    "### 2. Hugging Face Access Token\n",
    "\n",
    "An access token (also referred to as an API key) is a secure string of characters used to access Hugging Face services and resources.\n",
    "\n",
    "**When an Access Token is Needed:**\n",
    "*   When using a private or \"gated\" model (e.g., Meta's LLaMA) or an Inference API.\n",
    "*   When uploading models, datasets, or Spaces to the Hugging Face Hub.\n",
    "\n",
    "**When an Access Token is NOT Needed:**\n",
    "*   When accessing public models (e.g., GPT-2) which are freely available to download and use without authentication.\n",
    "*   When using models via the Transformers library, as many are publicly available and downloadable without an API key.\n",
    "\n",
    "To create an access token, you need to create an account on the official Hugging Face website (`huggingface.co/join`), then navigate to your profile, select \"Access Tokens,\" and create a new token. It is crucial **not to share** your access tokens with anyone.\n",
    "\n",
    "### 3. Installing Hugging Face Libraries on Google Colab\n",
    "\n",
    "Google Colab is a free web application that can be used to run Python notebooks. You can easily install Hugging Face libraries there.\n",
    "\n",
    "**General Installation Command:**\n",
    "The general command to install libraries using `pip` (a Python package manager) on Google Colab includes an exclamation mark (`!`) at the beginning.\n",
    "\n",
    "```python\n",
    "# General command to install a library on Google Colab\n",
    "!pip install <library_name>\n",
    "```\n",
    "\n",
    "**Specific Installation Commands:**\n",
    "*   **Transformers Library**: `!pip install transformers`\n",
    "*   **Datasets Library**: `!pip install datasets`\n",
    "*   **Tokenizers Library**: `!pip install tokenizers`\n",
    "*   **PyTorch (often required for models)**: `!pip install torch`\n",
    "*   **Diffusers Library (for text-to-image/video)**: `!pip install diffusers`\n",
    "\n",
    "You can change the runtime type on Google Colab (e.g., to T4 GPU or V2-8 TPU for complex projects) for better efficiency.\n",
    "\n",
    "### 4. Downloading a Dataset\n",
    "\n",
    "The `datasets` library allows easy access to a wide variety of datasets for machine learning.\n",
    "\n",
    "**Example: Downloading the IMDB Dataset**\n",
    "\n",
    "```python\n",
    "# 1. Install the datasets library (if not already installed)\n",
    "!pip install datasets\n",
    "\n",
    "# 2. Import the necessary function\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 3. Load the IMDB dataset\n",
    "# The load_dataset function downloads datasets from the Hugging Face Hub or loads from local files.\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# 4. Print the dataset to see its structure\n",
    "# This will display an overview, including the number of samples in each split (train and test).\n",
    "print(imdb_dataset)\n",
    "```\n",
    "\n",
    "**Output Explanation:**\n",
    "The output shows a `DatasetDict` structure, typically with 'train' and 'test' splits.\n",
    "*   **`train`**: Contains 25,000 rows with features like 'text' (movie reviews) and 'label' (sentiment: positive or negative).\n",
    "*   **`test`**: Contains 25,000 rows for testing, with the same features.\n",
    "*   The 'unsupervised' split (50,000 rows) is often used for pre-training or semi-supervised learning and typically lacks labels.\n",
    "\n",
    "### 5. Downloading a Model\n",
    "\n",
    "Models can be downloaded using the `transformers` library, specifically using the `from_pretrained` method.\n",
    "\n",
    "**Example: Downloading a pre-trained BERT Model**\n",
    "\n",
    "```python\n",
    "# 1. Install the transformers library (if not already installed)\n",
    "!pip install transformers\n",
    "\n",
    "# 2. Import necessary modules\n",
    "from transformers import AutoModel # Assuming AutoModel is used, similar to the source's implied usage for BERT.\n",
    "                                      # The source mentions from_pretrained for model weights, config, and tokenizer.\n",
    "                                      # For a full example, AutoTokenizer and AutoModel would typically be used together.\n",
    "\n",
    "# 3. Download a pre-trained BERT model\n",
    "# The from_pretrained method downloads the model weights, configuration, and tokenizer from the Hugging Face Hub.\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 4. (Optional) Pass an input and check output shape (as shown in source for BERT)\n",
    "# This requires a tokenizer first to convert text to input IDs.\n",
    "# from transformers import AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# inputs = tokenizer(\"hello hugging face\", return_tensors=\"pt\")\n",
    "# outputs = model(**inputs)\n",
    "# print(outputs.last_hidden_state.shape)\n",
    "\n",
    "# Expected output shape for BERT-base-uncased with \"hello hugging face\":\n",
    "# (1, 7, 768)\n",
    "# - 1: Batch size (one input sentence)\n",
    "# - 7: Sequence length (tokenized \"hello hugging face\" including special tokens)\n",
    "# - 768: Hidden size (each token is a 768-dimensional vector, standard for BERT's base architecture)\n",
    "```\n",
    "\n",
    "### 6. Sentiment Analysis\n",
    "\n",
    "Sentiment analysis determines the sentiment (positive, negative, or neutral) expressed in a piece of text.\n",
    "\n",
    "**Types of Sentiment Analysis:**\n",
    "*   **Polarity Detection**: Classifies sentiment as positive, negative, or neutral.\n",
    "    *   *Examples:* \"I love this product\" (positive), \"The service is terrible\" (negative), \"The package arrived on time\" (neutral).\n",
    "*   **Emotion Detection**: Identifies specific emotions like anger, joy, frustration, etc..\n",
    "    *   *Examples:* \"This is pathetic, so frustrating\" (anger), \"I'm thrilled about the results\" (joy).\n",
    "*   **Aspect-Based Sentiment Analysis**: Analyses sentiment towards specific aspects of a product or service.\n",
    "    *   *Example:* \"The food was great but the service was slow\" (positive for food, negative for service).\n",
    "*   **Intent Analysis**: Detects the user's intention, e.g., to purchase or complain.\n",
    "    *   *Example:* \"Where can I buy this product?\" (purchase intent).\n",
    "\n",
    "**Example: Performing Sentiment Analysis**\n",
    "\n",
    "```python\n",
    "# 1. Install required libraries\n",
    "!pip install transformers torch\n",
    "\n",
    "# 2. Import necessary modules and load the sentiment analysis pipeline\n",
    "# The pipeline function provides a simple way to perform various NLP tasks.\n",
    "from transformers import pipeline\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "# Note: For public models, an access token is not needed.\n",
    "\n",
    "# 3. Prepare input text\n",
    "texts_to_analyze = [\n",
    "    \"I love playing and watching cricket.\",\n",
    "    \"I hate when Virat Kohli misses a century.\"\n",
    "]\n",
    "\n",
    "# 4. Perform sentiment analysis\n",
    "results = sentiment_analyzer(texts_to_analyze)\n",
    "\n",
    "# 5. Display the output\n",
    "# The output is a list of dictionaries, with each dictionary containing the sentiment 'label' and 'score' (confidence).\n",
    "for text, result in zip(texts_to_analyze, results):\n",
    "    print(f\"Text: \\\"{text}\\\"\")\n",
    "    print(f\"  Label: {result['label']}, Score: {result['score']:.4f}\\n\")\n",
    "\n",
    "# Output Explanation:\n",
    "# The 'score' is a confidence level (probability) between 0 and 1. Closer to 1 means higher confidence.\n",
    "# The 'label' indicates the predicted sentiment (e.g., 'positive', 'negative').\n",
    "# High scores (close to 1) often indicate strong, unambiguous language in the input text.\n",
    "```\n",
    "\n",
    "### 7. Text Classification\n",
    "\n",
    "Text classification categorises text into predefined classes, such as spam detection, news article classification, or intent detection.\n",
    "\n",
    "**Difference from Sentiment Analysis:**\n",
    "*   **Sentiment Analysis**: Narrow and specific to sentiment (positive, negative, neutral).\n",
    "*   **Text Classification**: Broader, labels depend on the specific task (e.g., spam/not spam, different topics like sports/technology).\n",
    "\n",
    "**Example: Detecting Spam (Text Classification)**\n",
    "\n",
    "```python\n",
    "# 1. Install required libraries\n",
    "!pip install transformers torch\n",
    "\n",
    "# 2. Import necessary modules and load a pre-trained spam detection model\n",
    "from transformers import pipeline\n",
    "# Using a model fine-tuned for sentiment analysis but adapted for spam detection.\n",
    "spam_classifier = pipeline(\"sentiment-analysis\", model=\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "# No access token is needed for this publicly available model.\n",
    "\n",
    "# 3. Prepare input text (list of strings to classify)\n",
    "texts_to_classify = [\n",
    "    \"Congratulations! You have won a 500 INR Amazon gift card! Click here to claim.\",\n",
    "    \"Hi Amit, Let's have a meeting tomorrow at 12 p.m.\",\n",
    "    \"Your Gmail account has been compromised. Click here to verify immediately.\"\n",
    "]\n",
    "\n",
    "# 4. Map labels (as the model is sentiment-based)\n",
    "# Negative sentiment can map to 'spam', neutral and positive to 'not spam'.\n",
    "label_mapping = {\n",
    "    \"NEGATIVE\": \"spam\",\n",
    "    \"NEUTRAL\": \"not spam\",\n",
    "    \"POSITIVE\": \"not spam\"\n",
    "}\n",
    "\n",
    "# 5. Perform spam detection and display results\n",
    "for text in texts_to_classify:\n",
    "    result = spam_classifier(text) # The pipeline returns a list, take the first element.\n",
    "    predicted_label = label_mapping.get(result['label'], \"unknown\") # Use .get() for safer access.\n",
    "    print(f\"Text: \\\"{text}\\\"\")\n",
    "    print(f\"  Predicted Label: {predicted_label}, Confidence Score: {result['score']:.4f}\\n\")\n",
    "\n",
    "# Output Explanation:\n",
    "# The output includes a 'label' (e.g., \"NEGATIVE\", \"POSITIVE\") and a 'score' (confidence).\n",
    "# Low confidence scores (e.g., < 0.7) indicate uncertainty in the model's prediction.\n",
    "# The model might be fine-tuned for general sentiment, so it might not be perfectly accurate for spam detection out-of-the-box.\n",
    "```\n",
    "\n",
    "### 8. Text Summarization\n",
    "\n",
    "Text summarization is used to condense long articles, documents, or research papers into shorter snippets or extract key points.\n",
    "\n",
    "**Example: Summarizing Text**\n",
    "\n",
    "```python\n",
    "# 1. Install required libraries\n",
    "!pip install transformers torch\n",
    "\n",
    "# 2. Import necessary modules and load model and tokenizer\n",
    "# AutoModelForSeq2SeqLM is used for sequence-to-sequence tasks like summarization.\n",
    "# AutoTokenizer for tokenizing text.\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load a pre-trained model for summarization (publicly available, no access token needed).\n",
    "model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Alternatively, use pipeline for simplicity:\n",
    "# summarizer = pipeline(\"summarization\", model=model_name)\n",
    "\n",
    "# 3. Set input text to summarize\n",
    "input_text = \"\"\"\n",
    "Hugging Face is a company and open-source community that focuses on Natural Language Processing and Artificial Intelligence.\n",
    "It is best known for its transformers library which provides tools and pre-trained models for a wide range of NLP tasks\n",
    "such as text classification, sentiment analysis, machine translation, and more. Hugging Face also includes a model hub\n",
    "that is a platform where users can share and download pre-trained models, datasets, and other resources.\n",
    "Additionally, it provides a library for a variety of datasets called the datasets library, and a platform for hosting\n",
    "and sharing machine learning demos and applications called Spaces. With Hugging Face, users can easily deploy and\n",
    "use models in production environments. The community is strong, with developers and AI enthusiasts contributing to the ecosystem.\n",
    "\"\"\"\n",
    "\n",
    "# 4. Tokenize the input text\n",
    "# return_tensors=\"pt\" ensures PyTorch tensors, max_length ensures truncation if needed.\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# 5. Generate the summary\n",
    "# Parameters like max_length, min_length, length_penalty, num_beams control summary length and quality.\n",
    "summary_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=150,  # Maximum number of tokens in the summary\n",
    "    min_length=30,   # Minimum number of tokens in the summary\n",
    "    length_penalty=2.0, # Encourages longer summaries (value > 1.0)\n",
    "    num_beams=4,     # Controls beam search width; higher values improve quality but slow down inference\n",
    "    early_stopping=True # Stop beam search when all beams have reached a certain stage.\n",
    ")\n",
    "\n",
    "# 6. Decode the generated tokens back to text and print the summary\n",
    "summary = tokenizer.decode(summary_ids, skip_special_tokens=True)\n",
    "print(f\"Original Text:\\n{input_text}\\n\")\n",
    "print(f\"Generated Summary:\\n{summary}\\n\")\n",
    "```\n",
    "\n",
    "### 9. Machine Translation (Text-to-Text Generation)\n",
    "\n",
    "Machine translation involves translating text from one language to another. This falls under **text-to-text generation**, which encompasses tasks where the model takes an input sequence and generates an output sequence, including summarization, paraphrasing, and question answering.\n",
    "\n",
    "**Difference from Text Generation:**\n",
    "*   **Text Generation**: Used for auto-regressive generation (one token at a time), e.g., dialogue systems.\n",
    "*   **Text-to-Text Generation**: Used for sequence-to-sequence tasks, taking an input sequence to generate an output sequence (e.g., translation, summarization).\n",
    "\n",
    "**Example: Translating English to Spanish**\n",
    "\n",
    "```python\n",
    "# 1. Install required libraries\n",
    "!pip install transformers torch\n",
    "\n",
    "# 2. Import necessary modules and load a pre-trained translation model\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "# Load the T5 model (a versatile text-to-text model).\n",
    "model_name = \"t5-small\" # A smaller version of T5 model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Alternatively, use pipeline for simplicity:\n",
    "# translator = pipeline(\"translation_en_to_es\", model=model_name)\n",
    "\n",
    "# 3. Prepare input text with a task-specific prefix\n",
    "# T5 models often require a task prefix like \"translate English to Spanish:\".\n",
    "input_text = \"translate English to Spanish: My name is Amit Diwan and I love cricket.\"\n",
    "\n",
    "# 4. Tokenize the input text\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# 5. Generate the translated text\n",
    "# max_length and num_beams can be customised.\n",
    "translated_ids = model.generate(input_ids, max_length=50, num_beams=4)\n",
    "\n",
    "# 6. Decode output tokens and print the translated text\n",
    "translated_text = tokenizer.decode(translated_ids, skip_special_tokens=True)\n",
    "print(f\"Original Text: {input_text}\\n\")\n",
    "print(f\"Translated Text: {translated_text}\\n\")\n",
    "```\n",
    "\n",
    "### 10. Question Answering\n",
    "\n",
    "Question answering involves finding the answer to a question within a given \"context\" (a paragraph or text).\n",
    "\n",
    "**Example: Performing Question Answering**\n",
    "\n",
    "```python\n",
    "# 1. Install required libraries\n",
    "!pip install transformers torch\n",
    "\n",
    "# 2. Import necessary modules and load a pre-trained QA model and tokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a publicly available QA model (no access token needed).\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "# 3. Prepare the context and question\n",
    "context = \"Amit Diwan is a software engineer based in Delhi. He works for a tech company.\"\n",
    "question = \"Where is Amit Diwan based?\"\n",
    "\n",
    "# 4. Get the model's prediction\n",
    "# The pipeline handles tokenization, model prediction, and answer extraction internally.\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "\n",
    "# 5. Display the answer\n",
    "print(f\"Context: {context}\\n\")\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"Confidence Score: {result['score']:.4f}\\n\")\n",
    "\n",
    "# Output Explanation:\n",
    "# The output directly provides the 'answer' extracted from the context and a 'score' indicating confidence.\n",
    "```\n",
    "\n",
    "### 11. Text to Image\n",
    "\n",
    "Text-to-image generation creates an image from a textual description. This often utilises diffusion models, which are a class of generative models. The Hugging Face **Diffusers library** is an open-source Python library focusing on diffusion models for generating images, audio, and other data types. **Stable Diffusion** is a popular latent diffusion model within the Diffusers library for high-quality image generation from text prompts.\n",
    "\n",
    "**Example: Generating an Image from Text**\n",
    "\n",
    "```python\n",
    "# 1. Install required libraries\n",
    "!pip install diffusers transformers torch accelerate\n",
    "# 'accelerate' is often needed for optimisations when running diffusers models.\n",
    "\n",
    "# 2. Import necessary modules and load the Stable Diffusion pipeline\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# Load a publicly available Stable Diffusion model (no access token needed).\n",
    "# Ensure you accept the model's terms if prompted, which might require logging in to Hugging Face Hub (but not an access token for public models).\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(model_id)\n",
    "\n",
    "# If running on Colab GPU, move pipeline to CUDA for faster inference\n",
    "# pipeline.to(\"cuda\")\n",
    "\n",
    "# 3. Define the text prompt\n",
    "prompt = \"Flying cars soar over a futuristic cityscape at sunset.\"\n",
    "\n",
    "# 4. Generate the image\n",
    "# The pipeline generates an image by passing the text prompt.\n",
    "image = pipeline(prompt).images\n",
    "\n",
    "# 5. Save and display the image\n",
    "image_path = \"generated_image.png\"\n",
    "image.save(image_path)\n",
    "print(f\"Image saved as {image_path}\")\n",
    "\n",
    "# To display the image in Colab (requires PIL/Pillow):\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=image_path))\n",
    "\n",
    "# The image will be saved in the Google Colab file system.\n",
    "```\n",
    "\n",
    "### 12. Text to Video\n",
    "\n",
    "Text-to-video synthesis involves generating a video from textual descriptions. This is a complex task that combines NLP models with generative models or diffusion models. Hugging Face provides models and tools for this, often leveraging the **Diffusers library**.\n",
    "\n",
    "**Example: Generating a Video from Text (Stitching Frames)**\n",
    "\n",
    "This example shows how to generate individual frames using a text-to-image model and then stitch them into a video using OpenCV.\n",
    "\n",
    "```python\n",
    "# 1. Install required libraries\n",
    "# OpenCV (cv2) and NumPy are needed for video stitching.\n",
    "!pip install diffusers transformers torch accelerate opencv-python numpy\n",
    "\n",
    "# 2. Import necessary modules and load a text-to-image model\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load a publicly available text-to-image model (Stable Diffusion).\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(model_id)\n",
    "\n",
    "# If running on Colab GPU, move pipeline to CUDA for faster inference\n",
    "# pipeline.to(\"cuda\")\n",
    "\n",
    "# 3. Define the text prompt\n",
    "prompt = \"A futuristic cityscape at night with flying cars.\"\n",
    "\n",
    "# 4. Generate individual frames based on the text description\n",
    "num_frames = 10\n",
    "frames = []\n",
    "for i in range(num_frames):\n",
    "    # Generating slightly different images for each frame based on the same prompt.\n",
    "    # In a real text-to-video model, the model itself would handle temporal consistency.\n",
    "    # Here, we're simulating by generating distinct images.\n",
    "    image = pipeline(prompt).images\n",
    "    frames.append(np.array(image)) # Convert PIL Image to NumPy array for OpenCV\n",
    "    image.save(f\"frame_{i}.png\") # Save individual frames\n",
    "    print(f\"Generated frame_{i}.png\")\n",
    "\n",
    "# 5. Stitch the frames into a video using OpenCV\n",
    "if frames:\n",
    "    height, width, layers = frames.shape\n",
    "    video_name = 'output_video.mp4'\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Codec for .mp4 files\n",
    "    video = cv2.VideoWriter(video_name, fourcc, 1, (width, height)) # 1 FPS for demonstration\n",
    "\n",
    "    for frame in frames:\n",
    "        video.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)) # Convert RGB to BGR for OpenCV\n",
    "\n",
    "    video.release() # Release the video writer object\n",
    "    print(f\"\\nVideo saved as {video_name}\")\n",
    "\n",
    "# You can download the generated frames (frame_0.png to frame_9.png) and output_video.mp4 from Colab's file browser.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebf7d8a",
   "metadata": {},
   "source": [
    "Here are detailed notes with code examples for various tasks using Hugging Face, drawing upon the provided YouTube source.\n",
    "\n",
    "***\n",
    "\n",
    "### 1. Introduction to Hugging Face and its Core Libraries\n",
    "\n",
    "Hugging Face is a company and open-source community focused on Natural Language Processing (NLP) and Artificial Intelligence (AI). It is best known for its **Transformers library**, which offers tools and pre-trained models for a wide range of NLP tasks. Beyond Transformers, Hugging Face also provides other widely used libraries, including **Datasets** and **Tokenizers**. It also features a **Model Hub** for sharing and downloading pre-trained models, datasets, and other resources, and **Spaces** for hosting and sharing machine learning demos and applications.\n",
    "\n",
    "**Key Libraries:**\n",
    "*   **Transformers Library**: The core library for pre-trained models and pipelines, providing access to thousands of pre-trained models for NLP tasks like translation, text summarization, and text classification. It is simple to use with complex NLP models, offers cutting-edge models, supports customization, and has a large, active community.\n",
    "*   **Datasets Library**: Provides easy access to a wide variety of datasets for NLP and other machine learning tasks. It enables efficient work with large datasets through lazy loading and streaming, offers a unified API for processing datasets, and integrates well with the Transformers library and other ML frameworks. Hugging Face provides over 350,000 datasets on its platform.\n",
    "*   **Tokenizers Library**: A fast, efficient, and flexible library designed for tokenizing text data, a crucial step in NLP. Tokenization involves splitting text into smaller units (words, subwords, characters) and converting them into numerical representations for ML models. It is optimised for fast tokenization, supports custom tokenizers, and integrates with other Hugging Face libraries like Transformers.\n",
    "\n",
    "### 2. Hugging Face Access Token\n",
    "\n",
    "An access token (also referred to as an API key) is a secure string of characters used to access Hugging Face services and resources.\n",
    "\n",
    "**When an Access Token is Needed:**\n",
    "*   When using a private or \"gated\" model (e.g., Meta's LLaMA) or an Inference API.\n",
    "*   When uploading models, datasets, or Spaces to the Hugging Face Hub.\n",
    "\n",
    "**When an Access Token is NOT Needed:**\n",
    "*   When accessing public models (e.g., GPT-2) which are freely available to download and use without authentication.\n",
    "*   When using models via the Transformers library, as many are publicly available and downloadable without an API key.\n",
    "\n",
    "To create an access token, you need to create an account on the official Hugging Face website (`huggingface.co/join`), then navigate to your profile, select \"Access Tokens,\" and create a new token. It is crucial **not to share** your access tokens with anyone.\n",
    "\n",
    "### 3. Installing Hugging Face Libraries on Google Colab\n",
    "\n",
    "Google Colab is a free web application that can be used to run Python notebooks. You can easily install Hugging Face libraries there.\n",
    "\n",
    "**General Installation Command:**\n",
    "The general command to install libraries using `pip` (a Python package manager) on Google Colab includes an exclamation mark (`!`) at the beginning.\n",
    "\n",
    "```python\n",
    "# General command to install a library on Google Colab\n",
    "!pip install <library_name>\n",
    "```\n",
    "\n",
    "**Specific Installation Commands:**\n",
    "*   **Transformers Library**: `!pip install transformers`\n",
    "*   **Datasets Library**: `!pip install datasets`\n",
    "*   **Tokenizers Library**: `!pip install tokenizers`\n",
    "*   **PyTorch (often required for models)**: `!pip install torch`\n",
    "*   **Diffusers Library (for text-to-image/video)**: `!pip install diffusers`\n",
    "\n",
    "You can change the runtime type on Google Colab (e.g., to T4 GPU or V2-8 TPU for complex projects) for better efficiency.\n",
    "\n",
    "### 4. Downloading a Dataset\n",
    "\n",
    "The `datasets` library allows easy access to a wide variety of datasets for machine learning.\n",
    "\n",
    "**Example: Downloading the IMDB Dataset**\n",
    "\n",
    "```python\n",
    "# 1. Install the datasets library (if not already installed)\n",
    "!pip install datasets\n",
    "\n",
    "# 2. Import the necessary function\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 3. Load the IMDB dataset\n",
    "# The load_dataset function downloads datasets from the Hugging Face Hub or loads from local files.\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# 4. Print the dataset to see its structure\n",
    "# This will display an overview, including the number of samples in each split (train and test).\n",
    "print(imdb_dataset)\n",
    "```\n",
    "\n",
    "**Output Explanation:**\n",
    "The output shows a `DatasetDict` structure, typically with 'train' and 'test' splits.\n",
    "*   **`train`**: Contains 25,000 rows with features like 'text' (movie reviews) and 'label' (sentiment: positive or negative).\n",
    "*   **`test`**: Contains 25,000 rows for testing, with the same features.\n",
    "*   The 'unsupervised' split (50,000 rows) is often used for pre-training or semi-supervised learning and typically lacks labels.\n",
    "\n",
    "### 5. Downloading a Model\n",
    "\n",
    "Models can be downloaded using the `transformers` library, specifically using the `from_pretrained` method.\n",
    "\n",
    "**Example: Downloading a pre-trained BERT Model**\n",
    "\n",
    "```python\n",
    "# 1. Install the transformers library (if not already installed)\n",
    "!pip install transformers\n",
    "\n",
    "# 2. Import necessary modules\n",
    "from transformers import AutoModel # Assuming AutoModel is used, similar to the source's implied usage for BERT.\n",
    "                                      # The source mentions from_pretrained for model weights, config, and tokenizer.\n",
    "                                      # For a full example, AutoTokenizer and AutoModel would typically be used together.\n",
    "\n",
    "# 3. Download a pre-trained BERT model\n",
    "# The from_pretrained method downloads the model weights, configuration, and tokenizer from the Hugging Face Hub.\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 4. (Optional) Pass an input and check output shape (as shown in source for BERT)\n",
    "# This requires a tokenizer first to convert text to input IDs.\n",
    "# from transformers import AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# inputs = tokenizer(\"hello hugging face\", return_tensors=\"pt\")\n",
    "# outputs = model(**inputs)\n",
    "# print(outputs.last_hidden_state.shape)\n",
    "\n",
    "# Expected output shape for BERT-base-uncased with \"hello hugging face\":\n",
    "# (1, 7, 768)\n",
    "# - 1: Batch size (one input sentence)\n",
    "# - 7: Sequence length (tokenized \"hello hugging face\" including special tokens)\n",
    "# - 768: Hidden size (each token is a 768-dimensional vector, standard for BERT's base architecture)\n",
    "```\n",
    "\n",
    "### 6. Sentiment Analysis\n",
    "\n",
    "Sentiment analysis determines the sentiment (positive, negative, or neutral) expressed in a piece of text.\n",
    "\n",
    "**Types of Sentiment Analysis:**\n",
    "*   **Polarity Detection**: Classifies sentiment as positive, negative, or neutral.\n",
    "    *   *Examples:* \"I love this product\" (positive), \"The service is terrible\" (negative), \"The package arrived on time\" (neutral).\n",
    "*   **Emotion Detection**: Identifies specific emotions like anger, joy, frustration, etc..\n",
    "    *   *Examples:* \"This is pathetic, so frustrating\" (anger), \"I'm thrilled about the results\" (joy).\n",
    "*   **Aspect-Based Sentiment Analysis**: Analyses sentiment towards specific aspects of a product or service.\n",
    "    *   *Example:* \"The food was great but the service was slow\" (positive for food, negative for service).\n",
    "*   **Intent Analysis**: Detects the user's intention, e.g., to purchase or complain.\n",
    "    *   *Example:* \"Where can I buy this product?\" (purchase intent).\n",
    "\n",
    "**Example: Performing Sentiment Analysis**\n",
    "\n",
    "```python\n",
    "# 1. Install required libraries\n",
    "!pip install transformers torch\n",
    "\n",
    "# 2. Import necessary modules and load the sentiment analysis pipeline\n",
    "# The pipeline function provides a simple way to perform various NLP tasks.\n",
    "from transformers import pipeline\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "# Note: For public models, an access token is not needed.\n",
    "\n",
    "# 3. Prepare input text\n",
    "texts_to_analyze = [\n",
    "    \"I love playing and watching cricket.\",\n",
    "    \"I hate when Virat Kohli misses a century.\"\n",
    "]\n",
    "\n",
    "# 4. Perform sentiment analysis\n",
    "results = sentiment_analyzer(texts_to_analyze)\n",
    "\n",
    "# 5. Display the output\n",
    "# The output is a list of dictionaries, with each dictionary containing the sentiment 'label' and 'score' (confidence).\n",
    "for text, result in zip(texts_to_analyze, results):\n",
    "    print(f\"Text: \\\"{text}\\\"\")\n",
    "    print(f\"  Label: {result['label']}, Score: {result['score']:.4f}\\n\")\n",
    "\n",
    "# Output Explanation:\n",
    "# The 'score' is a confidence level (probability) between 0 and 1. Closer to 1 means higher confidence.\n",
    "# The 'label' indicates the predicted sentiment (e.g., 'positive', 'negative').\n",
    "# High scores (close to 1) often indicate strong, unambiguous language in the input text.\n",
    "```\n",
    "\n",
    "### 7. Text Classification\n",
    "\n",
    "Text classification categorises text into predefined classes, such as spam detection, news article classification, or intent detection.\n",
    "\n",
    "**Difference from Sentiment Analysis:**\n",
    "*   **Sentiment Analysis**: Narrow and specific to sentiment (positive, negative, neutral).\n",
    "*   **Text Classification**: Broader, labels depend on the specific task (e.g., spam/not spam, different topics like sports/technology).\n",
    "\n",
    "**Example: Detecting Spam (Text Classification)**\n",
    "\n",
    "```python\n",
    "# 1. Install required libraries\n",
    "!pip install transformers torch\n",
    "\n",
    "# 2. Import necessary modules and load a pre-trained spam detection model\n",
    "from transformers import pipeline\n",
    "# Using a model fine-tuned for sentiment analysis but adapted for spam detection.\n",
    "spam_classifier = pipeline(\"sentiment-analysis\", model=\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "# No access token is needed for this publicly available model.\n",
    "\n",
    "# 3. Prepare input text (list of strings to classify)\n",
    "texts_to_classify = [\n",
    "    \"Congratulations! You have won a 500 INR Amazon gift card! Click here to claim.\",\n",
    "    \"Hi Amit, Let's have a meeting tomorrow at 12 p.m.\",\n",
    "    \"Your Gmail account has been compromised. Click here to verify immediately.\"\n",
    "]\n",
    "\n",
    "# 4. Map labels (as the model is sentiment-based)\n",
    "# Negative sentiment can map to 'spam', neutral and positive to 'not spam'.\n",
    "label_mapping = {\n",
    "    \"NEGATIVE\": \"spam\",\n",
    "    \"NEUTRAL\": \"not spam\",\n",
    "    \"POSITIVE\": \"not spam\"\n",
    "}\n",
    "\n",
    "# 5. Perform spam detection and display results\n",
    "for text in texts_to_classify:\n",
    "    result = spam_classifier(text) # The pipeline returns a list, take the first element.\n",
    "    predicted_label = label_mapping.get(result['label'], \"unknown\") # Use .get() for safer access.\n",
    "    print(f\"Text: \\\"{text}\\\"\")\n",
    "    print(f\"  Predicted Label: {predicted_label}, Confidence Score: {result['score']:.4f}\\n\")\n",
    "\n",
    "# Output Explanation:\n",
    "# The output includes a 'label' (e.g., \"NEGATIVE\", \"POSITIVE\") and a 'score' (confidence).\n",
    "# Low confidence scores (e.g., < 0.7) indicate uncertainty in the model's prediction.\n",
    "# The model might be fine-tuned for general sentiment, so it might not be perfectly accurate for spam detection out-of-the-box.\n",
    "```\n",
    "\n",
    "### 8. Text Summarization\n",
    "\n",
    "Text summarization is used to condense long articles, documents, or research papers into shorter snippets or extract key points.\n",
    "\n",
    "**Example: Summarizing Text**\n",
    "\n",
    "```python\n",
    "# 1. Install required libraries\n",
    "!pip install transformers torch\n",
    "\n",
    "# 2. Import necessary modules and load model and tokenizer\n",
    "# AutoModelForSeq2SeqLM is used for sequence-to-sequence tasks like summarization.\n",
    "# AutoTokenizer for tokenizing text.\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load a pre-trained model for summarization (publicly available, no access token needed).\n",
    "model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Alternatively, use pipeline for simplicity:\n",
    "# summarizer = pipeline(\"summarization\", model=model_name)\n",
    "\n",
    "# 3. Set input text to summarize\n",
    "input_text = \"\"\"\n",
    "Hugging Face is a company and open-source community that focuses on Natural Language Processing and Artificial Intelligence.\n",
    "It is best known for its transformers library which provides tools and pre-trained models for a wide range of NLP tasks\n",
    "such as text classification, sentiment analysis, machine translation, and more. Hugging Face also includes a model hub\n",
    "that is a platform where users can share and download pre-trained models, datasets, and other resources.\n",
    "Additionally, it provides a library for a variety of datasets called the datasets library, and a platform for hosting\n",
    "and sharing machine learning demos and applications called Spaces. With Hugging Face, users can easily deploy and\n",
    "use models in production environments. The community is strong, with developers and AI enthusiasts contributing to the ecosystem.\n",
    "\"\"\"\n",
    "\n",
    "# 4. Tokenize the input text\n",
    "# return_tensors=\"pt\" ensures PyTorch tensors, max_length ensures truncation if needed.\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# 5. Generate the summary\n",
    "# Parameters like max_length, min_length, length_penalty, num_beams control summary length and quality.\n",
    "summary_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=150,  # Maximum number of tokens in the summary\n",
    "    min_length=30,   # Minimum number of tokens in the summary\n",
    "    length_penalty=2.0, # Encourages longer summaries (value > 1.0)\n",
    "    num_beams=4,     # Controls beam search width; higher values improve quality but slow down inference\n",
    "    early_stopping=True # Stop beam search when all beams have reached a certain stage.\n",
    ")\n",
    "\n",
    "# 6. Decode the generated tokens back to text and print the summary\n",
    "summary = tokenizer.decode(summary_ids, skip_special_tokens=True)\n",
    "print(f\"Original Text:\\n{input_text}\\n\")\n",
    "print(f\"Generated Summary:\\n{summary}\\n\")\n",
    "```\n",
    "\n",
    "### 9. Machine Translation (Text-to-Text Generation)\n",
    "\n",
    "Machine translation involves translating text from one language to another. This falls under **text-to-text generation**, which encompasses tasks where the model takes an input sequence and generates an output sequence, including summarization, paraphrasing, and question answering.\n",
    "\n",
    "**Difference from Text Generation:**\n",
    "*   **Text Generation**: Used for auto-regressive generation (one token at a time), e.g., dialogue systems.\n",
    "*   **Text-to-Text Generation**: Used for sequence-to-sequence tasks, taking an input sequence to generate an output sequence (e.g., translation, summarization).\n",
    "\n",
    "**Example: Translating English to Spanish**\n",
    "\n",
    "```python\n",
    "# 1. Install required libraries\n",
    "!pip install transformers torch\n",
    "\n",
    "# 2. Import necessary modules and load a pre-trained translation model\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "# Load the T5 model (a versatile text-to-text model).\n",
    "model_name = \"t5-small\" # A smaller version of T5 model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Alternatively, use pipeline for simplicity:\n",
    "# translator = pipeline(\"translation_en_to_es\", model=model_name)\n",
    "\n",
    "# 3. Prepare input text with a task-specific prefix\n",
    "# T5 models often require a task prefix like \"translate English to Spanish:\".\n",
    "input_text = \"translate English to Spanish: My name is Amit Diwan and I love cricket.\"\n",
    "\n",
    "# 4. Tokenize the input text\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# 5. Generate the translated text\n",
    "# max_length and num_beams can be customised.\n",
    "translated_ids = model.generate(input_ids, max_length=50, num_beams=4)\n",
    "\n",
    "# 6. Decode output tokens and print the translated text\n",
    "translated_text = tokenizer.decode(translated_ids, skip_special_tokens=True)\n",
    "print(f\"Original Text: {input_text}\\n\")\n",
    "print(f\"Translated Text: {translated_text}\\n\")\n",
    "```\n",
    "\n",
    "### 10. Question Answering\n",
    "\n",
    "Question answering involves finding the answer to a question within a given \"context\" (a paragraph or text).\n",
    "\n",
    "**Example: Performing Question Answering**\n",
    "\n",
    "```python\n",
    "# 1. Install required libraries\n",
    "!pip install transformers torch\n",
    "\n",
    "# 2. Import necessary modules and load a pre-trained QA model and tokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a publicly available QA model (no access token needed).\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "# 3. Prepare the context and question\n",
    "context = \"Amit Diwan is a software engineer based in Delhi. He works for a tech company.\"\n",
    "question = \"Where is Amit Diwan based?\"\n",
    "\n",
    "# 4. Get the model's prediction\n",
    "# The pipeline handles tokenization, model prediction, and answer extraction internally.\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "\n",
    "# 5. Display the answer\n",
    "print(f\"Context: {context}\\n\")\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"Confidence Score: {result['score']:.4f}\\n\")\n",
    "\n",
    "# Output Explanation:\n",
    "# The output directly provides the 'answer' extracted from the context and a 'score' indicating confidence.\n",
    "```\n",
    "\n",
    "### 11. Text to Image\n",
    "\n",
    "Text-to-image generation creates an image from a textual description. This often utilises diffusion models, which are a class of generative models. The Hugging Face **Diffusers library** is an open-source Python library focusing on diffusion models for generating images, audio, and other data types. **Stable Diffusion** is a popular latent diffusion model within the Diffusers library for high-quality image generation from text prompts.\n",
    "\n",
    "**Example: Generating an Image from Text**\n",
    "\n",
    "```python\n",
    "# 1. Install required libraries\n",
    "!pip install diffusers transformers torch accelerate\n",
    "# 'accelerate' is often needed for optimisations when running diffusers models.\n",
    "\n",
    "# 2. Import necessary modules and load the Stable Diffusion pipeline\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# Load a publicly available Stable Diffusion model (no access token needed).\n",
    "# Ensure you accept the model's terms if prompted, which might require logging in to Hugging Face Hub (but not an access token for public models).\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(model_id)\n",
    "\n",
    "# If running on Colab GPU, move pipeline to CUDA for faster inference\n",
    "# pipeline.to(\"cuda\")\n",
    "\n",
    "# 3. Define the text prompt\n",
    "prompt = \"Flying cars soar over a futuristic cityscape at sunset.\"\n",
    "\n",
    "# 4. Generate the image\n",
    "# The pipeline generates an image by passing the text prompt.\n",
    "image = pipeline(prompt).images\n",
    "\n",
    "# 5. Save and display the image\n",
    "image_path = \"generated_image.png\"\n",
    "image.save(image_path)\n",
    "print(f\"Image saved as {image_path}\")\n",
    "\n",
    "# To display the image in Colab (requires PIL/Pillow):\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=image_path))\n",
    "\n",
    "# The image will be saved in the Google Colab file system.\n",
    "```\n",
    "\n",
    "### 12. Text to Video\n",
    "\n",
    "Text-to-video synthesis involves generating a video from textual descriptions. This is a complex task that combines NLP models with generative models or diffusion models. Hugging Face provides models and tools for this, often leveraging the **Diffusers library**.\n",
    "\n",
    "**Example: Generating a Video from Text (Stitching Frames)**\n",
    "\n",
    "This example shows how to generate individual frames using a text-to-image model and then stitch them into a video using OpenCV.\n",
    "\n",
    "```python\n",
    "# 1. Install required libraries\n",
    "# OpenCV (cv2) and NumPy are needed for video stitching.\n",
    "!pip install diffusers transformers torch accelerate opencv-python numpy\n",
    "\n",
    "# 2. Import necessary modules and load a text-to-image model\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load a publicly available text-to-image model (Stable Diffusion).\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(model_id)\n",
    "\n",
    "# If running on Colab GPU, move pipeline to CUDA for faster inference\n",
    "# pipeline.to(\"cuda\")\n",
    "\n",
    "# 3. Define the text prompt\n",
    "prompt = \"A futuristic cityscape at night with flying cars.\"\n",
    "\n",
    "# 4. Generate individual frames based on the text description\n",
    "num_frames = 10\n",
    "frames = []\n",
    "for i in range(num_frames):\n",
    "    # Generating slightly different images for each frame based on the same prompt.\n",
    "    # In a real text-to-video model, the model itself would handle temporal consistency.\n",
    "    # Here, we're simulating by generating distinct images.\n",
    "    image = pipeline(prompt).images\n",
    "    frames.append(np.array(image)) # Convert PIL Image to NumPy array for OpenCV\n",
    "    image.save(f\"frame_{i}.png\") # Save individual frames\n",
    "    print(f\"Generated frame_{i}.png\")\n",
    "\n",
    "# 5. Stitch the frames into a video using OpenCV\n",
    "if frames:\n",
    "    height, width, layers = frames.shape\n",
    "    video_name = 'output_video.mp4'\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Codec for .mp4 files\n",
    "    video = cv2.VideoWriter(video_name, fourcc, 1, (width, height)) # 1 FPS for demonstration\n",
    "\n",
    "    for frame in frames:\n",
    "        video.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)) # Convert RGB to BGR for OpenCV\n",
    "\n",
    "    video.release() # Release the video writer object\n",
    "    print(f\"\\nVideo saved as {video_name}\")\n",
    "\n",
    "# You can download the generated frames (frame_0.png to frame_9.png) and output_video.mp4 from Colab's file browser.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faabc706",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
