{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97787acc",
   "metadata": {},
   "source": [
    "\n",
    "# ğŸ“˜ **Hugging Face for GenAI â€” Full Teaching Modules**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceada4b5",
   "metadata": {},
   "source": [
    "## **Module 1: Introduction to Hugging Face**\n",
    "\n",
    "**Goal:** Understand the HF ecosystem, purpose, and core libraries.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* What is Hugging Face?\n",
    "* Evolution from transformers to full GenAI ecosystem\n",
    "* Libraries overview\n",
    "\n",
    "  * `transformers`\n",
    "  * `diffusers`\n",
    "  * `datasets`\n",
    "  * `tokenizers`\n",
    "  * `accelerate`\n",
    "  * `peft`\n",
    "  * `gradio` & `streamlit` (Spaces)\n",
    "\n",
    "### **Demo**\n",
    "\n",
    "* Visit: [https://huggingface.co](https://huggingface.co)\n",
    "* Show trending models & community spaces.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61e196a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cd724a6",
   "metadata": {},
   "source": [
    "# â­ **Module 1: Introduction to Hugging Face**\n",
    "\n",
    "**Goal:** Understand what Hugging Face is, why it is important, and the components of its GenAI ecosystem.\n",
    "\n",
    "---\n",
    "\n",
    "# 1. **What is Hugging Face? (Definition)**\n",
    "\n",
    "**Hugging Face is an open-source AI company and community platform that provides tools, models, datasets, and libraries to build modern Machine Learning and Generative AI applications.**\n",
    "\n",
    "### Key Points:\n",
    "\n",
    "* Started as a chatbot startup â†’ evolved into the **worldâ€™s largest open-source AI hub**.\n",
    "* Famous for the **Transformers** library (NLP â†’ CV â†’ Audio â†’ Multimodal).\n",
    "* Community-driven with thousands of contributors.\n",
    "* Over **500,000+ models**, **100,000+ datasets**, **100,000+ Spaces (apps)**.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. **Why Hugging Face is Important (Purpose)**\n",
    "\n",
    "### âœ” Makes AI accessible (easy APIs, powerful models)\n",
    "\n",
    "### âœ” Supports open-source, transparent research\n",
    "\n",
    "### âœ” Standardizes model sharing\n",
    "\n",
    "### âœ” Enables reproducibility\n",
    "\n",
    "### âœ” Reduces compute cost using PEFT, Accelerate, etc.\n",
    "\n",
    "### âœ” One platform for Text, Image, Audio, Video, Multimodal AI\n",
    "\n",
    "---\n",
    "\n",
    "# 3. **Hugging Face Ecosystem Overview**\n",
    "\n",
    "The ecosystem has **5 main components**:\n",
    "\n",
    "```\n",
    " â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    " |                 Hugging Face Ecosystem             |\n",
    " â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    " | 1. Hub (Models, Datasets, Spaces)                  |\n",
    " | 2. Libraries (Transformers, Diffusers, Datasets)   |\n",
    " | 3. Tools (Tokenizers, Evaluate, Accelerate, PEFT)  |\n",
    " | 4. Inference (Inference API, Endpoints, TGI)       |\n",
    " | 5. Deployment (Spaces â€“ Gradio, Streamlit)         |\n",
    " â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "Letâ€™s understand each briefly.\n",
    "\n",
    "---\n",
    "\n",
    "# 4. **Main Hugging Face Components (Definitions + Explanation)**\n",
    "\n",
    "## **4.1 Hugging Face Hub (Definition)**\n",
    "\n",
    "A central platform where developers store, share, and explore:\n",
    "\n",
    "* **Models**\n",
    "* **Datasets**\n",
    "* **Applications (Spaces)**\n",
    "\n",
    "Think of it like **GitHub for AI**.\n",
    "\n",
    "---\n",
    "\n",
    "## **4.2 Transformers Library (Definition)**\n",
    "\n",
    "**Transformers is Hugging Faceâ€™s main library that provides state-of-the-art pretrained models for NLP, CV, Audio, and Multimodal tasks.**\n",
    "\n",
    "### Features:\n",
    "\n",
    "* 100+ architectures (BERT, GPT, T5, ViT, CLIP, Whisper, etc.)\n",
    "* One-line inference using Pipelines\n",
    "* AutoModel classes for quick loading\n",
    "* Supports TensorFlow, PyTorch, and JAX\n",
    "\n",
    "### Example:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"Hugging Face makes AI simple!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4.3 Diffusers Library (Definition)**\n",
    "\n",
    "A library for **image, video, and audio generation** using **diffusion models**.\n",
    "\n",
    "Examples:\n",
    "\n",
    "* Stable Diffusion\n",
    "* Kandinsky\n",
    "* ControlNet\n",
    "* AudioDiffusion\n",
    "\n",
    "### Example:\n",
    "\n",
    "```python\n",
    "from diffusers import DiffusionPipeline\n",
    "pipe = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
    "img = pipe(\"A robot teaching AI\").images[0]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4.4 Datasets Library (Definition)**\n",
    "\n",
    "A unified library for **loading, preprocessing, and sharing** datasets for ML and GenAI.\n",
    "\n",
    "### Features:\n",
    "\n",
    "* Streaming large datasets\n",
    "* Built-in preprocessing\n",
    "* Load with one line\n",
    "\n",
    "### Example:\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"imdb\")\n",
    "print(ds[\"train\"][0])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4.5 Tokenizers (Definition)**\n",
    "\n",
    "Library for **fast, hardware-optimized tokenization** built in **Rust**.\n",
    "\n",
    "### Common types:\n",
    "\n",
    "* BPE (GPT models)\n",
    "* WordPiece (BERT)\n",
    "* SentencePiece (T5)\n",
    "* Unigram\n",
    "\n",
    "### Example:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tok(\"Hello world!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4.6 Accelerate (Definition)**\n",
    "\n",
    "A library that makes **multi-GPU, mixed precision, and distributed training** simple.\n",
    "\n",
    "Purpose:\n",
    "â— â€œTrain faster with fewer lines of code.â€\n",
    "\n",
    "---\n",
    "\n",
    "## **4.7 PEFT â€“ Parameter Efficient Fine Tuning (Definition)**\n",
    "\n",
    "Techniques like:\n",
    "\n",
    "* LoRA\n",
    "* Prefix Tuning\n",
    "* QLoRA\n",
    "\n",
    "These allow **fine-tuning large models using 10â€“100x fewer parameters**.\n",
    "\n",
    "---\n",
    "\n",
    "## **4.8 Spaces (Definition)**\n",
    "\n",
    "A platform to **deploy AI apps** using:\n",
    "\n",
    "* **Gradio**\n",
    "* **Streamlit**\n",
    "\n",
    "Great for demos, hackathons, and sharing projects.\n",
    "\n",
    "### Example (Gradio):\n",
    "\n",
    "```python\n",
    "import gradio as gr\n",
    "\n",
    "def greet(name):\n",
    "    return f\"Hello {name}\"\n",
    "\n",
    "gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\").launch()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 5. **History of Hugging Face**\n",
    "\n",
    "### Timeline:\n",
    "\n",
    "* **2016**: Launched as a chatbot company\n",
    "* **2018**: Released Transformers library â†’ huge adoption\n",
    "* **2020**: Released Datasets & Tokenizers\n",
    "* **2021**: Introduced Spaces\n",
    "* **2022**: Diffusers for image generation\n",
    "* **2023â€“2025**: Expanded into open LLMs, inference solutions, serverless, and multimodal AI\n",
    "\n",
    "Hugging Face = **OpenAI + GitHub + Model Zoo + AI deployment platform combined.**\n",
    "\n",
    "---\n",
    "\n",
    "# 6. **Why Hugging Face Became Popular?**\n",
    "\n",
    "### âœ” Open source & community-driven\n",
    "\n",
    "### âœ” Easy-to-use APIs\n",
    "\n",
    "### âœ” Support for SOTA models (GPT, T5, Stable Diffusion, etc.)\n",
    "\n",
    "### âœ” Standardization across industry\n",
    "\n",
    "### âœ” Cross-framework support (PyTorch, TensorFlow, JAX)\n",
    "\n",
    "### âœ” Ready-to-run hosted inference\n",
    "\n",
    "---\n",
    "\n",
    "# 7. **Use Cases of Hugging Face**\n",
    "\n",
    "### ğŸ”¹ NLP\n",
    "\n",
    "* Sentiment analysis\n",
    "* Text generation\n",
    "* Summarization\n",
    "* Translation\n",
    "\n",
    "### ğŸ”¹ Computer Vision\n",
    "\n",
    "* Image generation\n",
    "* Object detection\n",
    "* Image captioning\n",
    "\n",
    "### ğŸ”¹ Audio & Speech\n",
    "\n",
    "* Speech-to-text (Whisper)\n",
    "* Text-to-speech\n",
    "\n",
    "### ğŸ”¹ Multimodal\n",
    "\n",
    "* CLIP-based search\n",
    "* Q&A on PDFs/images\n",
    "\n",
    "### ğŸ”¹ Enterprise\n",
    "\n",
    "* Deploy models at scale\n",
    "* Customized LLM solutions\n",
    "* Cost-efficient fine-tuning (QLoRA)\n",
    "\n",
    "---\n",
    "\n",
    "# 8. **Example: A Simple Sentiment Classifier**\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "sentiment = pipeline(\"sentiment-analysis\")\n",
    "print(sentiment(\"Hugging Face is amazing!\"))\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "[{'label': 'POSITIVE', 'score': 0.99}]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 9. **Diagram â€” Hugging Face Workflow**\n",
    "\n",
    "```\n",
    "       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "       â”‚ Hugging Face  â”‚\n",
    "       â”‚     Hub       â”‚\n",
    "       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚   Download Model      â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "               â”‚\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚  Load with Transformersâ”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "               â”‚\n",
    "      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "      â”‚  Inference / App â”‚\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 10. **Module 1 Summary**\n",
    "\n",
    "### You learned:\n",
    "\n",
    "âœ” What Hugging Face is\n",
    "âœ” Why it is important in GenAI\n",
    "âœ” Hugging Face ecosystem & workflows\n",
    "âœ” Key libraries (Transformers, Diffusers, Datasets)\n",
    "âœ” Supporting tools (Accelerate, PEFT, Tokenizers)\n",
    "âœ” What Spaces are\n",
    "âœ” Basic examples\n",
    "\n",
    "---\n",
    "\n",
    "# 11. **Learning Outcomes (Skills After Module 1)**\n",
    "\n",
    "After completing Module 1, the learner can:\n",
    "\n",
    "ğŸ¯ Explain Hugging Face ecosystem and purpose\n",
    "ğŸ¯ Identify major libraries and tools\n",
    "ğŸ¯ Understand how models/datasets/apps are organized\n",
    "ğŸ¯ Run a basic pipeline example\n",
    "ğŸ¯ Navigate the Hugging Face Hub confidently\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe4abb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260a206b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98caedf5",
   "metadata": {},
   "source": [
    "\n",
    "### Example:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"Hugging Face makes AI simple!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4.3 Diffusers Library (Definition)**\n",
    "\n",
    "A library for **image, video, and audio generation** using **diffusion models**.\n",
    "\n",
    "Examples:\n",
    "\n",
    "* Stable Diffusion\n",
    "* Kandinsky\n",
    "* ControlNet\n",
    "* AudioDiffusion\n",
    "\n",
    "### Example:\n",
    "\n",
    "```python\n",
    "from diffusers import DiffusionPipeline\n",
    "pipe = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
    "img = pipe(\"A robot teaching AI\").images[0]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4.4 Datasets Library (Definition)**\n",
    "\n",
    "A unified library for **loading, preprocessing, and sharing** datasets for ML and GenAI.\n",
    "\n",
    "### Features:\n",
    "\n",
    "* Streaming large datasets\n",
    "* Built-in preprocessing\n",
    "* Load with one line\n",
    "\n",
    "### Example:\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"imdb\")\n",
    "print(ds[\"train\"][0])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4.5 Tokenizers (Definition)**\n",
    "\n",
    "Library for **fast, hardware-optimized tokenization** built in **Rust**.\n",
    "\n",
    "### Common types:\n",
    "\n",
    "* BPE (GPT models)\n",
    "* WordPiece (BERT)\n",
    "* SentencePiece (T5)\n",
    "* Unigram\n",
    "\n",
    "### Example:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tok(\"Hello world!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4.6 Accelerate (Definition)**\n",
    "\n",
    "A library that makes **multi-GPU, mixed precision, and distributed training** simple.\n",
    "\n",
    "Purpose:\n",
    "â— â€œTrain faster with fewer lines of code.â€\n",
    "\n",
    "---\n",
    "\n",
    "## **4.7 PEFT â€“ Parameter Efficient Fine Tuning (Definition)**\n",
    "\n",
    "Techniques like:\n",
    "\n",
    "* LoRA\n",
    "* Prefix Tuning\n",
    "* QLoRA\n",
    "\n",
    "These allow **fine-tuning large models using 10â€“100x fewer parameters**.\n",
    "\n",
    "---\n",
    "\n",
    "## **4.8 Spaces (Definition)**\n",
    "\n",
    "A platform to **deploy AI apps** using:\n",
    "\n",
    "* **Gradio**\n",
    "* **Streamlit**\n",
    "\n",
    "Great for demos, hackathons, and sharing projects.\n",
    "\n",
    "### Example (Gradio):\n",
    "\n",
    "```python\n",
    "import gradio as gr\n",
    "\n",
    "def greet(name):\n",
    "    return f\"Hello {name}\"\n",
    "\n",
    "gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\").launch()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 5. **History of Hugging Face**\n",
    "\n",
    "### Timeline:\n",
    "\n",
    "* **2016**: Launched as a chatbot company\n",
    "* **2018**: Released Transformers library â†’ huge adoption\n",
    "* **2020**: Released Datasets & Tokenizers\n",
    "* **2021**: Introduced Spaces\n",
    "* **2022**: Diffusers for image generation\n",
    "* **2023â€“2025**: Expanded into open LLMs, inference solutions, serverless, and multimodal AI\n",
    "\n",
    "Hugging Face = **OpenAI + GitHub + Model Zoo + AI deployment platform combined.**\n",
    "\n",
    "---\n",
    "\n",
    "# 6. **Why Hugging Face Became Popular?**\n",
    "\n",
    "### âœ” Open source & community-driven\n",
    "\n",
    "### âœ” Easy-to-use APIs\n",
    "\n",
    "### âœ” Support for SOTA models (GPT, T5, Stable Diffusion, etc.)\n",
    "\n",
    "### âœ” Standardization across industry\n",
    "\n",
    "### âœ” Cross-framework support (PyTorch, TensorFlow, JAX)\n",
    "\n",
    "### âœ” Ready-to-run hosted inference\n",
    "\n",
    "---\n",
    "\n",
    "# 7. **Use Cases of Hugging Face**\n",
    "\n",
    "### ğŸ”¹ NLP\n",
    "\n",
    "* Sentiment analysis\n",
    "* Text generation\n",
    "* Summarization\n",
    "* Translation\n",
    "\n",
    "### ğŸ”¹ Computer Vision\n",
    "\n",
    "* Image generation\n",
    "* Object detection\n",
    "* Image captioning\n",
    "\n",
    "### ğŸ”¹ Audio & Speech\n",
    "\n",
    "* Speech-to-text (Whisper)\n",
    "* Text-to-speech\n",
    "\n",
    "### ğŸ”¹ Multimodal\n",
    "\n",
    "* CLIP-based search\n",
    "* Q&A on PDFs/images\n",
    "\n",
    "### ğŸ”¹ Enterprise\n",
    "\n",
    "* Deploy models at scale\n",
    "* Customized LLM solutions\n",
    "* Cost-efficient fine-tuning (QLoRA)\n",
    "\n",
    "---\n",
    "\n",
    "# 8. **Example: A Simple Sentiment Classifier**\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "sentiment = pipeline(\"sentiment-analysis\")\n",
    "print(sentiment(\"Hugging Face is amazing!\"))\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "[{'label': 'POSITIVE', 'score': 0.99}]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 9. **Diagram â€” Hugging Face Workflow**\n",
    "\n",
    "```\n",
    "       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "       â”‚ Hugging Face  â”‚\n",
    "       â”‚     Hub       â”‚\n",
    "       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚   Download Model      â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "               â”‚\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚  Load with Transformersâ”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "               â”‚\n",
    "      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "      â”‚  Inference / App â”‚\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 10. **Module 1 Summary**\n",
    "\n",
    "### You learned:\n",
    "\n",
    "âœ” What Hugging Face is\n",
    "âœ” Why it is important in GenAI\n",
    "âœ” Hugging Face ecosystem & workflows\n",
    "âœ” Key libraries (Transformers, Diffusers, Datasets)\n",
    "âœ” Supporting tools (Accelerate, PEFT, Tokenizers)\n",
    "âœ” What Spaces are\n",
    "âœ” Basic examples\n",
    "\n",
    "---\n",
    "\n",
    "# 11. **Learning Outcomes (Skills After Module 1)**\n",
    "\n",
    "After completing Module 1, the learner can:\n",
    "\n",
    "ğŸ¯ Explain Hugging Face ecosystem and purpose\n",
    "ğŸ¯ Identify major libraries and tools\n",
    "ğŸ¯ Understand how models/datasets/apps are organized\n",
    "ğŸ¯ Run a basic pipeline example\n",
    "ğŸ¯ Navigate the Hugging Face Hub confidently\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ba62c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84649ee5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53f0321b",
   "metadata": {},
   "source": [
    "# ğŸŒŸ **MODULE 5 â€“ INFERENCE WITH TRANSFORMERS**\n",
    "\n",
    "### *(Beginner-Friendly + Technical Detailed Notes)*\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£ **What is Inference?**\n",
    "\n",
    "### â­ Simple Definition (Non-Technical):\n",
    "\n",
    "**Inference means using a trained AI model to get predictions or outputs.**\n",
    "You are *not* training the model; you are only *using* it.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Input: \"I love AI\"\n",
    "Output: \"Positive\"\n",
    "```\n",
    "\n",
    "### â­ Technical Definition:\n",
    "\n",
    "Inference is the forward pass of the model:\n",
    "\n",
    "```\n",
    "Tokens â†’ Model â†’ Output logits â†’ Final prediction\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 2ï¸âƒ£ **How Inference Works (Simple Explanation)**\n",
    "\n",
    "```\n",
    "Text/Image/Audio\n",
    "        â†“\n",
    "   Tokenizer\n",
    "        â†“\n",
    "Transformer Model\n",
    "        â†“\n",
    "   Prediction\n",
    "```\n",
    "\n",
    "### Beginners think of it as:\n",
    "\n",
    "ğŸ‘‰ Asking the model a question and getting an answer.\n",
    "\n",
    "---\n",
    "\n",
    "# 3ï¸âƒ£ **Types of Inference Tasks**\n",
    "\n",
    "Transformers library supports **many tasks**.\n",
    "\n",
    "| Category   | Task                 | Example              |\n",
    "| ---------- | -------------------- | -------------------- |\n",
    "| NLP        | Sentiment Analysis   | \"Good\" / \"Bad\"       |\n",
    "| NLP        | Text Generation      | Chatbots             |\n",
    "| NLP        | Question Answering   | â€œWho invented AI?â€   |\n",
    "| NLP        | Summarization        | Shorter versions     |\n",
    "| NLP        | Translation          | English â†’ Hindi      |\n",
    "| Vision     | Image Classification | Cat/Dog              |\n",
    "| Vision     | Object Detection     | Bounding boxes       |\n",
    "| Audio      | Speech Recognition   | Voice â†’ Text         |\n",
    "| Multimodal | Image Captioning     | â€œA dog playing ballâ€ |\n",
    "\n",
    "You will learn how to run each using simple pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "# 4ï¸âƒ£ **The Easiest Method: `pipeline()`**\n",
    "\n",
    "### â­ Non-Technical:\n",
    "\n",
    "Pipeline = **ready-made tool**\n",
    "You only tell the task.\n",
    "\n",
    "### â­ Technical:\n",
    "\n",
    "`pipeline(task_name)` loads:\n",
    "\n",
    "* Best default model\n",
    "* Tokenizer\n",
    "* Preprocessing\n",
    "* Postprocessing\n",
    "\n",
    "---\n",
    "\n",
    "# 5ï¸âƒ£ **Sentiment Analysis (Beginner + Code)**\n",
    "\n",
    "### â­ Beginner:\n",
    "\n",
    "It checks if text is positive or negative.\n",
    "\n",
    "### â­ Code:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "sentiment = pipeline(\"sentiment-analysis\")\n",
    "sentiment(\"I love learning AI!\")\n",
    "```\n",
    "\n",
    "### Example Output:\n",
    "\n",
    "```\n",
    "[{'label': 'POSITIVE', 'score': 0.999}]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 6ï¸âƒ£ **Text Generation**\n",
    "\n",
    "### â­ Beginner:\n",
    "\n",
    "The model **writes text** based on what you give.\n",
    "\n",
    "### â­ Code:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "generator(\"AI will change the world because\", max_length=40)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 7ï¸âƒ£ **Question Answering (QA)**\n",
    "\n",
    "### â­ Beginner:\n",
    "\n",
    "Ask a question â†’ Model gives the answer from a paragraph.\n",
    "\n",
    "### â­ Code:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "qa = pipeline(\"question-answering\")\n",
    "\n",
    "qa({\n",
    "    \"context\": \"Hugging Face is an AI company creating Transformers library.\",\n",
    "    \"question\": \"What does Hugging Face create?\"\n",
    "})\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "{'answer': 'Transformers library'}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 8ï¸âƒ£ **Summarization**\n",
    "\n",
    "### â­ Beginner:\n",
    "\n",
    "Makes long text shorter.\n",
    "\n",
    "### â­ Code:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "summarizer(\"AI is transforming multiple industries including health, finance, education...\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 9ï¸âƒ£ **Translation**\n",
    "\n",
    "### â­ Beginner:\n",
    "\n",
    "Convert one language to another.\n",
    "\n",
    "### â­ Code:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "translator = pipeline(\"translation_en_to_hi\")\n",
    "\n",
    "translator(\"Artificial Intelligence is the future.\")\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "\"à¤•à¥ƒà¤¤à¥à¤°à¤¿à¤® à¤¬à¥à¤¦à¥à¤§à¤¿à¤®à¤¤à¥à¤¤à¤¾ à¤­à¤µà¤¿à¤·à¥à¤¯ à¤¹à¥ˆà¥¤\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ”Ÿ **Named Entity Recognition (NER)**\n",
    "\n",
    "### â­ Beginner:\n",
    "\n",
    "Finds names, places, dates in text.\n",
    "\n",
    "### â­ Code:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Rahul and I live in Delhi.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£1ï¸âƒ£ **Image Classification (Vision)**\n",
    "\n",
    "### â­ Beginner:\n",
    "\n",
    "Model identifies whatâ€™s in an image.\n",
    "\n",
    "### â­ Code:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"image-classification\")\n",
    "\n",
    "classifier(\"cat.png\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£2ï¸âƒ£ **Object Detection (Vision)**\n",
    "\n",
    "### â­ Beginner:\n",
    "\n",
    "Model draws boxes around objects.\n",
    "\n",
    "### â­ Code:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "detector = pipeline(\"object-detection\")\n",
    "\n",
    "detector(\"street.jpg\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£3ï¸âƒ£ **Speech-to-Text (Audio ASR)**\n",
    "\n",
    "### â­ Beginner:\n",
    "\n",
    "Convert your voice to text.\n",
    "\n",
    "### â­ Code:\n",
    "\n",
    "```python\n",
    "asr = pipeline(\"automatic-speech-recognition\")\n",
    "asr(\"speech.wav\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£4ï¸âƒ£ **Image Captioning (Multimodal)**\n",
    "\n",
    "### â­ Beginner:\n",
    "\n",
    "Model describes what is in the image.\n",
    "\n",
    "### â­ Code:\n",
    "\n",
    "```python\n",
    "captioner = pipeline(\"image-to-text\")\n",
    "captioner(\"dog.jpg\")\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "\"A cute dog playing in the garden.\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£5ï¸âƒ£ **Using Models with Auto Classes (Technical Users)**\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "inputs = tokenizer(\"I love AI!\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£6ï¸âƒ£ **Batch Inference (Technical)**\n",
    "\n",
    "```python\n",
    "texts = [\"I like AI\", \"I hate bugs\"]\n",
    "\n",
    "pipeline(\"sentiment-analysis\")(texts)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£7ï¸âƒ£ **GPU Inference (Technical)**\n",
    "\n",
    "```python\n",
    "pipe = pipeline(\"text-generation\", model=\"gpt2\", device=0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£8ï¸âƒ£ **Optimizing Inference**\n",
    "\n",
    "### Methods:\n",
    "\n",
    "* Use smaller models: DistilBERT, MobileBERT\n",
    "* Use quantization (int8, int4)\n",
    "* Use ONNX Runtime\n",
    "* Use batch inference\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£9ï¸âƒ£ **Troubleshooting (For Students)**\n",
    "\n",
    "| Issue          | Cause            | Solution                     |\n",
    "| -------------- | ---------------- | ---------------------------- |\n",
    "| Slow model     | Large model      | Use smaller model            |\n",
    "| CUDA error     | Wrong GPU config | Install correct PyTorch      |\n",
    "| Token mismatch | Wrong tokenizer  | Use matching model-tokenizer |\n",
    "| Length error   | Long text        | Use `truncation=True`        |\n",
    "\n",
    "---\n",
    "\n",
    "# 2ï¸âƒ£0ï¸âƒ£ **Understanding Output Format**\n",
    "\n",
    "### Most tasks return:\n",
    "\n",
    "* `label` â†’ predicted class\n",
    "* `score` â†’ confidence\n",
    "* `start`, `end` â†’ QA spans\n",
    "* `boxes` â†’ object detection bounding boxes\n",
    "\n",
    "---\n",
    "\n",
    "# 2ï¸âƒ£1ï¸âƒ£ **Visual Diagram â€“ Inference Flow**\n",
    "\n",
    "```\n",
    "Input Text/Image\n",
    "        â†“\n",
    "Tokenizer / Preprocessor\n",
    "        â†“\n",
    "Transformer Model\n",
    "        â†“\n",
    "Prediction (Probabilities or Text)\n",
    "        â†“\n",
    "Readable Output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 2ï¸âƒ£2ï¸âƒ£ **Real-World Use Cases of Inference**\n",
    "\n",
    "### NLP:\n",
    "\n",
    "âœ” Chatbots\n",
    "âœ” Document summarization\n",
    "âœ” Customer sentiment\n",
    "âœ” Email auto-response\n",
    "\n",
    "### Vision:\n",
    "\n",
    "âœ” Security cameras\n",
    "âœ” Defect detection\n",
    "âœ” Product categorization\n",
    "\n",
    "### Audio:\n",
    "\n",
    "âœ” Voice assistants\n",
    "âœ” Meeting transcription\n",
    "\n",
    "---\n",
    "\n",
    "# 2ï¸âƒ£3ï¸âƒ£ **Learning Outcomes (Module 5)**\n",
    "\n",
    "After this module, students can:\n",
    "\n",
    "### â­ Beginner:\n",
    "\n",
    "âœ” Use pipeline() for different tasks\n",
    "âœ” Run sentiment analysis, translation, QA\n",
    "âœ” Use models without coding (online demos)\n",
    "\n",
    "### â­ Technical:\n",
    "\n",
    "âœ” Use AutoTokenizer + AutoModel\n",
    "âœ” Run batch/GPU inference\n",
    "âœ” Understand probability outputs\n",
    "âœ” Optimize inference speed\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32889b7d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3967273b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d6fea69",
   "metadata": {},
   "source": [
    "# ğŸŒŸ **MODULE 6 â€“ FINE-TUNING (TRAINER API + CUSTOM DATASETS)**\n",
    "\n",
    "### *(Beginner-Friendly + Technical Detailed Notes)*\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£ **What is Fine-Tuning?**\n",
    "\n",
    "### â­ Simple Definition (Non-Technical):\n",
    "\n",
    "Fine-tuning means **teaching a pre-trained AI model a new skill** using your own data.\n",
    "\n",
    "Example:\n",
    "You take a general model (like BERT trained on Wikipedia)\n",
    "and teach it:\n",
    "\n",
    "* To classify movie reviews\n",
    "* To detect spam\n",
    "* To answer domain-specific questions\n",
    "\n",
    "### â­ Technical Definition:\n",
    "\n",
    "Fine-tuning = updating **only part (or all)** of the model weights on a labeled dataset using supervised learning.\n",
    "\n",
    "---\n",
    "\n",
    "# 2ï¸âƒ£ **Why Do We Fine-Tune?**\n",
    "\n",
    "### â­ Non-Technical:\n",
    "\n",
    "* Saves time (model already knows language)\n",
    "* Needs less data\n",
    "* Cheaper than training from scratch\n",
    "* Produces accurate results\n",
    "\n",
    "### â­ Technical:\n",
    "\n",
    "* Improves model performance on downstream tasks\n",
    "* Requires small datasets (1kâ€“50k)\n",
    "* Backpropagation updates parameters\n",
    "* Works for text, images, audio\n",
    "\n",
    "---\n",
    "\n",
    "# 3ï¸âƒ£ **Fine-Tuning Workflow (Beginner Diagram)**\n",
    "\n",
    "```\n",
    "      Pretrained Model (BERT, DistilBERT, GPT)\n",
    "                        â†“\n",
    "            Add small labeled dataset\n",
    "                        â†“\n",
    "                Train (few minutes)\n",
    "                        â†“\n",
    "           Model learns your specific task\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 4ï¸âƒ£ **Hugging Face Tools for Fine-Tuning**\n",
    "\n",
    "```\n",
    "Transformers â†’ Models + Tokenizers\n",
    "Datasets     â†’ Load your dataset\n",
    "Trainer API  â†’ Training loop\n",
    "PEFT         â†’ Efficient training (LoRA)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 5ï¸âƒ£ **Key Concepts Before Fine-Tuning**\n",
    "\n",
    "| Concept           | Beginner-Friendly Meaning            |\n",
    "| ----------------- | ------------------------------------ |\n",
    "| **Epoch**         | One full pass over the dataset       |\n",
    "| **Batch Size**    | Number of samples processed together |\n",
    "| **Learning Rate** | How fast the model learns            |\n",
    "| **Loss**          | Model mistake level (lower = better) |\n",
    "| **Evaluation**    | Checking model accuracy              |\n",
    "| **Metrics**       | Accuracy, F1, Precision, Recall      |\n",
    "\n",
    "---\n",
    "\n",
    "# 6ï¸âƒ£ **Trainer API â€” Easiest Way to Fine-Tune**\n",
    "\n",
    "### â­ Non-Technical Explanation:\n",
    "\n",
    "Trainer is a **ready-made training engine** that trains models for you.\n",
    "\n",
    "### â­ Technical Explanation:\n",
    "\n",
    "Trainer manages:\n",
    "\n",
    "* Data loaders\n",
    "* Optimizers\n",
    "* Schedulers\n",
    "* Logging\n",
    "* Mixed precision (fp16)\n",
    "* Evaluation loops\n",
    "* Saving checkpoints\n",
    "\n",
    "---\n",
    "\n",
    "# 7ï¸âƒ£ **Steps for Fine-Tuning Using Trainer API**\n",
    "\n",
    "```\n",
    "1. Load dataset\n",
    "2. Load tokenizer\n",
    "3. Tokenize dataset\n",
    "4. Load pretrained model\n",
    "5. Define training settings\n",
    "6. Train with Trainer()\n",
    "7. Evaluate\n",
    "8. Save/push model\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 8ï¸âƒ£ **Beginner Code: Text Classification Fine-Tuning**\n",
    "\n",
    "### âœ” Install\n",
    "\n",
    "```bash\n",
    "pip install transformers datasets\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1 â€” Load Dataset\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2 â€” Load Tokenizer\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3 â€” Tokenize the Data\n",
    "\n",
    "```python\n",
    "def tok_fn(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized = dataset.map(tok_fn, batched=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4 â€” Load Pretrained Model\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5 â€” Training Arguments\n",
    "\n",
    "```python\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 6 â€” Trainer Object\n",
    "\n",
    "```python\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"]\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 7 â€” Train\n",
    "\n",
    "```python\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 8 â€” Evaluate\n",
    "\n",
    "```python\n",
    "trainer.evaluate()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 9 â€” Save Model\n",
    "\n",
    "```python\n",
    "trainer.save_model(\"sentiment_model\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 9ï¸âƒ£ **Fine-Tuning Custom Datasets (CSV, JSON, Excel)**\n",
    "\n",
    "### Load CSV:\n",
    "\n",
    "```python\n",
    "dataset = load_dataset(\"csv\", data_files=\"mydata.csv\")\n",
    "```\n",
    "\n",
    "### Columns should include:\n",
    "\n",
    "* `text`\n",
    "* `label`\n",
    "\n",
    "If names differ, rename:\n",
    "\n",
    "```python\n",
    "dataset = dataset.rename_column(\"review\", \"text\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ”Ÿ **Evaluation Metrics**\n",
    "\n",
    "### Beginner-Friendly:\n",
    "\n",
    "Metrics tell how good the model is.\n",
    "\n",
    "### Technical:\n",
    "\n",
    "Using `evaluate` library:\n",
    "\n",
    "```python\n",
    "import evaluate\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred\n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "```\n",
    "\n",
    "Add to Trainer:\n",
    "\n",
    "```python\n",
    "Trainer(... compute_metrics=compute_metrics)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£1ï¸âƒ£ **Saving & Uploading to Hugging Face Hub**\n",
    "\n",
    "```python\n",
    "trainer.push_to_hub(\"my-finetuned-model\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£2ï¸âƒ£ **PEFT: Parameter Efficient Fine-Tuning (Beginner + Technical)**\n",
    "\n",
    "### Beginner Explanation:\n",
    "\n",
    "PEFT = Fine-tune **only small parts** of the model â†’ saves memory.\n",
    "\n",
    "### Technical:\n",
    "\n",
    "Use LoRA, QLoRA, Prefix Tuning.\n",
    "\n",
    "### Example:\n",
    "\n",
    "```python\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£3ï¸âƒ£ **Fine-Tuning Tips for Students**\n",
    "\n",
    "### â­ Beginner:\n",
    "\n",
    "* Use small models (DistilBERT)\n",
    "* Use small batch sizes (8â€“16)\n",
    "* Train 2â€“3 epochs\n",
    "\n",
    "### â­ Technical:\n",
    "\n",
    "* Use mixed precision\n",
    "* Use weight decay\n",
    "* Use gradient checkpointing\n",
    "* Perform hyperparameter tuning\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£4ï¸âƒ£ **Common Training Errors**\n",
    "\n",
    "| Error              | Cause            | Fix                  |\n",
    "| ------------------ | ---------------- | -------------------- |\n",
    "| CUDA Out of Memory | Large batch size | Reduce batch size    |\n",
    "| Slow training      | Large model      | Use DistilBERT, PEFT |\n",
    "| Wrong labels       | Dataset mismatch | Check label mapping  |\n",
    "| Token mismatch     | Wrong tokenizer  | Use same model-name  |\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£5ï¸âƒ£ **Fine-Tuning Diagram (Technical + Beginner)**\n",
    "\n",
    "```\n",
    "Raw Dataset\n",
    "     â†“\n",
    "Tokenizer\n",
    "     â†“\n",
    "Tokenized Dataset\n",
    "     â†“\n",
    "Pretrained Model (BERT/T5)\n",
    "     â†“\n",
    "Trainer (TrainingArguments)\n",
    "     â†“\n",
    "Fine-tuned Model\n",
    "     â†“\n",
    "Evaluation / Save / Deploy\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£6ï¸âƒ£ **Real-World Use Cases of Fine-Tuning**\n",
    "\n",
    "### NLP:\n",
    "\n",
    "âœ” Sentiment analysis\n",
    "âœ” Email classification\n",
    "âœ” Chatbot for a company\n",
    "âœ” Resume screening\n",
    "âœ” Domain-specific QA\n",
    "\n",
    "### Vision:\n",
    "\n",
    "âœ” Medical image classification\n",
    "âœ” Product defect detection\n",
    "\n",
    "### Audio:\n",
    "\n",
    "âœ” Accent-specific speech recognition\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£7ï¸âƒ£ **Learning Outcomes (Module 6)**\n",
    "\n",
    "After finishing Module 6, students can:\n",
    "\n",
    "### â­ Beginner:\n",
    "\n",
    "âœ” Explain what fine-tuning is\n",
    "âœ” Understand datasets, epochs, labels\n",
    "âœ” Know why fine-tuning is needed\n",
    "\n",
    "### â­ Technical:\n",
    "\n",
    "âœ” Use Trainer API\n",
    "âœ” Tokenize datasets\n",
    "âœ” Run training & evaluation\n",
    "âœ” Save and upload models\n",
    "âœ” Use PEFT for efficient training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba7e7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e774c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6e76985",
   "metadata": {},
   "source": [
    "\n",
    "# ğŸŒŸ **MODULE 8 â€” ACCELERATE & PEFT (Efficient Training for Large Models)**\n",
    "\n",
    "### *(Beginner-Friendly + Technical Detailed Notes)*\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£ **What is Efficient Training?**\n",
    "\n",
    "### â­ Simple Definition (Non-Technical):\n",
    "\n",
    "Efficient training means **training big AI models using less memory, less cost, and faster speed**.\n",
    "\n",
    "### â­ Technical Definition:\n",
    "\n",
    "Techniques like:\n",
    "\n",
    "* Distributed training\n",
    "* Mixed precision (fp16/bf16)\n",
    "* Parameter-efficient fine-tuning\n",
    "* Quantization\n",
    "\n",
    "allow training large models on limited hardware (even a single GPU).\n",
    "\n",
    "---\n",
    "\n",
    "# 2ï¸âƒ£ **Why Do We Need Efficient Training?**\n",
    "\n",
    "### â­ Beginners:\n",
    "\n",
    "* Many models are huge (billions of parameters)\n",
    "* Normal computers cannot train them\n",
    "* Efficient methods make training possible\n",
    "\n",
    "### â­ Technical Users:\n",
    "\n",
    "* Reduce VRAM usage (40â€“70%)\n",
    "* Reduce training time\n",
    "* Enable multi-GPU training\n",
    "* Allow fine-tuning LLMs (7Bâ€“70B) on a single GPU\n",
    "\n",
    "---\n",
    "\n",
    "# 3ï¸âƒ£ **Two Major Tools in Hugging Face:**\n",
    "\n",
    "```\n",
    "1. Accelerate  â†’ Efficient training on any hardware \n",
    "2. PEFT        â†’ Train only small parts of model (LoRA, QLoRA)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ”µ **PART A â€” ACCELERATE**\n",
    "\n",
    "---\n",
    "\n",
    "# 4ï¸âƒ£ **What is Accelerate?**\n",
    "\n",
    "### â­ Simple Explanation:\n",
    "\n",
    "Accelerate helps you **train models on CPU or GPU easily**, without writing complex code.\n",
    "\n",
    "### â­ Technical Explanation:\n",
    "\n",
    "* Supports distributed training\n",
    "* Mixed precision (fp16, bf16)\n",
    "* TPU support\n",
    "* Multi-GPU handling\n",
    "* Device mapping\n",
    "* Zero-code-scale training\n",
    "\n",
    "---\n",
    "\n",
    "# 5ï¸âƒ£ **Accelerate Installation**\n",
    "\n",
    "```bash\n",
    "pip install accelerate\n",
    "accelerate config\n",
    "```\n",
    "\n",
    "The config command helps you choose:\n",
    "\n",
    "* CPU\n",
    "* Single GPU\n",
    "* Multiple GPUs\n",
    "* Mixed precision\n",
    "\n",
    "---\n",
    "\n",
    "# 6ï¸âƒ£ **Accelerate Workflow Diagram**\n",
    "\n",
    "```\n",
    "Your Model & Training Code\n",
    "             â†“\n",
    "     accelerate.prepare()\n",
    "             â†“\n",
    " Multi-GPU / TPU / CPU Auto Handling\n",
    "             â†“\n",
    "         Efficient Training\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 7ï¸âƒ£ **Basic Accelerate Example (Technical)**\n",
    "\n",
    "```python\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "model, optimizer, train_loader = accelerator.prepare(\n",
    "    model, optimizer, train_loader\n",
    ")\n",
    "```\n",
    "\n",
    "This automatically:\n",
    "\n",
    "* moves model to GPU\n",
    "* handles mixed precision\n",
    "* handles distributed training\n",
    "\n",
    "---\n",
    "\n",
    "# 8ï¸âƒ£ **Accelerate with Trainer**\n",
    "\n",
    "The Trainer API **already integrates** Accelerate internally.\n",
    "\n",
    "No change needed â€” Accelerate is used automatically.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸŸ¢ **PART B â€” PEFT (Parameter-Efficient Fine-Tuning)**\n",
    "\n",
    "---\n",
    "\n",
    "# 9ï¸âƒ£ **What is PEFT?**\n",
    "\n",
    "### â­ Beginner-Friendly:\n",
    "\n",
    "PEFT means **fine-tuning only small parts of a large model**, instead of updating all weights.\n",
    "\n",
    "This makes training:\n",
    "\n",
    "* Cheaper\n",
    "* Faster\n",
    "* Possible on normal GPUs\n",
    "\n",
    "### â­ Technical:\n",
    "\n",
    "PEFT updates **1â€“5%** of total parameters.\n",
    "\n",
    "Supported methods:\n",
    "\n",
    "* LoRA\n",
    "* QLoRA\n",
    "* Prefix Tuning\n",
    "* P-Tuning v2\n",
    "* Adapters\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ”Ÿ **Why PEFT is Important?**\n",
    "\n",
    "### â­ Beginners:\n",
    "\n",
    "Big models (like Llama, Mistral, GPT-J) are too heavy.\n",
    "PEFT lets you fine-tune them on a laptop or Google Colab.\n",
    "\n",
    "### â­ Technical Users:\n",
    "\n",
    "* Huge memory savings (50â€“80%)\n",
    "* Enables 4-bit training\n",
    "* Supports LLMs (7Bâ€“70B)\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£1ï¸âƒ£ **PEFT Diagram (Simple)**\n",
    "\n",
    "```\n",
    "Full Model (7 Billion Params)\n",
    " â†“\n",
    "Freeze 99% of weights\n",
    " â†“\n",
    "Train only small LoRA layers\n",
    " â†“\n",
    "Small, fast fine-tuning\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£2ï¸âƒ£ **PEFT Techniques Explained Simply**\n",
    "\n",
    "| Technique         | Simple Meaning             | Technical Meaning             |\n",
    "| ----------------- | -------------------------- | ----------------------------- |\n",
    "| **LoRA**          | Train small extra layers   | Low-rank matrix decomposition |\n",
    "| **QLoRA**         | Train LoRA in 4-bit        | Uses NF4 quantization         |\n",
    "| **Prefix Tuning** | Add extra learnable tokens | Learnable prefix embeddings   |\n",
    "| **Adapters**      | Insert small modules       | Residual adapter layers       |\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£3ï¸âƒ£ **LoRA Example (Beginner-Friendly)**\n",
    "\n",
    "LoRA adds small layers to the model like â€œplug-insâ€.\n",
    "\n",
    "Instead of updating the whole model, LoRA updates only the added plug-in layers.\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£4ï¸âƒ£ **Technical Code: Apply LoRA to a Transformer Model**\n",
    "\n",
    "```python\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=2\n",
    ")\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n",
    "```\n",
    "\n",
    "This prints:\n",
    "\n",
    "```\n",
    "Trainable params: 1% (LoRA only)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£5ï¸âƒ£ **QLoRA (The Most Popular Technique)**\n",
    "\n",
    "### â­ Beginner Explanation:\n",
    "\n",
    "QLoRA lets you fine-tune **very big models** using **very low memory**.\n",
    "\n",
    "### â­ Technical Explanation:\n",
    "\n",
    "* Uses 4-bit quantization (NF4)\n",
    "* Keeps base model frozen\n",
    "* Trains LoRA adapters\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£6ï¸âƒ£ **QLoRA Code Example**\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=\"float16\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"facebook/opt-1.3b\",\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£7ï¸âƒ£ **Advantages of Accelerate + PEFT**\n",
    "\n",
    "### â­ Beginners:\n",
    "\n",
    "* Train faster\n",
    "* Use cheap hardware\n",
    "* Learn large models easily\n",
    "\n",
    "### â­ Technical:\n",
    "\n",
    "* Minimal VRAM usage\n",
    "* Supports LLM fine-tuning\n",
    "* Multi-GPU distributed training\n",
    "* Mixed precision FP16/BF16\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£8ï¸âƒ£ **When to Use Accelerate?**\n",
    "\n",
    "âœ” When training on multiple GPUs\n",
    "âœ” When training big models\n",
    "âœ” When you want mixed precision\n",
    "âœ” When you need distributed training\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£9ï¸âƒ£ **When to Use PEFT?**\n",
    "\n",
    "âœ” You want to fine-tune LLMs (7Bâ€“70B)\n",
    "âœ” You have 1 GPU with 8â€“16 GB VRAM\n",
    "âœ” You want lightweight models for deployment\n",
    "âœ” You want low training cost\n",
    "\n",
    "---\n",
    "\n",
    "# 2ï¸âƒ£0ï¸âƒ£ **Full Workflow Diagram**\n",
    "\n",
    "```\n",
    "Load Dataset\n",
    "    â†“\n",
    "Load Pretrained Model\n",
    "    â†“\n",
    "Apply PEFT (LoRA / QLoRA / Adapters)\n",
    "    â†“\n",
    "Prepare Model using Accelerate\n",
    "    â†“\n",
    "Train using Trainer or custom loop\n",
    "    â†“\n",
    "Save & Push to Hugging Face Hub\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 2ï¸âƒ£1ï¸âƒ£ **Real-World Use Cases**\n",
    "\n",
    "### NLP\n",
    "\n",
    "âœ” Customer-specific chatbot\n",
    "âœ” Legal domain Q&A\n",
    "âœ” Medical text classifier\n",
    "âœ” Email classification\n",
    "\n",
    "### Vision\n",
    "\n",
    "âœ” Fine-tuning ViT with LoRA\n",
    "âœ” Product defect classification\n",
    "\n",
    "### Audio\n",
    "\n",
    "âœ” Language-specific Whisper fine-tuning\n",
    "\n",
    "---\n",
    "\n",
    "# 2ï¸âƒ£2ï¸âƒ£ **Beginner Activity**\n",
    "\n",
    "Ask students to:\n",
    "\n",
    "1. Load a small BERT model\n",
    "2. Apply LoRA\n",
    "3. Print trainable parameters\n",
    "4. Understand difference between full training vs PEFT\n",
    "\n",
    "---\n",
    "\n",
    "# 2ï¸âƒ£3ï¸âƒ£ **Technical Exercise**\n",
    "\n",
    "âœ” Fine-tune Llama 7B using QLoRA\n",
    "âœ” Try multi-GPU training with Accelerate\n",
    "âœ” Compare fp32 vs fp16 speed differences\n",
    "âœ” Measure VRAM usage\n",
    "\n",
    "---\n",
    "\n",
    "# 2ï¸âƒ£4ï¸âƒ£ **Learning Outcomes (Module 8)**\n",
    "\n",
    "After this module, students can:\n",
    "\n",
    "### â­ Beginners:\n",
    "\n",
    "âœ” Explain what efficient training is\n",
    "âœ” Understand LoRA & QLoRA in simple words\n",
    "âœ” Explain why large models need PEFT\n",
    "\n",
    "### â­ Technical:\n",
    "\n",
    "âœ” Use Accelerate for GPU training\n",
    "âœ” Apply PEFT to transformer models\n",
    "âœ” Train LLMs with QLoRA\n",
    "âœ” Perform distributed training\n",
    "âœ” Optimize memory usage\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c4e1da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ba3c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb8aa736",
   "metadata": {},
   "source": [
    "\n",
    "# ğŸŒŸ **MODULE 9 â€“ HUGGING FACE SPACES (APP DEPLOYMENT)**\n",
    "\n",
    "### *(Beginner-Friendly + Technical Detailed Notes)*\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£ **What Are Hugging Face Spaces?**\n",
    "\n",
    "### â­ Simple Definition (Non-Technical):\n",
    "\n",
    "Spaces are **AI apps that run on the Hugging Face website**.\n",
    "You can create apps without any servers.\n",
    "\n",
    "### â­ Technical Definition:\n",
    "\n",
    "A Git-based hosting platform where users can deploy:\n",
    "\n",
    "* **Gradio apps**\n",
    "* **Streamlit apps**\n",
    "* **Static HTML apps**\n",
    "* **Docker apps**\n",
    "\n",
    "Spaces provide:\n",
    "\n",
    "* Free CPU\n",
    "* Optional GPU/TPU\n",
    "* Auto-deployment\n",
    "* Git versioning\n",
    "\n",
    "---\n",
    "\n",
    "# 2ï¸âƒ£ **Why Use Spaces?**\n",
    "\n",
    "### â­ Beginner:\n",
    "\n",
    "* No need to buy cloud servers\n",
    "* You can create demos easily\n",
    "* Share your project with one link\n",
    "* Works like hosting website + AI inside\n",
    "\n",
    "### â­ Technical:\n",
    "\n",
    "* CI/CD deployment via Git\n",
    "* Private/public hosting\n",
    "* Built-in inference compute\n",
    "* Environment pinning using `requirements.txt`\n",
    "* Ideal for ML demo, prototype, production-lite apps\n",
    "\n",
    "---\n",
    "\n",
    "# 3ï¸âƒ£ **Types of Spaces**\n",
    "\n",
    "```\n",
    "1. Gradio     â†’ Build UI for AI apps easily\n",
    "2. Streamlit  â†’ Interactive web dashboards\n",
    "3. Static     â†’ HTML, CSS, JS websites\n",
    "4. Docker     â†’ Custom containerized apps\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 4ï¸âƒ£ **Spaces Folder Structure**\n",
    "\n",
    "Every Space needs at least:\n",
    "\n",
    "```\n",
    "app.py               â† Main app code\n",
    "requirements.txt     â† Python libraries\n",
    "README.md            â† App description\n",
    "```\n",
    "\n",
    "Optional:\n",
    "\n",
    "```\n",
    "runtime.txt\n",
    "Dockerfile\n",
    "assets/\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 5ï¸âƒ£ **Workflow Diagram: How Spaces Work**\n",
    "\n",
    "```\n",
    "Write Code (Gradio/Streamlit)\n",
    "            â†“\n",
    "Push to Hugging Face\n",
    "            â†“\n",
    "Auto Build & Deploy\n",
    "            â†“\n",
    "Public App URL You Can Share\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 6ï¸âƒ£ **Creating a Space (Beginner-Friendly)**\n",
    "\n",
    "### Step 1:\n",
    "\n",
    "Visit â†’ [https://huggingface.co/spaces](https://huggingface.co/spaces)\n",
    "\n",
    "### Step 2:\n",
    "\n",
    "Click: **Create new Space**\n",
    "\n",
    "### Step 3: Choose:\n",
    "\n",
    "* Gradio\n",
    "* Streamlit\n",
    "* Docker\n",
    "* Static\n",
    "\n",
    "### Step 4:\n",
    "\n",
    "Fill:\n",
    "\n",
    "* Space name\n",
    "* License\n",
    "* Public / Private\n",
    "\n",
    "### Step 5:\n",
    "\n",
    "Upload:\n",
    "\n",
    "* `app.py`\n",
    "* `requirements.txt`\n",
    "\n",
    "Deployment happens automatically.\n",
    "\n",
    "---\n",
    "\n",
    "# 7ï¸âƒ£ **Gradio Basics (For Non-Technical Students)**\n",
    "\n",
    "Gradio makes simple UI components like:\n",
    "\n",
    "* Textbox\n",
    "* Button\n",
    "* Image upload\n",
    "* Dropdown\n",
    "* Text output\n",
    "\n",
    "Very easy to build apps.\n",
    "\n",
    "---\n",
    "\n",
    "# 8ï¸âƒ£ **Gradio Example App (Beginner + Technical)**\n",
    "\n",
    "### â­ app.py\n",
    "\n",
    "```python\n",
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "def predict(text):\n",
    "    return classifier(text)[0]['label']\n",
    "\n",
    "iface = gr.Interface(fn=predict, inputs=\"text\", outputs=\"text\")\n",
    "iface.launch()\n",
    "```\n",
    "\n",
    "### â­ requirements.txt\n",
    "\n",
    "```\n",
    "gradio\n",
    "transformers\n",
    "torch\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 9ï¸âƒ£ **Streamlit Example App**\n",
    "\n",
    "### â­ app.py\n",
    "\n",
    "```python\n",
    "import streamlit as st\n",
    "from transformers import pipeline\n",
    "\n",
    "st.title(\"Sentiment Analyzer\")\n",
    "model = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "text = st.text_input(\"Enter text\")\n",
    "if text:\n",
    "    result = model(text)[0]\n",
    "    st.write(result)\n",
    "```\n",
    "\n",
    "### â­ requirements.txt\n",
    "\n",
    "```\n",
    "streamlit\n",
    "transformers\n",
    "torch\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ”Ÿ **Adding GPU Support (Technical Students)**\n",
    "\n",
    "Inside the Space â†’\n",
    "**Settings â†’ Hardware** â†’ choose:\n",
    "\n",
    "* CPU (Free)\n",
    "* T4 GPU\n",
    "* A10G GPU\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£1ï¸âƒ£ **Spaces Deployment Process (Technical)**\n",
    "\n",
    "### If using Git locally:\n",
    "\n",
    "```bash\n",
    "git clone https://huggingface.co/spaces/username/myapp\n",
    "cd myapp\n",
    "git add .\n",
    "git commit -m \"first commit\"\n",
    "git push\n",
    "```\n",
    "\n",
    "Spaces auto-builds and deploys.\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£2ï¸âƒ£ **Environment Management**\n",
    "\n",
    "### âœ” Python version\n",
    "\n",
    "Add:\n",
    "\n",
    "```\n",
    "runtime.txt\n",
    "```\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "python-3.10\n",
    "```\n",
    "\n",
    "### âœ” Specific versions of libraries\n",
    "\n",
    "Add:\n",
    "\n",
    "```\n",
    "transformers==4.36.0\n",
    "gradio==4.0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£3ï¸âƒ£ **Adding Images & Files to Space**\n",
    "\n",
    "Create folder:\n",
    "\n",
    "```\n",
    "/assets\n",
    "```\n",
    "\n",
    "Store:\n",
    "\n",
    "* images\n",
    "* audio\n",
    "* pdfs\n",
    "* logos\n",
    "\n",
    "Use in app:\n",
    "\n",
    "```python\n",
    "img = \"assets/logo.png\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£4ï¸âƒ£ **Advanced Features**\n",
    "\n",
    "### â­ Secrets / API Keys\n",
    "\n",
    "Go to:\n",
    "\n",
    "```\n",
    "Space â†’ Settings â†’ Secrets\n",
    "```\n",
    "\n",
    "Use in code:\n",
    "\n",
    "```python\n",
    "import os\n",
    "api = os.getenv(\"MY_API_KEY\")\n",
    "```\n",
    "\n",
    "### â­ Persistent Storage\n",
    "\n",
    "Use:\n",
    "\n",
    "```\n",
    "hf://\n",
    "```\n",
    "\n",
    "### â­ Live Logs\n",
    "\n",
    "Check logs in top-right menu.\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£5ï¸âƒ£ **Space Maintenance for Students**\n",
    "\n",
    "| Task                       | Why Important       |\n",
    "| -------------------------- | ------------------- |\n",
    "| Update dependencies        | avoid errors        |\n",
    "| Check logs                 | debug failures      |\n",
    "| Add README                 | explain project     |\n",
    "| Add screenshots            | better presentation |\n",
    "| Use GPL/Apache/MIT license | legal clarity       |\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£6ï¸âƒ£ **Real-World Apps Students Can Build**\n",
    "\n",
    "### NLP:\n",
    "\n",
    "âœ” Chatbot\n",
    "âœ” Summarizer\n",
    "âœ” Translator\n",
    "âœ” Grammar corrector\n",
    "âœ” Sentiment app\n",
    "\n",
    "### Vision:\n",
    "\n",
    "âœ” Image classifier\n",
    "âœ” Object detector\n",
    "âœ” Image generator (Stable Diffusion)\n",
    "\n",
    "### Audio:\n",
    "\n",
    "âœ” Speech-to-text app (Whisper)\n",
    "\n",
    "### Education:\n",
    "\n",
    "âœ” PDF Q&A app\n",
    "âœ” Notes summarizer\n",
    "âœ” Assignment evaluator\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£7ï¸âƒ£ **Visual Diagram: Space Structure**\n",
    "\n",
    "```\n",
    "my-space/\n",
    "â”œâ”€â”€ app.py\n",
    "â”œâ”€â”€ requirements.txt\n",
    "â”œâ”€â”€ README.md\n",
    "â””â”€â”€ assets/\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£8ï¸âƒ£ **Common Errors & Solutions**\n",
    "\n",
    "| Error                   | Reason          | Fix                     |\n",
    "| ----------------------- | --------------- | ----------------------- |\n",
    "| App stuck at â€œBuildingâ€ | Wrong versions  | Fix requirements.txt    |\n",
    "| Red error screen        | Syntax errors   | Check app.py            |\n",
    "| Model not loading       | Missing library | Add in requirements     |\n",
    "| Space slow              | Large model     | Use smaller model / GPU |\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£9ï¸âƒ£ **Beginner Activity**\n",
    "\n",
    "Ask students to:\n",
    "\n",
    "1. Create a Space\n",
    "2. Add simple Gradio app (text â†’ text)\n",
    "3. Share URL with class\n",
    "\n",
    "---\n",
    "\n",
    "# 2ï¸âƒ£0ï¸âƒ£ **Technical Exercises**\n",
    "\n",
    "âœ” Build chatbot using Llama or Mistral\n",
    "âœ” Add file upload for PDF summarization\n",
    "âœ” Deploy image classifier with GPU\n",
    "âœ” Add theme customization for UI\n",
    "\n",
    "---\n",
    "\n",
    "# 2ï¸âƒ£1ï¸âƒ£ **Learning Outcomes (Module 9)**\n",
    "\n",
    "After this module, students can:\n",
    "\n",
    "### â­ Beginners:\n",
    "\n",
    "âœ” Create a Space\n",
    "âœ” Deploy simple apps\n",
    "âœ” Use Gradio/Streamlit\n",
    "âœ” Share AI apps publicly\n",
    "\n",
    "### â­ Technical:\n",
    "\n",
    "âœ” Version-control apps\n",
    "âœ” Manage requirements\n",
    "âœ” Use GPU runtime\n",
    "âœ” Secure API Keys\n",
    "âœ” Build full ML prototypes\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310d912d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87379ce6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb4dad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc61810f",
   "metadata": {},
   "source": [
    "# ğŸŒŸ **MODULE 10 â€” REAL-WORLD PROJECTS (NLP, Vision, Audio & Multimodal)**\n",
    "\n",
    "### *(Beginner-Friendly + Technical Detailed Notes)*\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£ **Why Real-World Projects?**\n",
    "\n",
    "### â­ Beginner:\n",
    "\n",
    "Projects help you *see AI working in real life*.\n",
    "\n",
    "### â­ Technical:\n",
    "\n",
    "Projects combine:\n",
    "\n",
    "* Models\n",
    "* Datasets\n",
    "* Tokenizers\n",
    "* Inference workflows\n",
    "* Deployment (Spaces)\n",
    "\n",
    "This module connects all earlier modules.\n",
    "\n",
    "---\n",
    "\n",
    "# 2ï¸âƒ£ **Project Categories**\n",
    "\n",
    "```\n",
    "1. NLP (Text)\n",
    "2. Vision (Images)\n",
    "3. Audio (Speech)\n",
    "4. Multimodal (Text + Image)\n",
    "5. Full-Stack AI Apps (UI + Backend)\n",
    "```\n",
    "\n",
    "Each project can be:\n",
    "\n",
    "* Basic (for freshers)\n",
    "* Intermediate\n",
    "* Advanced (for technical learners)\n",
    "\n",
    "---\n",
    "\n",
    "# 3ï¸âƒ£ **NLP PROJECTS (Text-Based)**\n",
    "\n",
    "## ğŸŸ¦ **Project 1: Sentiment Analysis App**\n",
    "\n",
    "### â­ Beginner:\n",
    "\n",
    "* Input: Text\n",
    "* Output: Positive / Negative\n",
    "\n",
    "### â­ Technical:\n",
    "\n",
    "Use DistilBERT:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "sentiment = pipeline(\"sentiment-analysis\")\n",
    "sentiment(\"I love Hugging Face!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŸ© **Project 2: Text Summarization Tool**\n",
    "\n",
    "### â­ Beginner:\n",
    "\n",
    "Summarizes long text into short form.\n",
    "\n",
    "### â­ Technical:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "summ = pipeline(\"summarization\")\n",
    "summ(long_text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŸ§ **Project 3: English â†’ Hindi Translator**\n",
    "\n",
    "### â­ Beginner:\n",
    "\n",
    "Convert English sentences to Hindi.\n",
    "\n",
    "### â­ Technical:\n",
    "\n",
    "```python\n",
    "translator = pipeline(\"translation_en_to_hi\")\n",
    "translator(\"Artificial intelligence is the future.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŸ¥ **Project 4: Domain Chatbot**\n",
    "\n",
    "### â­ Beginner:\n",
    "\n",
    "Chatbot for:\n",
    "\n",
    "* Education\n",
    "* Healthcare\n",
    "* Banking\n",
    "* Travel\n",
    "\n",
    "### â­ Technical:\n",
    "\n",
    "Use Q&A + RAG:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "qa = pipeline(\"question-answering\")\n",
    "```\n",
    "\n",
    "Add FAISS/Chroma for RAG retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "# 4ï¸âƒ£ **VISION PROJECTS (Image-Based)**\n",
    "\n",
    "## ğŸŸ¦ **Project 5: Image Classification App**\n",
    "\n",
    "Example: Dog vs Cat\n",
    "\n",
    "### â­ Technical:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "model = pipeline(\"image-classification\")\n",
    "model(\"cat.jpg\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŸ© **Project 6: Object Detection**\n",
    "\n",
    "Detect objects in user-uploaded images.\n",
    "\n",
    "### â­ Technical:\n",
    "\n",
    "```python\n",
    "detector = pipeline(\"object-detection\")\n",
    "detector(\"street.jpg\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŸ§ **Project 7: Image Captioning**\n",
    "\n",
    "Generate captions from images.\n",
    "\n",
    "### â­ Technical:\n",
    "\n",
    "```python\n",
    "captioner = pipeline(\"image-to-text\")\n",
    "captioner(\"dog.jpg\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŸ¥ **Project 8: Image Generator Using Stable Diffusion**\n",
    "\n",
    "Generate images with prompts.\n",
    "\n",
    "### â­ Technical:\n",
    "\n",
    "```python\n",
    "from diffusers import DiffusionPipeline\n",
    "pipe = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
    "pipe(\"A cute robot teaching AI\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 5ï¸âƒ£ **AUDIO PROJECTS**\n",
    "\n",
    "## ğŸŸ¦ **Project 9: Speech-to-Text App**\n",
    "\n",
    "Convert voice to text.\n",
    "\n",
    "### â­ Technical:\n",
    "\n",
    "```python\n",
    "asr = pipeline(\"automatic-speech-recognition\")\n",
    "asr(\"audio.wav\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŸ© **Project 10: Emotion Detection from Voice**\n",
    "\n",
    "Uses audio classification models.\n",
    "\n",
    "---\n",
    "\n",
    "# 6ï¸âƒ£ **MULTIMODAL PROJECTS**\n",
    "\n",
    "## ğŸŸ¦ **Project 11: Image-Based Q&A App**\n",
    "\n",
    "Ask a question about an image.\n",
    "\n",
    "### â­ Technical:\n",
    "\n",
    "Models like:\n",
    "\n",
    "* BLIP\n",
    "* LLaVA\n",
    "* Donut\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŸ© **Project 12: PDF Q&A App**\n",
    "\n",
    "Upload PDF â†’ Ask questions â†’ Get answers.\n",
    "\n",
    "Workflow:\n",
    "\n",
    "1. Extract text from PDF\n",
    "2. Chunk & store in vector DB (FAISS/Chroma)\n",
    "3. Ask questions\n",
    "4. Model answers from chunks\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŸ§ **Project 13: Multimodal Chatbot**\n",
    "\n",
    "Combine:\n",
    "\n",
    "* Text\n",
    "* Image\n",
    "* Audio\n",
    "\n",
    "Use models like:\n",
    "\n",
    "* FLAN-T5\n",
    "* LLaVA\n",
    "* Whisper\n",
    "\n",
    "---\n",
    "\n",
    "# 7ï¸âƒ£ **FULL APP PROJECTS (End-to-End)**\n",
    "\n",
    "Below are full-stack projects combining:\n",
    "\n",
    "* Model\n",
    "* Dataset\n",
    "* Tokenizer\n",
    "* Training\n",
    "* App deployment (Spaces)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ **Project 14: News Summarizer + Sentiment Dashboard**\n",
    "\n",
    "### Features:\n",
    "\n",
    "* Paste any news article\n",
    "* Summarized result\n",
    "* Sentiment score\n",
    "* Deploy on Spaces (Gradio)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ **Project 15: Resume Analyzer**\n",
    "\n",
    "### Features:\n",
    "\n",
    "* Upload resume\n",
    "* Extract skills\n",
    "* Match with JD\n",
    "* Provide scoring\n",
    "\n",
    "Uses:\n",
    "\n",
    "* Tokenizer\n",
    "* Text classification\n",
    "* Summarization\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ **Project 16: Exam MCQ Generator**\n",
    "\n",
    "Enter topic â†’ Generate MCQs\n",
    "Use:\n",
    "\n",
    "* Text generation (T5/GPT2)\n",
    "* Inference pipelines\n",
    "\n",
    "---\n",
    "\n",
    "# 8ï¸âƒ£ **Project Templates (Technical)**\n",
    "\n",
    "### â­ Basic Gradio Template\n",
    "\n",
    "```python\n",
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "model = pipeline(\"text-classification\")\n",
    "\n",
    "def predict(text):\n",
    "    return model(text)[0]['label']\n",
    "\n",
    "gr.Interface(fn=predict, inputs=\"text\", outputs=\"text\").launch()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### â­ Basic Streamlit Template\n",
    "\n",
    "```python\n",
    "import streamlit as st\n",
    "from transformers import pipeline\n",
    "\n",
    "st.title(\"Text Classifier\")\n",
    "cls = pipeline(\"text-classification\")\n",
    "\n",
    "txt = st.text_input(\"Enter text:\")\n",
    "if txt:\n",
    "    st.write(cls(txt))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### â­ Image App Deployment\n",
    "\n",
    "```python\n",
    "img_model = pipeline(\"image-classification\")\n",
    "\n",
    "def classify(image):\n",
    "    return img_model(image)\n",
    "\n",
    "gr.Interface(fn=classify, inputs=\"image\", outputs=\"label\").launch()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 9ï¸âƒ£ **Projects for Beginners (Zero-Code)**\n",
    "\n",
    "Students can use Hugging Face website directly:\n",
    "\n",
    "### âœ” Try: Sentiment models\n",
    "\n",
    "### âœ” Try: Translation widgets\n",
    "\n",
    "### âœ” Try: Image classifier Spaces\n",
    "\n",
    "### âœ” Try: Audio ASR inside Spaces\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ”Ÿ **Projects for Technical Learners**\n",
    "\n",
    "### âœ” Fine-tune DistilBERT on custom data\n",
    "\n",
    "### âœ” Build QLoRA Llama chatbot\n",
    "\n",
    "### âœ” Deploy Stable Diffusion app\n",
    "\n",
    "### âœ” Create a multimodal Space\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£1ï¸âƒ£ **Real-World Workflow Diagram**\n",
    "\n",
    "```\n",
    "Choose Task\n",
    "     â†“\n",
    "Pick Dataset\n",
    "     â†“\n",
    "Choose Pretrained Model\n",
    "     â†“\n",
    "Tokenize & Preprocess\n",
    "     â†“\n",
    "Fine-Tune (optional)\n",
    "     â†“\n",
    "Inference App (Gradio/Streamlit)\n",
    "     â†“\n",
    "Deploy to Hugging Face Spaces\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£2ï¸âƒ£ **Beginner Activity**\n",
    "\n",
    "Ask students to build:\n",
    "âœ” A sentiment analyzer\n",
    "âœ” A translator\n",
    "âœ” An image classifier\n",
    "\n",
    "Give them:\n",
    "\n",
    "* Dummy dataset\n",
    "* Prewritten code\n",
    "* Step-by-step instructions\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£3ï¸âƒ£ **Technical Tasks**\n",
    "\n",
    "âœ” Deploy a multimodal app\n",
    "âœ” Use PEFT to fine-tune Llama\n",
    "âœ” Use FAISS for a PDF Q&A tool\n",
    "âœ” Build a GPU-based Stable Diffusion app\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£4ï¸âƒ£ **Learning Outcomes (Module 10)**\n",
    "\n",
    "After this module, students can:\n",
    "\n",
    "### â­ Beginners:\n",
    "\n",
    "âœ” Understand AI project structure\n",
    "âœ” Run simple NLP, Vision, Audio models\n",
    "âœ” Use Gradio/Streamlit apps\n",
    "âœ” Share AI apps publicly\n",
    "\n",
    "### â­ Technical:\n",
    "\n",
    "âœ” Build end-to-end AI apps\n",
    "âœ” Use pipelines, Tokenizers, AutoModels\n",
    "âœ” Fine-tune for specific tasks\n",
    "âœ” Deploy full apps using Spaces\n",
    "âœ” Create multimodal systems\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d311ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3783298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c620c512",
   "metadata": {},
   "source": [
    "all\n",
    "<!-- \n",
    "\n",
    "## **Module 1: Introduction to Hugging Face**\n",
    "\n",
    "**Goal:** Understand the HF ecosystem, purpose, and core libraries.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* What is Hugging Face?\n",
    "* Evolution from transformers to full GenAI ecosystem\n",
    "* Libraries overview\n",
    "\n",
    "  * `transformers`\n",
    "  * `diffusers`\n",
    "  * `datasets`\n",
    "  * `tokenizers`\n",
    "  * `accelerate`\n",
    "  * `peft`\n",
    "  * `gradio` & `streamlit` (Spaces)\n",
    "\n",
    "### **Demo**\n",
    "\n",
    "* Visit: [https://huggingface.co](https://huggingface.co)\n",
    "* Show trending models & community spaces.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## **Module 2: Hugging Face Hub**\n",
    "\n",
    "**Goal:** Learn to access and use models, datasets, and Spaces.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* Model Hub\n",
    "* Dataset Hub\n",
    "* Spaces Hub\n",
    "* Model cards\n",
    "* Search & filtering\n",
    "* Licensing + safe model usage\n",
    "\n",
    "### **Hands-on**\n",
    "\n",
    "```python\n",
    "from huggingface_hub import login, hf_hub_download\n",
    "login()\n",
    "hf_hub_download(repo_id=\"bert-base-uncased\", filename=\"config.json\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 3: Transformers Library**\n",
    "\n",
    "**Goal:** Understand transformer models & use pipelines.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* Importance of Transformers architecture\n",
    "* Pretrained models & checkpoints\n",
    "* Auto classes:\n",
    "\n",
    "  * `AutoModel`, `AutoTokenizer`, `AutoModelForSequenceClassification`, etc.\n",
    "* Pipelines (easy inference interface)\n",
    "\n",
    "### **Demo code**\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "sentiment = pipeline(\"sentiment-analysis\")\n",
    "sentiment(\"Hugging Face makes AI easy!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 4: Tokenizers**\n",
    "\n",
    "**Goal:** Deep understanding of tokenization.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* Why tokenization matters\n",
    "* Types:\n",
    "\n",
    "  * WordPiece\n",
    "  * BPE\n",
    "  * SentencePiece\n",
    "  * Unigram\n",
    "* Special tokens: PAD, CLS, SEP\n",
    "* Fast tokenizers (Rust-backed)\n",
    "\n",
    "### **Hands-on**\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tok(\"Hello Hugging Face!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 5: Inference with Transformers**\n",
    "\n",
    "**Goal:** Use models for real tasks.\n",
    "\n",
    "### **Tasks**\n",
    "\n",
    "* Text classification\n",
    "* Text generation\n",
    "* Named Entity Recognition\n",
    "* Question Answering\n",
    "* Translation\n",
    "* Summarization\n",
    "\n",
    "### **Quick demo**\n",
    "\n",
    "```python\n",
    "gen = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "gen(\"Explain Hugging Face in 10 words:\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 6: Fine-Tuning**\n",
    "\n",
    "**Goal:** Train your own models on custom datasets.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* Full vs. partial training\n",
    "* Trainer API\n",
    "* TrainingArguments\n",
    "* Metrics: accuracy, F1, BLEU, ROUGE\n",
    "* Building datasets (CSV/JSON/Parquet)\n",
    "\n",
    "### **Example Fine-tuning Script**\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "args = TrainingArguments(\"output\", evaluation_strategy=\"epoch\")\n",
    "trainer = Trainer(model=model, args=args)\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 7: Datasets Library**\n",
    "\n",
    "**Goal:** Learn to load, clean, and preprocess NLP datasets.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* Load from Hub\n",
    "* Map, filter, split\n",
    "* Tokenization with datasets\n",
    "* Streaming large datasets\n",
    "* Data collators\n",
    "\n",
    "### **Demo**\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"imdb\")\n",
    "print(ds[\"train\"][0])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 8: Accelerate & PEFT**\n",
    "\n",
    "**Goal:** Efficient training on low compute.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* Why efficient training? (cost + speed)\n",
    "* `accelerate` for device mapping & multi-GPU\n",
    "* PEFT:\n",
    "\n",
    "  * LoRA\n",
    "  * Prefix tuning\n",
    "  * QLoRA\n",
    "* Small models with big results\n",
    "\n",
    "### **Example**\n",
    "\n",
    "```python\n",
    "from peft import LoraConfig, get_peft_model\n",
    "config = LoraConfig(task_type=\"SEQ_CLS\")\n",
    "peft_model = get_peft_model(model, config)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 9: Hugging Face Spaces**\n",
    "\n",
    "**Goal:** Deploy real GenAI apps.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* What are Spaces?\n",
    "* Gradio apps\n",
    "* Streamlit apps\n",
    "* Repo structure:\n",
    "\n",
    "  * `app.py`\n",
    "  * `requirements.txt`\n",
    "* GPU vs CPU spaces\n",
    "* Public/Private deployments\n",
    "\n",
    "### **Gradio demo**\n",
    "\n",
    "```python\n",
    "import gradio as gr\n",
    "def greet(text): return \"Hello \" + text\n",
    "gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\").launch()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 10: Real-World GenAI Projects**\n",
    "\n",
    "**Goal:** Build industry-standard applications.\n",
    "\n",
    "### **Project Examples**\n",
    "\n",
    "1. **Sentiment Analyzer** (Transformers)\n",
    "2. **Text Summarizer** (T5/Falcon/Mistral)\n",
    "3. **Image Generator App** (Diffusers + Gradio)\n",
    "4. **RAG-based Q&A System**\n",
    "5. **Text-to-SQL chatbot**\n",
    "6. **Document classification app**\n",
    "7. **Audio Transcription App** (Whisper)\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 11: Deployment & Sharing**\n",
    "\n",
    "**Goal:** Publish models, datasets, and applications.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* Creating model cards\n",
    "* Uploading models\n",
    "* Versioning\n",
    "* Using Inference API\n",
    "* How to share Spaces\n",
    "* API rate-limits and considerations\n",
    "\n",
    "### **Upload Example**\n",
    "\n",
    "```python\n",
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"pytorch_model.bin\",\n",
    "    path_in_repo=\"pytorch_model.bin\",\n",
    "    repo_id=\"username/my-model\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# âœ… **Complete Course Outcomes**\n",
    "\n",
    "By the end, students can:\n",
    "\n",
    "âœ” Use HF Hub, datasets, and pretrained models\n",
    "âœ” Build text/image/audio GenAI apps\n",
    "âœ” Fine-tune transformer models\n",
    "âœ” Deploy apps on HuggingFace Spaces\n",
    "âœ” Publish & share professional models and demos\n",
    "\n",
    "---\n",
    "\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21789b35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "388575b4",
   "metadata": {},
   "source": [
    "\n",
    "# ğŸŒŸ **MODULE 11 â€” DEPLOYMENT & SHARING**\n",
    "\n",
    "### *(Model Cards, Hub Push, Versioning, Inference API)*\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£ **What Does â€œDeployment & Sharingâ€ Mean?**\n",
    "\n",
    "### â­ Simple Definition (Non-Technical):\n",
    "\n",
    "Deployment means **publishing your AI model or app so others can use it**.\n",
    "\n",
    "Sharing means **uploading it to Hugging Face Hub** for easy access.\n",
    "\n",
    "### â­ Technical Definition:\n",
    "\n",
    "Deployment involves:\n",
    "\n",
    "* Saving model & tokenizer files\n",
    "* Creating a model repository\n",
    "* Writing a model card\n",
    "* Pushing weights, config, and tokenizer\n",
    "* Exposing inference APIs\n",
    "* Version control & access management\n",
    "\n",
    "---\n",
    "\n",
    "# 2ï¸âƒ£ **Why Deploy Models?**\n",
    "\n",
    "### â­ Beginner:\n",
    "\n",
    "* Show your work\n",
    "* Share projects with teachers/friends\n",
    "* Useful for resume / portfolio\n",
    "\n",
    "### â­ Technical:\n",
    "\n",
    "* Enables reproducible research\n",
    "* CI/CD workflows\n",
    "* Team collaboration\n",
    "* API-based integration\n",
    "* Public or private storage\n",
    "\n",
    "---\n",
    "\n",
    "# 3ï¸âƒ£ **Three Things You Can Deploy on HF Hub**\n",
    "\n",
    "```\n",
    "1. Models     â†’ Transformer weights  \n",
    "2. Datasets   â†’ Custom data  \n",
    "3. Spaces     â†’ Apps (Gradio / Streamlit)\n",
    "```\n",
    "\n",
    "Module 9 covered Spaces.\n",
    "Now we focus on **Models** and **Inference API**.\n",
    "\n",
    "---\n",
    "\n",
    "# 4ï¸âƒ£ **Model Repository Structure**\n",
    "\n",
    "When you fine-tune and save a model, it generates:\n",
    "\n",
    "```\n",
    "my-model/\n",
    "â”œâ”€â”€ config.json\n",
    "â”œâ”€â”€ pytorch_model.bin\n",
    "â”œâ”€â”€ model.safetensors\n",
    "â”œâ”€â”€ tokenizer.json\n",
    "â”œâ”€â”€ tokenizer_config.json\n",
    "â”œâ”€â”€ vocab.txt (if WordPiece)\n",
    "â”œâ”€â”€ special_tokens_map.json\n",
    "â””â”€â”€ training_args.bin\n",
    "```\n",
    "\n",
    "### â­ Beginner Explanation:\n",
    "\n",
    "These files contain:\n",
    "\n",
    "* Model settings\n",
    "* Model brain\n",
    "* Tokenizer\n",
    "* Special tokens\n",
    "\n",
    "### â­ Technical Explanation:\n",
    "\n",
    "Used by:\n",
    "\n",
    "* `AutoModel`\n",
    "* `AutoTokenizer`\n",
    "* Inference API\n",
    "* Pipeline()\n",
    "\n",
    "---\n",
    "\n",
    "# 5ï¸âƒ£ **Creating a New Repository on Hugging Face Hub**\n",
    "\n",
    "### â­ Method 1: Using the Website\n",
    "\n",
    "1. Go to: [https://huggingface.co/new](https://huggingface.co/new)\n",
    "2. Select:\n",
    "\n",
    "   * Model / Dataset / Space\n",
    "3. Enter name\n",
    "4. Choose `Public` or `Private`\n",
    "5. Create repository\n",
    "\n",
    "---\n",
    "\n",
    "# 6ï¸âƒ£ **Logging In Programmatically**\n",
    "\n",
    "### Install\n",
    "\n",
    "```bash\n",
    "pip install huggingface_hub\n",
    "```\n",
    "\n",
    "### Login\n",
    "\n",
    "```python\n",
    "from huggingface_hub import login\n",
    "login()\n",
    "```\n",
    "\n",
    "OR CLI:\n",
    "\n",
    "```bash\n",
    "huggingface-cli login\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 7ï¸âƒ£ **Pushing a Fine-Tuned Model to Hub**\n",
    "\n",
    "### â­ Using Transformers Trainer API (Beginner-Friendly)\n",
    "\n",
    "After training:\n",
    "\n",
    "```python\n",
    "trainer.push_to_hub(\"sentiment-model\")\n",
    "```\n",
    "\n",
    "Everything uploads automatically:\n",
    "\n",
    "* config\n",
    "* tokenizer\n",
    "* weights\n",
    "* model card\n",
    "\n",
    "---\n",
    "\n",
    "### â­ Manual Upload (Technical)\n",
    "\n",
    "```python\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "api.create_repo(\"username/my-model\")\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=\"sentiment_model\",\n",
    "    repo_id=\"username/my-model\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 8ï¸âƒ£ **Writing a Model Card (README.md)**\n",
    "\n",
    "A Model Card describes:\n",
    "\n",
    "* What the model does\n",
    "* How it was trained\n",
    "* Dataset used\n",
    "* Intended use cases\n",
    "* Limitations\n",
    "* Ethical considerations\n",
    "\n",
    "### â­ Template\n",
    "\n",
    "````\n",
    "# Model Name\n",
    "\n",
    "## ğŸ§  Model Description\n",
    "Short explanation of what this model does.\n",
    "\n",
    "## ğŸ‹ï¸ Training Details\n",
    "- Dataset:\n",
    "- Epochs:\n",
    "- Learning Rate:\n",
    "\n",
    "## ğŸ“‚ Model Files\n",
    "- config.json\n",
    "- pytorch_model.bin\n",
    "- tokenizer.json\n",
    "\n",
    "## ğŸš€ Usage\n",
    "```python\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-classification\", model=\"username/my-model\")\n",
    "````\n",
    "\n",
    "## âš ï¸ Limitations\n",
    "\n",
    "State known problems.\n",
    "\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "# 9ï¸âƒ£ **Version Control on Hugging Face**\n",
    "\n",
    "HF uses **Git-LFS** for storing large model files.\n",
    "\n",
    "### â­ Technical Steps\n",
    "```bash\n",
    "git lfs install\n",
    "git clone https://huggingface.co/username/my-model\n",
    "cd my-model\n",
    "git add .\n",
    "git commit -m \"update\"\n",
    "git push\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ”Ÿ **Model Access: Public vs Private**\n",
    "\n",
    "### â­ Public:\n",
    "\n",
    "* Anyone can use\n",
    "* Appears in search\n",
    "* Good for portfolio\n",
    "\n",
    "### â­ Private:\n",
    "\n",
    "* Only specific users can access\n",
    "* Good for company data/models\n",
    "\n",
    "Control access via:\n",
    "\n",
    "```\n",
    "Settings â†’ Manage Collaborators\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£1ï¸âƒ£ **Inference API (Use Model as REST API)**\n",
    "\n",
    "Once your model is deployed, HF provides an **API endpoint**.\n",
    "\n",
    "### â­ Beginner:\n",
    "\n",
    "Use it like any web service.\n",
    "\n",
    "### â­ Technical Example:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/username/my-model\"\n",
    "headers = {\"Authorization\": \"Bearer YOUR_API_KEY\"}\n",
    "\n",
    "payload = {\"inputs\": \"This is awesome!\"}\n",
    "res = requests.post(API_URL, headers=headers, json=payload)\n",
    "print(res.json())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£2ï¸âƒ£ **Widgets (No-Code Model Demo)**\n",
    "\n",
    "Each model page includes:\n",
    "\n",
    "* A browser widget\n",
    "* Zero-code testing\n",
    "\n",
    "Students can:\n",
    "\n",
    "* Type text\n",
    "* Upload image/audio\n",
    "* See prediction\n",
    "* No programming needed\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£3ï¸âƒ£ **Deploying a Full Application + Model**\n",
    "\n",
    "Steps:\n",
    "\n",
    "```\n",
    "1. Fine-tune model\n",
    "2. Push to hub\n",
    "3. Create Gradio app using your model\n",
    "4. Deploy to Spaces\n",
    "```\n",
    "\n",
    "Example Gradio loading your HF model:\n",
    "\n",
    "```python\n",
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"username/my-model\")\n",
    "\n",
    "def predict(text):\n",
    "    return classifier(text)\n",
    "\n",
    "gr.Interface(predict, \"text\", \"label\").launch()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£4ï¸âƒ£ **API Keys & Security**\n",
    "\n",
    "### Get API Key:\n",
    "\n",
    "Go to:\n",
    "\n",
    "```\n",
    "Settings â†’ Access Tokens\n",
    "```\n",
    "\n",
    "### Use securely:\n",
    "\n",
    "Store in `.env`, Secrets, or OS variables.\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£5ï¸âƒ£ **Monitoring Models (Technical Users)**\n",
    "\n",
    "HF Dashboard allows:\n",
    "\n",
    "* Download stats\n",
    "* API usage\n",
    "* Space performance\n",
    "* Logs\n",
    "* Hardware monitoring\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£6ï¸âƒ£ **Common Errors & Solutions**\n",
    "\n",
    "| Error              | Cause              | Fix                     |\n",
    "| ------------------ | ------------------ | ----------------------- |\n",
    "| Model not loading  | Missing tokenizer  | Upload tokenizer files  |\n",
    "| API rate limit     | Free plan exceeded | Use higher plan         |\n",
    "| File too large     | Missing Git-LFS    | Install git-lfs         |\n",
    "| Inference too slow | Large model        | Use smaller model / GPU |\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£7ï¸âƒ£ **Beginner Activity**\n",
    "\n",
    "Ask students to:\n",
    "\n",
    "1. Fine-tune DistilBERT on IMDB\n",
    "2. Upload model to Hub\n",
    "3. Write a model card\n",
    "4. Test in Inference Widget\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£8ï¸âƒ£ **Advanced Technical Tasks**\n",
    "\n",
    "âœ” Build API using HF Inference Endpoints\n",
    "âœ” Create private model for enterprise\n",
    "âœ” Enable GPU acceleration for inference\n",
    "âœ” Use Docker Spaces for custom backends\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£9ï¸âƒ£ **Deployment Workflow Diagram**\n",
    "\n",
    "```\n",
    "Fine-Tuned Model\n",
    "      â†“\n",
    "Save Model Locally\n",
    "      â†“\n",
    "Push to Hugging Face Hub\n",
    "      â†“\n",
    "Model Repository Created\n",
    "      â†“\n",
    "Test via Inference API / Widget\n",
    "      â†“\n",
    "Integrate into Apps / Spaces\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 2ï¸âƒ£0ï¸âƒ£ **Learning Outcomes (Module 11)**\n",
    "\n",
    "After this module, students can:\n",
    "\n",
    "### â­ Beginners:\n",
    "\n",
    "âœ” Publish models to the Hub\n",
    "âœ” Write a simple model card\n",
    "âœ” Use Inference Widget\n",
    "âœ” Test model as API\n",
    "\n",
    "### â­ Technical Users:\n",
    "\n",
    "âœ” Upload custom fine-tuned models\n",
    "âœ” Use Git-LFS & versioning\n",
    "âœ” Access models via Python/REST\n",
    "âœ” Deploy complete apps in Spaces\n",
    "âœ” Manage private/public access\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e94eac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f66be0c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58077cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1ac8d4c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **Module 2: Hugging Face Hub**\n",
    "\n",
    "**Goal:** Learn to access and use models, datasets, and Spaces.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* Model Hub\n",
    "* Dataset Hub\n",
    "* Spaces Hub\n",
    "* Model cards\n",
    "* Search & filtering\n",
    "* Licensing + safe model usage\n",
    "\n",
    "### **Hands-on**\n",
    "\n",
    "```python\n",
    "from huggingface_hub import login, hf_hub_download\n",
    "login()\n",
    "hf_hub_download(repo_id=\"bert-base-uncased\", filename=\"config.json\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 3: Transformers Library**\n",
    "\n",
    "**Goal:** Understand transformer models & use pipelines.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* Importance of Transformers architecture\n",
    "* Pretrained models & checkpoints\n",
    "* Auto classes:\n",
    "\n",
    "  * `AutoModel`, `AutoTokenizer`, `AutoModelForSequenceClassification`, etc.\n",
    "* Pipelines (easy inference interface)\n",
    "\n",
    "### **Demo code**\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "sentiment = pipeline(\"sentiment-analysis\")\n",
    "sentiment(\"Hugging Face makes AI easy!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 4: Tokenizers**\n",
    "\n",
    "**Goal:** Deep understanding of tokenization.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* Why tokenization matters\n",
    "* Types:\n",
    "\n",
    "  * WordPiece\n",
    "  * BPE\n",
    "  * SentencePiece\n",
    "  * Unigram\n",
    "* Special tokens: PAD, CLS, SEP\n",
    "* Fast tokenizers (Rust-backed)\n",
    "\n",
    "### **Hands-on**\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tok(\"Hello Hugging Face!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 5: Inference with Transformers**\n",
    "\n",
    "**Goal:** Use models for real tasks.\n",
    "\n",
    "### **Tasks**\n",
    "\n",
    "* Text classification\n",
    "* Text generation\n",
    "* Named Entity Recognition\n",
    "* Question Answering\n",
    "* Translation\n",
    "* Summarization\n",
    "\n",
    "### **Quick demo**\n",
    "\n",
    "```python\n",
    "gen = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "gen(\"Explain Hugging Face in 10 words:\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 6: Fine-Tuning**\n",
    "\n",
    "**Goal:** Train your own models on custom datasets.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* Full vs. partial training\n",
    "* Trainer API\n",
    "* TrainingArguments\n",
    "* Metrics: accuracy, F1, BLEU, ROUGE\n",
    "* Building datasets (CSV/JSON/Parquet)\n",
    "\n",
    "### **Example Fine-tuning Script**\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "args = TrainingArguments(\"output\", evaluation_strategy=\"epoch\")\n",
    "trainer = Trainer(model=model, args=args)\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 7: Datasets Library**\n",
    "\n",
    "**Goal:** Learn to load, clean, and preprocess NLP datasets.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* Load from Hub\n",
    "* Map, filter, split\n",
    "* Tokenization with datasets\n",
    "* Streaming large datasets\n",
    "* Data collators\n",
    "\n",
    "### **Demo**\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"imdb\")\n",
    "print(ds[\"train\"][0])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 8: Accelerate & PEFT**\n",
    "\n",
    "**Goal:** Efficient training on low compute.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* Why efficient training? (cost + speed)\n",
    "* `accelerate` for device mapping & multi-GPU\n",
    "* PEFT:\n",
    "\n",
    "  * LoRA\n",
    "  * Prefix tuning\n",
    "  * QLoRA\n",
    "* Small models with big results\n",
    "\n",
    "### **Example**\n",
    "\n",
    "```python\n",
    "from peft import LoraConfig, get_peft_model\n",
    "config = LoraConfig(task_type=\"SEQ_CLS\")\n",
    "peft_model = get_peft_model(model, config)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 9: Hugging Face Spaces**\n",
    "\n",
    "**Goal:** Deploy real GenAI apps.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* What are Spaces?\n",
    "* Gradio apps\n",
    "* Streamlit apps\n",
    "* Repo structure:\n",
    "\n",
    "  * `app.py`\n",
    "  * `requirements.txt`\n",
    "* GPU vs CPU spaces\n",
    "* Public/Private deployments\n",
    "\n",
    "### **Gradio demo**\n",
    "\n",
    "```python\n",
    "import gradio as gr\n",
    "def greet(text): return \"Hello \" + text\n",
    "gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\").launch()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 10: Real-World GenAI Projects**\n",
    "\n",
    "**Goal:** Build industry-standard applications.\n",
    "\n",
    "### **Project Examples**\n",
    "\n",
    "1. **Sentiment Analyzer** (Transformers)\n",
    "2. **Text Summarizer** (T5/Falcon/Mistral)\n",
    "3. **Image Generator App** (Diffusers + Gradio)\n",
    "4. **RAG-based Q&A System**\n",
    "5. **Text-to-SQL chatbot**\n",
    "6. **Document classification app**\n",
    "7. **Audio Transcription App** (Whisper)\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 11: Deployment & Sharing**\n",
    "\n",
    "**Goal:** Publish models, datasets, and applications.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* Creating model cards\n",
    "* Uploading models\n",
    "* Versioning\n",
    "* Using Inference API\n",
    "* How to share Spaces\n",
    "* API rate-limits and considerations\n",
    "\n",
    "### **Upload Example**\n",
    "\n",
    "```python\n",
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"pytorch_model.bin\",\n",
    "    path_in_repo=\"pytorch_model.bin\",\n",
    "    repo_id=\"username/my-model\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# âœ… **Complete Course Outcomes**\n",
    "\n",
    "By the end, students can:\n",
    "\n",
    "âœ” Use HF Hub, datasets, and pretrained models\n",
    "âœ” Build text/image/audio GenAI apps\n",
    "âœ” Fine-tune transformer models\n",
    "âœ” Deploy apps on HuggingFace Spaces\n",
    "âœ” Publish & share professional models and demos\n",
    "\n",
    "---\n",
    "\n",
    "# Want me to prepare next?\n",
    "\n",
    "I can generate any of the following:\n",
    "\n",
    "ğŸ“Œ **Full teaching slides (PPT)**\n",
    "ğŸ“Œ **PDF notes for each module**\n",
    "ğŸ“Œ **Hands-on assignments + solutions**\n",
    "ğŸ“Œ **End-to-end real-world project notebooks**\n",
    "ğŸ“Œ **Full GitHub-ready course folder (code + datasets + Spaces)**\n",
    "\n",
    "Tell me what you want, and Iâ€™ll create it.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
