{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32889b7d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "module_wise_hf copy.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e2133c",
   "metadata": {},
   "source": [
    "Great goal üëç ‚Äî this is **very realistic** and widely used in industry.\n",
    "\n",
    "Below is a **clear, end-to-end blueprint** for building a **Content Creation AI** for **LinkedIn & Facebook** using an **open-source LLM**, **fine-tuning**, and **ü§ó Accelerate**.\n",
    "\n",
    "I‚Äôll keep it **practical + exam/project friendly**, not overly complex.\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Project: Social Media Content Creation AI\n",
    "\n",
    "**Platforms:** LinkedIn, Facebook\n",
    "**Model:** Open-source LLM (LLaMA / Mistral)\n",
    "**Features:**\n",
    "\n",
    "* Generate posts\n",
    "* Match platform tone\n",
    "* Improve quality via fine-tuning\n",
    "* Train faster using **Accelerate**\n",
    "\n",
    "---\n",
    "\n",
    "## üß† 1. System Architecture (High Level)\n",
    "\n",
    "```\n",
    "User Prompt\n",
    "   ‚Üì\n",
    "Open-Source LLM\n",
    "   ‚Üì\n",
    "Platform-Specific Prompting\n",
    "   ‚Üì\n",
    "Fine-Tuned Model\n",
    "   ‚Üì\n",
    "Optimized Post (LinkedIn / Facebook)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ñ 2. Choose an Open-Source LLM\n",
    "\n",
    "Recommended (beginner ‚Üí advanced):\n",
    "\n",
    "| Model          | Why                            |\n",
    "| -------------- | ------------------------------ |\n",
    "| **Mistral-7B** | Lightweight, powerful          |\n",
    "| **LLaMA-2-7B** | Widely supported               |\n",
    "| **Phi-2**      | Small, fast, great for content |\n",
    "| **Gemma-7B**   | Strong instruction tuning      |\n",
    "\n",
    "üëâ **Best choice:** `mistralai/Mistral-7B-Instruct`\n",
    "\n",
    "---\n",
    "\n",
    "## üìö 3. Prepare Training Dataset (Most Important Part)\n",
    "\n",
    "### Dataset Format (JSON)\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"instruction\": \"Write a LinkedIn post about AI in healthcare\",\n",
    "  \"input\": \"\",\n",
    "  \"output\": \"AI is transforming healthcare by improving diagnostics...\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Platform-Specific Examples\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"instruction\": \"Write a Facebook post promoting a startup\",\n",
    "  \"input\": \"\",\n",
    "  \"output\": \"We‚Äôre excited to announce our new journey...\"\n",
    "}\n",
    "```\n",
    "\n",
    "üìå Collect:\n",
    "\n",
    "* High-engagement LinkedIn posts\n",
    "* Facebook marketing posts\n",
    "* Your own writing style (best)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° 4. Fine-Tune the Model Using Accelerate (Core Part)\n",
    "\n",
    "### Install\n",
    "\n",
    "```bash\n",
    "pip install accelerate transformers datasets peft torch\n",
    "accelerate config\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîß 5. Fine-Tuning Code (LLM + Accelerate + LoRA)\n",
    "\n",
    "### Why LoRA?\n",
    "\n",
    "* Faster\n",
    "* Cheaper\n",
    "* Perfect for content generation\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Fine-Tuning Script\n",
    "\n",
    "```python\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"social_media_data.json\")\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"instruction\"] + example[\"output\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "dataset = dataset.map(tokenize)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./content-ai\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"]\n",
    ")\n",
    "\n",
    "# Train with Accelerate\n",
    "trainer.train()\n",
    "\n",
    "print(\"Fine-tuning completed!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ 6. Generate Platform-Specific Content\n",
    "\n",
    "```python\n",
    "prompt = \"\"\"\n",
    "Write a LinkedIn post about the future of AI.\n",
    "Tone: Professional, insightful, engaging\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=150)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß© 7. Platform Prompt Templates\n",
    "\n",
    "### LinkedIn\n",
    "\n",
    "```\n",
    "Tone: Professional\n",
    "Length: Medium\n",
    "Add CTA\n",
    "Use industry insights\n",
    "```\n",
    "\n",
    "### Facebook\n",
    "\n",
    "```\n",
    "Tone: Casual\n",
    "Add emojis\n",
    "Short paragraphs\n",
    "Engaging questions\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìà 8. How Fine-Tuning Improves Performance\n",
    "\n",
    "| Before Fine-Tuning | After Fine-Tuning |\n",
    "| ------------------ | ----------------- |\n",
    "| Generic responses  | Platform-specific |\n",
    "| Inconsistent tone  | Brand-aligned     |\n",
    "| Weak engagement    | High CTR & reach  |\n",
    "\n",
    "---\n",
    "\n",
    "## üìù 9. Exam / Viva Explanation (Perfect Answer)\n",
    "\n",
    "> We use an open-source LLM fine-tuned on social media content. Accelerate enables efficient multi-GPU and mixed-precision training, while LoRA reduces compute cost. The system generates optimized platform-specific content for LinkedIn and Facebook.\n",
    "\n",
    "---\n",
    "\n",
    "## üî• Next Enhancements (Real Product)\n",
    "\n",
    "* Hashtag generator\n",
    "* Image captioning\n",
    "* Scheduling posts\n",
    "* A/B testing posts\n",
    "* Analytics feedback loop\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can:\n",
    "\n",
    "* üìÑ Create **project documentation**\n",
    "* üß™ Add **evaluation metrics**\n",
    "* üåê Build a **web app UI**\n",
    "* üß† Explain this in **simple exam language**\n",
    "\n",
    "Just tell me üòä\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3967273b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d6fea69",
   "metadata": {},
   "source": [
    "# üåü **MODULE 6 ‚Äì FINE-TUNING (TRAINER API + CUSTOM DATASETS)**\n",
    "\n",
    "### *(Beginner-Friendly + Technical Detailed Notes)*\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ **What is Fine-Tuning?**\n",
    "\n",
    "### ‚≠ê Simple Definition (Non-Technical):\n",
    "\n",
    "Fine-tuning means **teaching a pre-trained AI model a new skill** using your own data.\n",
    "\n",
    "Example:\n",
    "You take a general model (like BERT trained on Wikipedia)\n",
    "and teach it:\n",
    "\n",
    "* To classify movie reviews\n",
    "* To detect spam\n",
    "* To answer domain-specific questions\n",
    "\n",
    "### ‚≠ê Technical Definition:\n",
    "\n",
    "Fine-tuning = updating **only part (or all)** of the model weights on a labeled dataset using supervised learning.\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ **Why Do We Fine-Tune?**\n",
    "\n",
    "### ‚≠ê Non-Technical:\n",
    "\n",
    "* Saves time (model already knows language)\n",
    "* Needs less data\n",
    "* Cheaper than training from scratch\n",
    "* Produces accurate results\n",
    "\n",
    "### ‚≠ê Technical:\n",
    "\n",
    "* Improves model performance on downstream tasks\n",
    "* Requires small datasets (1k‚Äì50k)\n",
    "* Backpropagation updates parameters\n",
    "* Works for text, images, audio\n",
    "\n",
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ **Fine-Tuning Workflow (Beginner Diagram)**\n",
    "\n",
    "```\n",
    "      Pretrained Model (BERT, DistilBERT, GPT)\n",
    "                        ‚Üì\n",
    "            Add small labeled dataset\n",
    "                        ‚Üì\n",
    "                Train (few minutes)\n",
    "                        ‚Üì\n",
    "           Model learns your specific task\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ **Hugging Face Tools for Fine-Tuning**\n",
    "\n",
    "```\n",
    "Transformers ‚Üí Models + Tokenizers\n",
    "Datasets     ‚Üí Load your dataset\n",
    "Trainer API  ‚Üí Training loop\n",
    "PEFT         ‚Üí Efficient training (LoRA)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ **Key Concepts Before Fine-Tuning**\n",
    "\n",
    "| Concept           | Beginner-Friendly Meaning            |\n",
    "| ----------------- | ------------------------------------ |\n",
    "| **Epoch**         | One full pass over the dataset       |\n",
    "| **Batch Size**    | Number of samples processed together |\n",
    "| **Learning Rate** | How fast the model learns            |\n",
    "| **Loss**          | Model mistake level (lower = better) |\n",
    "| **Evaluation**    | Checking model accuracy              |\n",
    "| **Metrics**       | Accuracy, F1, Precision, Recall      |\n",
    "\n",
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ **Trainer API ‚Äî Easiest Way to Fine-Tune**\n",
    "\n",
    "### ‚≠ê Non-Technical Explanation:\n",
    "\n",
    "Trainer is a **ready-made training engine** that trains models for you.\n",
    "\n",
    "### ‚≠ê Technical Explanation:\n",
    "\n",
    "Trainer manages:\n",
    "\n",
    "* Data loaders\n",
    "* Optimizers\n",
    "* Schedulers\n",
    "* Logging\n",
    "* Mixed precision (fp16)\n",
    "* Evaluation loops\n",
    "* Saving checkpoints\n",
    "\n",
    "---\n",
    "\n",
    "# 7Ô∏è‚É£ **Steps for Fine-Tuning Using Trainer API**\n",
    "\n",
    "```\n",
    "1. Load dataset\n",
    "2. Load tokenizer\n",
    "3. Tokenize dataset\n",
    "4. Load pretrained model\n",
    "5. Define training settings\n",
    "6. Train with Trainer()\n",
    "7. Evaluate\n",
    "8. Save/push model\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 8Ô∏è‚É£ **Beginner Code: Text Classification Fine-Tuning**\n",
    "\n",
    "### ‚úî Install\n",
    "\n",
    "```bash\n",
    "pip install transformers datasets\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1 ‚Äî Load Dataset\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2 ‚Äî Load Tokenizer\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3 ‚Äî Tokenize the Data\n",
    "\n",
    "```python\n",
    "def tok_fn(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized = dataset.map(tok_fn, batched=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4 ‚Äî Load Pretrained Model\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5 ‚Äî Training Arguments\n",
    "\n",
    "```python\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 6 ‚Äî Trainer Object\n",
    "\n",
    "```python\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"]\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 7 ‚Äî Train\n",
    "\n",
    "```python\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 8 ‚Äî Evaluate\n",
    "\n",
    "```python\n",
    "trainer.evaluate()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 9 ‚Äî Save Model\n",
    "\n",
    "```python\n",
    "trainer.save_model(\"sentiment_model\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 9Ô∏è‚É£ **Fine-Tuning Custom Datasets (CSV, JSON, Excel)**\n",
    "\n",
    "### Load CSV:\n",
    "\n",
    "```python\n",
    "dataset = load_dataset(\"csv\", data_files=\"mydata.csv\")\n",
    "```\n",
    "\n",
    "### Columns should include:\n",
    "\n",
    "* `text`\n",
    "* `label`\n",
    "\n",
    "If names differ, rename:\n",
    "\n",
    "```python\n",
    "dataset = dataset.rename_column(\"review\", \"text\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üîü **Evaluation Metrics**\n",
    "\n",
    "### Beginner-Friendly:\n",
    "\n",
    "Metrics tell how good the model is.\n",
    "\n",
    "### Technical:\n",
    "\n",
    "Using `evaluate` library:\n",
    "\n",
    "```python\n",
    "import evaluate\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred\n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "```\n",
    "\n",
    "Add to Trainer:\n",
    "\n",
    "```python\n",
    "Trainer(... compute_metrics=compute_metrics)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ **Saving & Uploading to Hugging Face Hub**\n",
    "\n",
    "```python\n",
    "trainer.push_to_hub(\"my-finetuned-model\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£2Ô∏è‚É£ **PEFT: Efficient Fine-Tuning (Beginner + Technical)**\n",
    "\n",
    "### Beginner Explanation:\n",
    "\n",
    "PEFT = Fine-tune **only small parts** of the model ‚Üí saves memory.\n",
    "\n",
    "### Technical:\n",
    "\n",
    "Use LoRA, QLoRA, Prefix Tuning.\n",
    "\n",
    "### Example:\n",
    "\n",
    "```python\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£3Ô∏è‚É£ **Fine-Tuning Tips for Students**\n",
    "\n",
    "### ‚≠ê Beginner:\n",
    "\n",
    "* Use small models (DistilBERT)\n",
    "* Use small batch sizes (8‚Äì16)\n",
    "* Train 2‚Äì3 epochs\n",
    "\n",
    "### ‚≠ê Technical:\n",
    "\n",
    "* Use mixed precision\n",
    "* Use weight decay\n",
    "* Use gradient checkpointing\n",
    "* Perform hyperparameter tuning\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£4Ô∏è‚É£ **Common Training Errors**\n",
    "\n",
    "| Error              | Cause            | Fix                  |\n",
    "| ------------------ | ---------------- | -------------------- |\n",
    "| CUDA Out of Memory | Large batch size | Reduce batch size    |\n",
    "| Slow training      | Large model      | Use DistilBERT, PEFT |\n",
    "| Wrong labels       | Dataset mismatch | Check label mapping  |\n",
    "| Token mismatch     | Wrong tokenizer  | Use same model-name  |\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£5Ô∏è‚É£ **Fine-Tuning Diagram (Technical + Beginner)**\n",
    "\n",
    "```\n",
    "Raw Dataset\n",
    "     ‚Üì\n",
    "Tokenizer\n",
    "     ‚Üì\n",
    "Tokenized Dataset\n",
    "     ‚Üì\n",
    "Pretrained Model (BERT/T5)\n",
    "     ‚Üì\n",
    "Trainer (TrainingArguments)\n",
    "     ‚Üì\n",
    "Fine-tuned Model\n",
    "     ‚Üì\n",
    "Evaluation / Save / Deploy\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£6Ô∏è‚É£ **Real-World Use Cases of Fine-Tuning**\n",
    "\n",
    "### NLP:\n",
    "\n",
    "‚úî Sentiment analysis\n",
    "‚úî Email classification\n",
    "‚úî Chatbot for a company\n",
    "‚úî Resume screening\n",
    "‚úî Domain-specific QA\n",
    "\n",
    "### Vision:\n",
    "\n",
    "‚úî Medical image classification\n",
    "‚úî Product defect detection\n",
    "\n",
    "### Audio:\n",
    "\n",
    "‚úî Accent-specific speech recognition\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£7Ô∏è‚É£ **Learning Outcomes (Module 6)**\n",
    "\n",
    "After finishing Module 6, students can:\n",
    "\n",
    "### ‚≠ê Beginner:\n",
    "\n",
    "‚úî Explain what fine-tuning is\n",
    "‚úî Understand datasets, epochs, labels\n",
    "‚úî Know why fine-tuning is needed\n",
    "\n",
    "### ‚≠ê Technical:\n",
    "\n",
    "‚úî Use Trainer API\n",
    "‚úî Tokenize datasets\n",
    "‚úî Run training & evaluation\n",
    "‚úî Save and upload models\n",
    "‚úî Use PEFT for efficient training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba7e7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e774c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6e76985",
   "metadata": {},
   "source": [
    "\n",
    "# üåü **MODULE 8 ‚Äî ACCELERATE & PEFT (Efficient Training for Large Models)**\n",
    "\n",
    "### *(Beginner-Friendly + Technical Detailed Notes)*\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ **What is Efficient Training?**\n",
    "\n",
    "### ‚≠ê Simple Definition (Non-Technical):\n",
    "\n",
    "Efficient training means **training big AI models using less memory, less cost, and faster speed**.\n",
    "\n",
    "### ‚≠ê Technical Definition:\n",
    "\n",
    "Techniques like:\n",
    "\n",
    "* Distributed training\n",
    "* Mixed precision (fp16/bf16)\n",
    "* Parameter-efficient fine-tuning\n",
    "* Quantization\n",
    "\n",
    "allow training large models on limited hardware (even a single GPU).\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ **Why Do We Need Efficient Training?**\n",
    "\n",
    "### ‚≠ê Beginners:\n",
    "\n",
    "* Many models are huge (billions of parameters)\n",
    "* Normal computers cannot train them\n",
    "* Efficient methods make training possible\n",
    "\n",
    "### ‚≠ê Technical Users:\n",
    "\n",
    "* Reduce VRAM usage (40‚Äì70%)\n",
    "* Reduce training time\n",
    "* Enable multi-GPU training\n",
    "* Allow fine-tuning LLMs (7B‚Äì70B) on a single GPU\n",
    "\n",
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ **Two Major Tools in Hugging Face:**\n",
    "\n",
    "```\n",
    "1. Accelerate  ‚Üí Efficient training on any hardware \n",
    "2. PEFT        ‚Üí Train only small parts of model (LoRA, QLoRA)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üîµ **PART A ‚Äî ACCELERATE**\n",
    "\n",
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ **What is Accelerate?**\n",
    "\n",
    "### ‚≠ê Simple Explanation:\n",
    "\n",
    "Accelerate helps you **train models on CPU or GPU easily**, without writing complex code.\n",
    "\n",
    "### ‚≠ê Technical Explanation:\n",
    "\n",
    "* Supports distributed training\n",
    "* Mixed precision (fp16, bf16)\n",
    "* TPU support\n",
    "* Multi-GPU handling\n",
    "* Device mapping\n",
    "* Zero-code-scale training\n",
    "\n",
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ **Accelerate Installation**\n",
    "\n",
    "```bash\n",
    "pip install accelerate\n",
    "accelerate config\n",
    "```\n",
    "\n",
    "The config command helps you choose:\n",
    "\n",
    "* CPU\n",
    "* Single GPU\n",
    "* Multiple GPUs\n",
    "* Mixed precision\n",
    "\n",
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ **Accelerate Workflow Diagram**\n",
    "\n",
    "```\n",
    "Your Model & Training Code\n",
    "             ‚Üì\n",
    "     accelerate.prepare()\n",
    "             ‚Üì\n",
    " Multi-GPU / TPU / CPU Auto Handling\n",
    "             ‚Üì\n",
    "         Efficient Training\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 7Ô∏è‚É£ **Basic Accelerate Example (Technical)**\n",
    "\n",
    "```python\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "model, optimizer, train_loader = accelerator.prepare(\n",
    "    model, optimizer, train_loader\n",
    ")\n",
    "```\n",
    "\n",
    "This automatically:\n",
    "\n",
    "* moves model to GPU\n",
    "* handles mixed precision\n",
    "* handles distributed training\n",
    "\n",
    "---\n",
    "\n",
    "# 8Ô∏è‚É£ **Accelerate with Trainer**\n",
    "\n",
    "The Trainer API **already integrates** Accelerate internally.\n",
    "\n",
    "No change needed ‚Äî Accelerate is used automatically.\n",
    "\n",
    "---\n",
    "\n",
    "# üü¢ **PART B ‚Äî PEFT (Parameter-Efficient Fine-Tuning)**\n",
    "\n",
    "---\n",
    "\n",
    "# 9Ô∏è‚É£ **What is PEFT?**\n",
    "\n",
    "### ‚≠ê Beginner-Friendly:\n",
    "\n",
    "PEFT means **fine-tuning only small parts of a large model**, instead of updating all weights.\n",
    "\n",
    "This makes training:\n",
    "\n",
    "* Cheaper\n",
    "* Faster\n",
    "* Possible on normal GPUs\n",
    "\n",
    "### ‚≠ê Technical:\n",
    "\n",
    "PEFT updates **1‚Äì5%** of total parameters.\n",
    "\n",
    "Supported methods:\n",
    "\n",
    "* LoRA\n",
    "* QLoRA\n",
    "* Prefix Tuning\n",
    "* P-Tuning v2\n",
    "* Adapters\n",
    "\n",
    "---\n",
    "\n",
    "# üîü **Why PEFT is Important?**\n",
    "\n",
    "### ‚≠ê Beginners:\n",
    "\n",
    "Big models (like Llama, Mistral, GPT-J) are too heavy.\n",
    "PEFT lets you fine-tune them on a laptop or Google Colab.\n",
    "\n",
    "### ‚≠ê Technical Users:\n",
    "\n",
    "* Huge memory savings (50‚Äì80%)\n",
    "* Enables 4-bit training\n",
    "* Supports LLMs (7B‚Äì70B)\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ **PEFT Diagram (Simple)**\n",
    "\n",
    "```\n",
    "Full Model (7 Billion Params)\n",
    " ‚Üì\n",
    "Freeze 99% of weights\n",
    " ‚Üì\n",
    "Train only small LoRA layers\n",
    " ‚Üì\n",
    "Small, fast fine-tuning\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£2Ô∏è‚É£ **PEFT Techniques Explained Simply**\n",
    "\n",
    "| Technique         | Simple Meaning             | Technical Meaning             |\n",
    "| ----------------- | -------------------------- | ----------------------------- |\n",
    "| **LoRA**          | Train small extra layers   | Low-rank matrix decomposition |\n",
    "| **QLoRA**         | Train LoRA in 4-bit        | Uses NF4 quantization         |\n",
    "| **Prefix Tuning** | Add extra learnable tokens | Learnable prefix embeddings   |\n",
    "| **Adapters**      | Insert small modules       | Residual adapter layers       |\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£3Ô∏è‚É£ **LoRA Example (Beginner-Friendly)**\n",
    "\n",
    "LoRA adds small layers to the model like ‚Äúplug-ins‚Äù.\n",
    "\n",
    "Instead of updating the whole model, LoRA updates only the added plug-in layers.\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£4Ô∏è‚É£ **Technical Code: Apply LoRA to a Transformer Model**\n",
    "\n",
    "```python\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=2\n",
    ")\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n",
    "```\n",
    "\n",
    "This prints:\n",
    "\n",
    "```\n",
    "Trainable params: 1% (LoRA only)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£5Ô∏è‚É£ **QLoRA (The Most Popular Technique)**\n",
    "\n",
    "### ‚≠ê Beginner Explanation:\n",
    "\n",
    "QLoRA lets you fine-tune **very big models** using **very low memory**.\n",
    "\n",
    "### ‚≠ê Technical Explanation:\n",
    "\n",
    "* Uses 4-bit quantization (NF4)\n",
    "* Keeps base model frozen\n",
    "* Trains LoRA adapters\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£6Ô∏è‚É£ **QLoRA Code Example**\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=\"float16\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"facebook/opt-1.3b\",\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£7Ô∏è‚É£ **Advantages of Accelerate + PEFT**\n",
    "\n",
    "### ‚≠ê Beginners:\n",
    "\n",
    "* Train faster\n",
    "* Use cheap hardware\n",
    "* Learn large models easily\n",
    "\n",
    "### ‚≠ê Technical:\n",
    "\n",
    "* Minimal VRAM usage\n",
    "* Supports LLM fine-tuning\n",
    "* Multi-GPU distributed training\n",
    "* Mixed precision FP16/BF16\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£8Ô∏è‚É£ **When to Use Accelerate?**\n",
    "\n",
    "‚úî When training on multiple GPUs\n",
    "‚úî When training big models\n",
    "‚úî When you want mixed precision\n",
    "‚úî When you need distributed training\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£9Ô∏è‚É£ **When to Use PEFT?**\n",
    "\n",
    "‚úî You want to fine-tune LLMs (7B‚Äì70B)\n",
    "‚úî You have 1 GPU with 8‚Äì16 GB VRAM\n",
    "‚úî You want lightweight models for deployment\n",
    "‚úî You want low training cost\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£0Ô∏è‚É£ **Full Workflow Diagram**\n",
    "\n",
    "```\n",
    "Load Dataset\n",
    "    ‚Üì\n",
    "Load Pretrained Model\n",
    "    ‚Üì\n",
    "Apply PEFT (LoRA / QLoRA / Adapters)\n",
    "    ‚Üì\n",
    "Prepare Model using Accelerate\n",
    "    ‚Üì\n",
    "Train using Trainer or custom loop\n",
    "    ‚Üì\n",
    "Save & Push to Hugging Face Hub\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£1Ô∏è‚É£ **Real-World Use Cases**\n",
    "\n",
    "### NLP\n",
    "\n",
    "‚úî Customer-specific chatbot\n",
    "‚úî Legal domain Q&A\n",
    "‚úî Medical text classifier\n",
    "‚úî Email classification\n",
    "\n",
    "### Vision\n",
    "\n",
    "‚úî Fine-tuning ViT with LoRA\n",
    "‚úî Product defect classification\n",
    "\n",
    "### Audio\n",
    "\n",
    "‚úî Language-specific Whisper fine-tuning\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£2Ô∏è‚É£ **Beginner Activity**\n",
    "\n",
    "Ask students to:\n",
    "\n",
    "1. Load a small BERT model\n",
    "2. Apply LoRA\n",
    "3. Print trainable parameters\n",
    "4. Understand difference between full training vs PEFT\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£3Ô∏è‚É£ **Technical Exercise**\n",
    "\n",
    "‚úî Fine-tune Llama 7B using QLoRA\n",
    "‚úî Try multi-GPU training with Accelerate\n",
    "‚úî Compare fp32 vs fp16 speed differences\n",
    "‚úî Measure VRAM usage\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£4Ô∏è‚É£ **Learning Outcomes (Module 8)**\n",
    "\n",
    "After this module, students can:\n",
    "\n",
    "### ‚≠ê Beginners:\n",
    "\n",
    "‚úî Explain what efficient training is\n",
    "‚úî Understand LoRA & QLoRA in simple words\n",
    "‚úî Explain why large models need PEFT\n",
    "\n",
    "### ‚≠ê Technical:\n",
    "\n",
    "‚úî Use Accelerate for GPU training\n",
    "‚úî Apply PEFT to transformer models\n",
    "‚úî Train LLMs with QLoRA\n",
    "‚úî Perform distributed training\n",
    "‚úî Optimize memory usage\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c4e1da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ba3c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb8aa736",
   "metadata": {},
   "source": [
    "\n",
    "# üåü **MODULE 9 ‚Äì HUGGING FACE SPACES (APP DEPLOYMENT)**\n",
    "\n",
    "### *(Beginner-Friendly + Technical Detailed Notes)*\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ **What Are Hugging Face Spaces?**\n",
    "\n",
    "### ‚≠ê Simple Definition (Non-Technical):\n",
    "\n",
    "Spaces are **AI apps that run on the Hugging Face website**.\n",
    "You can create apps without any servers.\n",
    "\n",
    "### ‚≠ê Technical Definition:\n",
    "\n",
    "A Git-based hosting platform where users can deploy:\n",
    "\n",
    "* **Gradio apps**\n",
    "* **Streamlit apps**\n",
    "* **Static HTML apps**\n",
    "* **Docker apps**\n",
    "\n",
    "Spaces provide:\n",
    "\n",
    "* Free CPU\n",
    "* Optional GPU/TPU\n",
    "* Auto-deployment\n",
    "* Git versioning\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ **Why Use Spaces?**\n",
    "\n",
    "### ‚≠ê Beginner:\n",
    "\n",
    "* No need to buy cloud servers\n",
    "* You can create demos easily\n",
    "* Share your project with one link\n",
    "* Works like hosting website + AI inside\n",
    "\n",
    "### ‚≠ê Technical:\n",
    "\n",
    "* CI/CD deployment via Git\n",
    "* Private/public hosting\n",
    "* Built-in inference compute\n",
    "* Environment pinning using `requirements.txt`\n",
    "* Ideal for ML demo, prototype, production-lite apps\n",
    "\n",
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ **Types of Spaces**\n",
    "\n",
    "```\n",
    "1. Gradio     ‚Üí Build UI for AI apps easily\n",
    "2. Streamlit  ‚Üí Interactive web dashboards\n",
    "3. Static     ‚Üí HTML, CSS, JS websites\n",
    "4. Docker     ‚Üí Custom containerized apps\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ **Spaces Folder Structure**\n",
    "\n",
    "Every Space needs at least:\n",
    "\n",
    "```\n",
    "app.py               ‚Üê Main app code\n",
    "requirements.txt     ‚Üê Python libraries\n",
    "README.md            ‚Üê App description\n",
    "```\n",
    "\n",
    "Optional:\n",
    "\n",
    "```\n",
    "runtime.txt\n",
    "Dockerfile\n",
    "assets/\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ **Workflow Diagram: How Spaces Work**\n",
    "\n",
    "```\n",
    "Write Code (Gradio/Streamlit)\n",
    "            ‚Üì\n",
    "Push to Hugging Face\n",
    "            ‚Üì\n",
    "Auto Build & Deploy\n",
    "            ‚Üì\n",
    "Public App URL You Can Share\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ **Creating a Space (Beginner-Friendly)**\n",
    "\n",
    "### Step 1:\n",
    "\n",
    "Visit ‚Üí [https://huggingface.co/spaces](https://huggingface.co/spaces)\n",
    "\n",
    "### Step 2:\n",
    "\n",
    "Click: **Create new Space**\n",
    "\n",
    "### Step 3: Choose:\n",
    "\n",
    "* Gradio\n",
    "* Streamlit\n",
    "* Docker\n",
    "* Static\n",
    "\n",
    "### Step 4:\n",
    "\n",
    "Fill:\n",
    "\n",
    "* Space name\n",
    "* License\n",
    "* Public / Private\n",
    "\n",
    "### Step 5:\n",
    "\n",
    "Upload:\n",
    "\n",
    "* `app.py`\n",
    "* `requirements.txt`\n",
    "\n",
    "Deployment happens automatically.\n",
    "\n",
    "---\n",
    "\n",
    "# 7Ô∏è‚É£ **Gradio Basics (For Non-Technical Students)**\n",
    "\n",
    "Gradio makes simple UI components like:\n",
    "\n",
    "* Textbox\n",
    "* Button\n",
    "* Image upload\n",
    "* Dropdown\n",
    "* Text output\n",
    "\n",
    "Very easy to build apps.\n",
    "\n",
    "---\n",
    "\n",
    "# 8Ô∏è‚É£ **Gradio Example App (Beginner + Technical)**\n",
    "\n",
    "### ‚≠ê app.py\n",
    "\n",
    "```python\n",
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "def predict(text):\n",
    "    return classifier(text)[0]['label']\n",
    "\n",
    "iface = gr.Interface(fn=predict, inputs=\"text\", outputs=\"text\")\n",
    "iface.launch()\n",
    "```\n",
    "\n",
    "### ‚≠ê requirements.txt\n",
    "\n",
    "```\n",
    "gradio\n",
    "transformers\n",
    "torch\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 9Ô∏è‚É£ **Streamlit Example App**\n",
    "\n",
    "### ‚≠ê app.py\n",
    "\n",
    "```python\n",
    "import streamlit as st\n",
    "from transformers import pipeline\n",
    "\n",
    "st.title(\"Sentiment Analyzer\")\n",
    "model = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "text = st.text_input(\"Enter text\")\n",
    "if text:\n",
    "    result = model(text)[0]\n",
    "    st.write(result)\n",
    "```\n",
    "\n",
    "### ‚≠ê requirements.txt\n",
    "\n",
    "```\n",
    "streamlit\n",
    "transformers\n",
    "torch\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üîü **Adding GPU Support (Technical Students)**\n",
    "\n",
    "Inside the Space ‚Üí\n",
    "**Settings ‚Üí Hardware** ‚Üí choose:\n",
    "\n",
    "* CPU (Free)\n",
    "* T4 GPU\n",
    "* A10G GPU\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ **Spaces Deployment Process (Technical)**\n",
    "\n",
    "### If using Git locally:\n",
    "\n",
    "```bash\n",
    "git clone https://huggingface.co/spaces/username/myapp\n",
    "cd myapp\n",
    "git add .\n",
    "git commit -m \"first commit\"\n",
    "git push\n",
    "```\n",
    "\n",
    "Spaces auto-builds and deploys.\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£2Ô∏è‚É£ **Environment Management**\n",
    "\n",
    "### ‚úî Python version\n",
    "\n",
    "Add:\n",
    "\n",
    "```\n",
    "runtime.txt\n",
    "```\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "python-3.10\n",
    "```\n",
    "\n",
    "### ‚úî Specific versions of libraries\n",
    "\n",
    "Add:\n",
    "\n",
    "```\n",
    "transformers==4.36.0\n",
    "gradio==4.0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£3Ô∏è‚É£ **Adding Images & Files to Space**\n",
    "\n",
    "Create folder:\n",
    "\n",
    "```\n",
    "/assets\n",
    "```\n",
    "\n",
    "Store:\n",
    "\n",
    "* images\n",
    "* audio\n",
    "* pdfs\n",
    "* logos\n",
    "\n",
    "Use in app:\n",
    "\n",
    "```python\n",
    "img = \"assets/logo.png\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£4Ô∏è‚É£ **Advanced Features**\n",
    "\n",
    "### ‚≠ê Secrets / API Keys\n",
    "\n",
    "Go to:\n",
    "\n",
    "```\n",
    "Space ‚Üí Settings ‚Üí Secrets\n",
    "```\n",
    "\n",
    "Use in code:\n",
    "\n",
    "```python\n",
    "import os\n",
    "api = os.getenv(\"MY_API_KEY\")\n",
    "```\n",
    "\n",
    "### ‚≠ê Persistent Storage\n",
    "\n",
    "Use:\n",
    "\n",
    "```\n",
    "hf://\n",
    "```\n",
    "\n",
    "### ‚≠ê Live Logs\n",
    "\n",
    "Check logs in top-right menu.\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£5Ô∏è‚É£ **Space Maintenance for Students**\n",
    "\n",
    "| Task                       | Why Important       |\n",
    "| -------------------------- | ------------------- |\n",
    "| Update dependencies        | avoid errors        |\n",
    "| Check logs                 | debug failures      |\n",
    "| Add README                 | explain project     |\n",
    "| Add screenshots            | better presentation |\n",
    "| Use GPL/Apache/MIT license | legal clarity       |\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£6Ô∏è‚É£ **Real-World Apps Students Can Build**\n",
    "\n",
    "### NLP:\n",
    "\n",
    "‚úî Chatbot\n",
    "‚úî Summarizer\n",
    "‚úî Translator\n",
    "‚úî Grammar corrector\n",
    "‚úî Sentiment app\n",
    "\n",
    "### Vision:\n",
    "\n",
    "‚úî Image classifier\n",
    "‚úî Object detector\n",
    "‚úî Image generator (Stable Diffusion)\n",
    "\n",
    "### Audio:\n",
    "\n",
    "‚úî Speech-to-text app (Whisper)\n",
    "\n",
    "### Education:\n",
    "\n",
    "‚úî PDF Q&A app\n",
    "‚úî Notes summarizer\n",
    "‚úî Assignment evaluator\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£7Ô∏è‚É£ **Visual Diagram: Space Structure**\n",
    "\n",
    "```\n",
    "my-space/\n",
    "‚îú‚îÄ‚îÄ app.py\n",
    "‚îú‚îÄ‚îÄ requirements.txt\n",
    "‚îú‚îÄ‚îÄ README.md\n",
    "‚îî‚îÄ‚îÄ assets/\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£8Ô∏è‚É£ **Common Errors & Solutions**\n",
    "\n",
    "| Error                   | Reason          | Fix                     |\n",
    "| ----------------------- | --------------- | ----------------------- |\n",
    "| App stuck at ‚ÄúBuilding‚Äù | Wrong versions  | Fix requirements.txt    |\n",
    "| Red error screen        | Syntax errors   | Check app.py            |\n",
    "| Model not loading       | Missing library | Add in requirements     |\n",
    "| Space slow              | Large model     | Use smaller model / GPU |\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£9Ô∏è‚É£ **Beginner Activity**\n",
    "\n",
    "Ask students to:\n",
    "\n",
    "1. Create a Space\n",
    "2. Add simple Gradio app (text ‚Üí text)\n",
    "3. Share URL with class\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£0Ô∏è‚É£ **Technical Exercises**\n",
    "\n",
    "‚úî Build chatbot using Llama or Mistral\n",
    "‚úî Add file upload for PDF summarization\n",
    "‚úî Deploy image classifier with GPU\n",
    "‚úî Add theme customization for UI\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£1Ô∏è‚É£ **Learning Outcomes (Module 9)**\n",
    "\n",
    "After this module, students can:\n",
    "\n",
    "### ‚≠ê Beginners:\n",
    "\n",
    "‚úî Create a Space\n",
    "‚úî Deploy simple apps\n",
    "‚úî Use Gradio/Streamlit\n",
    "‚úî Share AI apps publicly\n",
    "\n",
    "### ‚≠ê Technical:\n",
    "\n",
    "‚úî Version-control apps\n",
    "‚úî Manage requirements\n",
    "‚úî Use GPU runtime\n",
    "‚úî Secure API Keys\n",
    "‚úî Build full ML prototypes\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310d912d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87379ce6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb4dad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc61810f",
   "metadata": {},
   "source": [
    "# üåü **MODULE 10 ‚Äî REAL-WORLD PROJECTS (NLP, Vision, Audio & Multimodal)**\n",
    "\n",
    "### *(Beginner-Friendly + Technical Detailed Notes)*\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ **Why Real-World Projects?**\n",
    "\n",
    "### ‚≠ê Beginner:\n",
    "\n",
    "Projects help you *see AI working in real life*.\n",
    "\n",
    "### ‚≠ê Technical:\n",
    "\n",
    "Projects combine:\n",
    "\n",
    "* Models\n",
    "* Datasets\n",
    "* Tokenizers\n",
    "* Inference workflows\n",
    "* Deployment (Spaces)\n",
    "\n",
    "This module connects all earlier modules.\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ **Project Categories**\n",
    "\n",
    "```\n",
    "1. NLP (Text)\n",
    "2. Vision (Images)\n",
    "3. Audio (Speech)\n",
    "4. Multimodal (Text + Image)\n",
    "5. Full-Stack AI Apps (UI + Backend)\n",
    "```\n",
    "\n",
    "Each project can be:\n",
    "\n",
    "* Basic (for freshers)\n",
    "* Intermediate\n",
    "* Advanced (for technical learners)\n",
    "\n",
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ **NLP PROJECTS (Text-Based)**\n",
    "\n",
    "## üü¶ **Project 1: Sentiment Analysis App**\n",
    "\n",
    "### ‚≠ê Beginner:\n",
    "\n",
    "* Input: Text\n",
    "* Output: Positive / Negative\n",
    "\n",
    "### ‚≠ê Technical:\n",
    "\n",
    "Use DistilBERT:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "sentiment = pipeline(\"sentiment-analysis\")\n",
    "sentiment(\"I love Hugging Face!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üü© **Project 2: Text Summarization Tool**\n",
    "\n",
    "### ‚≠ê Beginner:\n",
    "\n",
    "Summarizes long text into short form.\n",
    "\n",
    "### ‚≠ê Technical:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "summ = pipeline(\"summarization\")\n",
    "summ(long_text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üüß **Project 3: English ‚Üí Hindi Translator**\n",
    "\n",
    "### ‚≠ê Beginner:\n",
    "\n",
    "Convert English sentences to Hindi.\n",
    "\n",
    "### ‚≠ê Technical:\n",
    "\n",
    "```python\n",
    "translator = pipeline(\"translation_en_to_hi\")\n",
    "translator(\"Artificial intelligence is the future.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üü• **Project 4: Domain Chatbot**\n",
    "\n",
    "### ‚≠ê Beginner:\n",
    "\n",
    "Chatbot for:\n",
    "\n",
    "* Education\n",
    "* Healthcare\n",
    "* Banking\n",
    "* Travel\n",
    "\n",
    "### ‚≠ê Technical:\n",
    "\n",
    "Use Q&A + RAG:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "qa = pipeline(\"question-answering\")\n",
    "```\n",
    "\n",
    "Add FAISS/Chroma for RAG retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ **VISION PROJECTS (Image-Based)**\n",
    "\n",
    "## üü¶ **Project 5: Image Classification App**\n",
    "\n",
    "Example: Dog vs Cat\n",
    "\n",
    "### ‚≠ê Technical:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "model = pipeline(\"image-classification\")\n",
    "model(\"cat.jpg\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üü© **Project 6: Object Detection**\n",
    "\n",
    "Detect objects in user-uploaded images.\n",
    "\n",
    "### ‚≠ê Technical:\n",
    "\n",
    "```python\n",
    "detector = pipeline(\"object-detection\")\n",
    "detector(\"street.jpg\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üüß **Project 7: Image Captioning**\n",
    "\n",
    "Generate captions from images.\n",
    "\n",
    "### ‚≠ê Technical:\n",
    "\n",
    "```python\n",
    "captioner = pipeline(\"image-to-text\")\n",
    "captioner(\"dog.jpg\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üü• **Project 8: Image Generator Using Stable Diffusion**\n",
    "\n",
    "Generate images with prompts.\n",
    "\n",
    "### ‚≠ê Technical:\n",
    "\n",
    "```python\n",
    "from diffusers import DiffusionPipeline\n",
    "pipe = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
    "pipe(\"A cute robot teaching AI\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ **AUDIO PROJECTS**\n",
    "\n",
    "## üü¶ **Project 9: Speech-to-Text App**\n",
    "\n",
    "Convert voice to text.\n",
    "\n",
    "### ‚≠ê Technical:\n",
    "\n",
    "```python\n",
    "asr = pipeline(\"automatic-speech-recognition\")\n",
    "asr(\"audio.wav\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üü© **Project 10: Emotion Detection from Voice**\n",
    "\n",
    "Uses audio classification models.\n",
    "\n",
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ **MULTIMODAL PROJECTS**\n",
    "\n",
    "## üü¶ **Project 11: Image-Based Q&A App**\n",
    "\n",
    "Ask a question about an image.\n",
    "\n",
    "### ‚≠ê Technical:\n",
    "\n",
    "Models like:\n",
    "\n",
    "* BLIP\n",
    "* LLaVA\n",
    "* Donut\n",
    "\n",
    "---\n",
    "\n",
    "## üü© **Project 12: PDF Q&A App**\n",
    "\n",
    "Upload PDF ‚Üí Ask questions ‚Üí Get answers.\n",
    "\n",
    "Workflow:\n",
    "\n",
    "1. Extract text from PDF\n",
    "2. Chunk & store in vector DB (FAISS/Chroma)\n",
    "3. Ask questions\n",
    "4. Model answers from chunks\n",
    "\n",
    "---\n",
    "\n",
    "## üüß **Project 13: Multimodal Chatbot**\n",
    "\n",
    "Combine:\n",
    "\n",
    "* Text\n",
    "* Image\n",
    "* Audio\n",
    "\n",
    "Use models like:\n",
    "\n",
    "* FLAN-T5\n",
    "* LLaVA\n",
    "* Whisper\n",
    "\n",
    "---\n",
    "\n",
    "# 7Ô∏è‚É£ **FULL APP PROJECTS (End-to-End)**\n",
    "\n",
    "Below are full-stack projects combining:\n",
    "\n",
    "* Model\n",
    "* Dataset\n",
    "* Tokenizer\n",
    "* Training\n",
    "* App deployment (Spaces)\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ **Project 14: News Summarizer + Sentiment Dashboard**\n",
    "\n",
    "### Features:\n",
    "\n",
    "* Paste any news article\n",
    "* Summarized result\n",
    "* Sentiment score\n",
    "* Deploy on Spaces (Gradio)\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ **Project 15: Resume Analyzer**\n",
    "\n",
    "### Features:\n",
    "\n",
    "* Upload resume\n",
    "* Extract skills\n",
    "* Match with JD\n",
    "* Provide scoring\n",
    "\n",
    "Uses:\n",
    "\n",
    "* Tokenizer\n",
    "* Text classification\n",
    "* Summarization\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ **Project 16: Exam MCQ Generator**\n",
    "\n",
    "Enter topic ‚Üí Generate MCQs\n",
    "Use:\n",
    "\n",
    "* Text generation (T5/GPT2)\n",
    "* Inference pipelines\n",
    "\n",
    "---\n",
    "\n",
    "# 8Ô∏è‚É£ **Project Templates (Technical)**\n",
    "\n",
    "### ‚≠ê Basic Gradio Template\n",
    "\n",
    "```python\n",
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "model = pipeline(\"text-classification\")\n",
    "\n",
    "def predict(text):\n",
    "    return model(text)[0]['label']\n",
    "\n",
    "gr.Interface(fn=predict, inputs=\"text\", outputs=\"text\").launch()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚≠ê Basic Streamlit Template\n",
    "\n",
    "```python\n",
    "import streamlit as st\n",
    "from transformers import pipeline\n",
    "\n",
    "st.title(\"Text Classifier\")\n",
    "cls = pipeline(\"text-classification\")\n",
    "\n",
    "txt = st.text_input(\"Enter text:\")\n",
    "if txt:\n",
    "    st.write(cls(txt))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚≠ê Image App Deployment\n",
    "\n",
    "```python\n",
    "img_model = pipeline(\"image-classification\")\n",
    "\n",
    "def classify(image):\n",
    "    return img_model(image)\n",
    "\n",
    "gr.Interface(fn=classify, inputs=\"image\", outputs=\"label\").launch()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 9Ô∏è‚É£ **Projects for Beginners (Zero-Code)**\n",
    "\n",
    "Students can use Hugging Face website directly:\n",
    "\n",
    "### ‚úî Try: Sentiment models\n",
    "\n",
    "### ‚úî Try: Translation widgets\n",
    "\n",
    "### ‚úî Try: Image classifier Spaces\n",
    "\n",
    "### ‚úî Try: Audio ASR inside Spaces\n",
    "\n",
    "---\n",
    "\n",
    "# üîü **Projects for Technical Learners**\n",
    "\n",
    "### ‚úî Fine-tune DistilBERT on custom data\n",
    "\n",
    "### ‚úî Build QLoRA Llama chatbot\n",
    "\n",
    "### ‚úî Deploy Stable Diffusion app\n",
    "\n",
    "### ‚úî Create a multimodal Space\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ **Real-World Workflow Diagram**\n",
    "\n",
    "```\n",
    "Choose Task\n",
    "     ‚Üì\n",
    "Pick Dataset\n",
    "     ‚Üì\n",
    "Choose Pretrained Model\n",
    "     ‚Üì\n",
    "Tokenize & Preprocess\n",
    "     ‚Üì\n",
    "Fine-Tune (optional)\n",
    "     ‚Üì\n",
    "Inference App (Gradio/Streamlit)\n",
    "     ‚Üì\n",
    "Deploy to Hugging Face Spaces\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£2Ô∏è‚É£ **Beginner Activity**\n",
    "\n",
    "Ask students to build:\n",
    "‚úî A sentiment analyzer\n",
    "‚úî A translator\n",
    "‚úî An image classifier\n",
    "\n",
    "Give them:\n",
    "\n",
    "* Dummy dataset\n",
    "* Prewritten code\n",
    "* Step-by-step instructions\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£3Ô∏è‚É£ **Technical Tasks**\n",
    "\n",
    "‚úî Deploy a multimodal app\n",
    "‚úî Use PEFT to fine-tune Llama\n",
    "‚úî Use FAISS for a PDF Q&A tool\n",
    "‚úî Build a GPU-based Stable Diffusion app\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£4Ô∏è‚É£ **Learning Outcomes (Module 10)**\n",
    "\n",
    "After this module, students can:\n",
    "\n",
    "### ‚≠ê Beginners:\n",
    "\n",
    "‚úî Understand AI project structure\n",
    "‚úî Run simple NLP, Vision, Audio models\n",
    "‚úî Use Gradio/Streamlit apps\n",
    "‚úî Share AI apps publicly\n",
    "\n",
    "### ‚≠ê Technical:\n",
    "\n",
    "‚úî Build end-to-end AI apps\n",
    "‚úî Use pipelines, Tokenizers, AutoModels\n",
    "‚úî Fine-tune for specific tasks\n",
    "‚úî Deploy full apps using Spaces\n",
    "‚úî Create multimodal systems\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d311ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3783298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c620c512",
   "metadata": {},
   "source": [
    "all\n",
    "<!-- \n",
    "\n",
    "## **Module 1: Introduction to Hugging Face**\n",
    "\n",
    "**Goal:** Understand the HF ecosystem, purpose, and core libraries.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* What is Hugging Face?\n",
    "* Evolution from transformers to full GenAI ecosystem\n",
    "* Libraries overview\n",
    "\n",
    "  * `transformers`\n",
    "  * `diffusers`\n",
    "  * `datasets`\n",
    "  * `tokenizers`\n",
    "  * `accelerate`\n",
    "  * `peft`\n",
    "  * `gradio` & `streamlit` (Spaces)\n",
    "\n",
    "### **Demo**\n",
    "\n",
    "* Visit: [https://huggingface.co](https://huggingface.co)\n",
    "* Show trending models & community spaces.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## **Module 2: Hugging Face Hub**\n",
    "\n",
    "**Goal:** Learn to access and use models, datasets, and Spaces.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* Model Hub\n",
    "* Dataset Hub\n",
    "* Spaces Hub\n",
    "* Model cards\n",
    "* Search & filtering\n",
    "* Licensing + safe model usage\n",
    "\n",
    "### **Hands-on**\n",
    "\n",
    "```python\n",
    "from huggingface_hub import login, hf_hub_download\n",
    "login()\n",
    "hf_hub_download(repo_id=\"bert-base-uncased\", filename=\"config.json\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 3: Transformers Library**\n",
    "\n",
    "**Goal:** Understand transformer models & use pipelines.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* Importance of Transformers architecture\n",
    "* Pretrained models & checkpoints\n",
    "* Auto classes:\n",
    "\n",
    "  * `AutoModel`, `AutoTokenizer`, `AutoModelForSequenceClassification`, etc.\n",
    "* Pipelines (easy inference interface)\n",
    "\n",
    "### **Demo code**\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "sentiment = pipeline(\"sentiment-analysis\")\n",
    "sentiment(\"Hugging Face makes AI easy!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 4: Tokenizers**\n",
    "\n",
    "**Goal:** Deep understanding of tokenization.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* Why tokenization matters\n",
    "* Types:\n",
    "\n",
    "  * WordPiece\n",
    "  * BPE\n",
    "  * SentencePiece\n",
    "  * Unigram\n",
    "* Special tokens: PAD, CLS, SEP\n",
    "* Fast tokenizers (Rust-backed)\n",
    "\n",
    "### **Hands-on**\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tok(\"Hello Hugging Face!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 5: Inference with Transformers**\n",
    "\n",
    "**Goal:** Use models for real tasks.\n",
    "\n",
    "### **Tasks**\n",
    "\n",
    "* Text classification\n",
    "* Text generation\n",
    "* Named Entity Recognition\n",
    "* Question Answering\n",
    "* Translation\n",
    "* Summarization\n",
    "\n",
    "### **Quick demo**\n",
    "\n",
    "```python\n",
    "gen = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "gen(\"Explain Hugging Face in 10 words:\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 6: Fine-Tuning**\n",
    "\n",
    "**Goal:** Train your own models on custom datasets.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* Full vs. partial training\n",
    "* Trainer API\n",
    "* TrainingArguments\n",
    "* Metrics: accuracy, F1, BLEU, ROUGE\n",
    "* Building datasets (CSV/JSON/Parquet)\n",
    "\n",
    "### **Example Fine-tuning Script**\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "args = TrainingArguments(\"output\", evaluation_strategy=\"epoch\")\n",
    "trainer = Trainer(model=model, args=args)\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 7: Datasets Library**\n",
    "\n",
    "**Goal:** Learn to load, clean, and preprocess NLP datasets.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* Load from Hub\n",
    "* Map, filter, split\n",
    "* Tokenization with datasets\n",
    "* Streaming large datasets\n",
    "* Data collators\n",
    "\n",
    "### **Demo**\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"imdb\")\n",
    "print(ds[\"train\"][0])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 8: Accelerate & PEFT**\n",
    "\n",
    "**Goal:** Efficient training on low compute.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* Why efficient training? (cost + speed)\n",
    "* `accelerate` for device mapping & multi-GPU\n",
    "* PEFT:\n",
    "\n",
    "  * LoRA\n",
    "  * Prefix tuning\n",
    "  * QLoRA\n",
    "* Small models with big results\n",
    "\n",
    "### **Example**\n",
    "\n",
    "```python\n",
    "from peft import LoraConfig, get_peft_model\n",
    "config = LoraConfig(task_type=\"SEQ_CLS\")\n",
    "peft_model = get_peft_model(model, config)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 9: Hugging Face Spaces**\n",
    "\n",
    "**Goal:** Deploy real GenAI apps.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* What are Spaces?\n",
    "* Gradio apps\n",
    "* Streamlit apps\n",
    "* Repo structure:\n",
    "\n",
    "  * `app.py`\n",
    "  * `requirements.txt`\n",
    "* GPU vs CPU spaces\n",
    "* Public/Private deployments\n",
    "\n",
    "### **Gradio demo**\n",
    "\n",
    "```python\n",
    "import gradio as gr\n",
    "def greet(text): return \"Hello \" + text\n",
    "gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\").launch()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 10: Real-World GenAI Projects**\n",
    "\n",
    "**Goal:** Build industry-standard applications.\n",
    "\n",
    "### **Project Examples**\n",
    "\n",
    "1. **Sentiment Analyzer** (Transformers)\n",
    "2. **Text Summarizer** (T5/Falcon/Mistral)\n",
    "3. **Image Generator App** (Diffusers + Gradio)\n",
    "4. **RAG-based Q&A System**\n",
    "5. **Text-to-SQL chatbot**\n",
    "6. **Document classification app**\n",
    "7. **Audio Transcription App** (Whisper)\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 11: Deployment & Sharing**\n",
    "\n",
    "**Goal:** Publish models, datasets, and applications.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* Creating model cards\n",
    "* Uploading models\n",
    "* Versioning\n",
    "* Using Inference API\n",
    "* How to share Spaces\n",
    "* API rate-limits and considerations\n",
    "\n",
    "### **Upload Example**\n",
    "\n",
    "```python\n",
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"pytorch_model.bin\",\n",
    "    path_in_repo=\"pytorch_model.bin\",\n",
    "    repo_id=\"username/my-model\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ **Complete Course Outcomes**\n",
    "\n",
    "By the end, students can:\n",
    "\n",
    "‚úî Use HF Hub, datasets, and pretrained models\n",
    "‚úî Build text/image/audio GenAI apps\n",
    "‚úî Fine-tune transformer models\n",
    "‚úî Deploy apps on HuggingFace Spaces\n",
    "‚úî Publish & share professional models and demos\n",
    "\n",
    "---\n",
    "\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21789b35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "388575b4",
   "metadata": {},
   "source": [
    "\n",
    "# üåü **MODULE 11 ‚Äî DEPLOYMENT & SHARING**\n",
    "\n",
    "### *(Model Cards, Hub Push, Versioning, Inference API)*\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ **What Does ‚ÄúDeployment & Sharing‚Äù Mean?**\n",
    "\n",
    "### ‚≠ê Simple Definition (Non-Technical):\n",
    "\n",
    "Deployment means **publishing your AI model or app so others can use it**.\n",
    "\n",
    "Sharing means **uploading it to Hugging Face Hub** for easy access.\n",
    "\n",
    "### ‚≠ê Technical Definition:\n",
    "\n",
    "Deployment involves:\n",
    "\n",
    "* Saving model & tokenizer files\n",
    "* Creating a model repository\n",
    "* Writing a model card\n",
    "* Pushing weights, config, and tokenizer\n",
    "* Exposing inference APIs\n",
    "* Version control & access management\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ **Why Deploy Models?**\n",
    "\n",
    "### ‚≠ê Beginner:\n",
    "\n",
    "* Show your work\n",
    "* Share projects with teachers/friends\n",
    "* Useful for resume / portfolio\n",
    "\n",
    "### ‚≠ê Technical:\n",
    "\n",
    "* Enables reproducible research\n",
    "* CI/CD workflows\n",
    "* Team collaboration\n",
    "* API-based integration\n",
    "* Public or private storage\n",
    "\n",
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ **Three Things You Can Deploy on HF Hub**\n",
    "\n",
    "```\n",
    "1. Models     ‚Üí Transformer weights  \n",
    "2. Datasets   ‚Üí Custom data  \n",
    "3. Spaces     ‚Üí Apps (Gradio / Streamlit)\n",
    "```\n",
    "\n",
    "Module 9 covered Spaces.\n",
    "Now we focus on **Models** and **Inference API**.\n",
    "\n",
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ **Model Repository Structure**\n",
    "\n",
    "When you fine-tune and save a model, it generates:\n",
    "\n",
    "```\n",
    "my-model/\n",
    "‚îú‚îÄ‚îÄ config.json\n",
    "‚îú‚îÄ‚îÄ pytorch_model.bin\n",
    "‚îú‚îÄ‚îÄ model.safetensors\n",
    "‚îú‚îÄ‚îÄ tokenizer.json\n",
    "‚îú‚îÄ‚îÄ tokenizer_config.json\n",
    "‚îú‚îÄ‚îÄ vocab.txt (if WordPiece)\n",
    "‚îú‚îÄ‚îÄ special_tokens_map.json\n",
    "‚îî‚îÄ‚îÄ training_args.bin\n",
    "```\n",
    "\n",
    "### ‚≠ê Beginner Explanation:\n",
    "\n",
    "These files contain:\n",
    "\n",
    "* Model settings\n",
    "* Model brain\n",
    "* Tokenizer\n",
    "* Special tokens\n",
    "\n",
    "### ‚≠ê Technical Explanation:\n",
    "\n",
    "Used by:\n",
    "\n",
    "* `AutoModel`\n",
    "* `AutoTokenizer`\n",
    "* Inference API\n",
    "* Pipeline()\n",
    "\n",
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ **Creating a New Repository on Hugging Face Hub**\n",
    "\n",
    "### ‚≠ê Method 1: Using the Website\n",
    "\n",
    "1. Go to: [https://huggingface.co/new](https://huggingface.co/new)\n",
    "2. Select:\n",
    "\n",
    "   * Model / Dataset / Space\n",
    "3. Enter name\n",
    "4. Choose `Public` or `Private`\n",
    "5. Create repository\n",
    "\n",
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ **Logging In Programmatically**\n",
    "\n",
    "### Install\n",
    "\n",
    "```bash\n",
    "pip install huggingface_hub\n",
    "```\n",
    "\n",
    "### Login\n",
    "\n",
    "```python\n",
    "from huggingface_hub import login\n",
    "login()\n",
    "```\n",
    "\n",
    "OR CLI:\n",
    "\n",
    "```bash\n",
    "huggingface-cli login\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 7Ô∏è‚É£ **Pushing a Fine-Tuned Model to Hub**\n",
    "\n",
    "### ‚≠ê Using Transformers Trainer API (Beginner-Friendly)\n",
    "\n",
    "After training:\n",
    "\n",
    "```python\n",
    "trainer.push_to_hub(\"sentiment-model\")\n",
    "```\n",
    "\n",
    "Everything uploads automatically:\n",
    "\n",
    "* config\n",
    "* tokenizer\n",
    "* weights\n",
    "* model card\n",
    "\n",
    "---\n",
    "\n",
    "### ‚≠ê Manual Upload (Technical)\n",
    "\n",
    "```python\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "api.create_repo(\"username/my-model\")\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=\"sentiment_model\",\n",
    "    repo_id=\"username/my-model\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 8Ô∏è‚É£ **Writing a Model Card (README.md)**\n",
    "\n",
    "A Model Card describes:\n",
    "\n",
    "* What the model does\n",
    "* How it was trained\n",
    "* Dataset used\n",
    "* Intended use cases\n",
    "* Limitations\n",
    "* Ethical considerations\n",
    "\n",
    "### ‚≠ê Template\n",
    "\n",
    "````\n",
    "# Model Name\n",
    "\n",
    "## üß† Model Description\n",
    "Short explanation of what this model does.\n",
    "\n",
    "## üèãÔ∏è Training Details\n",
    "- Dataset:\n",
    "- Epochs:\n",
    "- Learning Rate:\n",
    "\n",
    "## üìÇ Model Files\n",
    "- config.json\n",
    "- pytorch_model.bin\n",
    "- tokenizer.json\n",
    "\n",
    "## üöÄ Usage\n",
    "```python\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-classification\", model=\"username/my-model\")\n",
    "````\n",
    "\n",
    "## ‚ö†Ô∏è Limitations\n",
    "\n",
    "State known problems.\n",
    "\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "# 9Ô∏è‚É£ **Version Control on Hugging Face**\n",
    "\n",
    "HF uses **Git-LFS** for storing large model files.\n",
    "\n",
    "### ‚≠ê Technical Steps\n",
    "```bash\n",
    "git lfs install\n",
    "git clone https://huggingface.co/username/my-model\n",
    "cd my-model\n",
    "git add .\n",
    "git commit -m \"update\"\n",
    "git push\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "# üîü **Model Access: Public vs Private**\n",
    "\n",
    "### ‚≠ê Public:\n",
    "\n",
    "* Anyone can use\n",
    "* Appears in search\n",
    "* Good for portfolio\n",
    "\n",
    "### ‚≠ê Private:\n",
    "\n",
    "* Only specific users can access\n",
    "* Good for company data/models\n",
    "\n",
    "Control access via:\n",
    "\n",
    "```\n",
    "Settings ‚Üí Manage Collaborators\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ **Inference API (Use Model as REST API)**\n",
    "\n",
    "Once your model is deployed, HF provides an **API endpoint**.\n",
    "\n",
    "### ‚≠ê Beginner:\n",
    "\n",
    "Use it like any web service.\n",
    "\n",
    "### ‚≠ê Technical Example:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/username/my-model\"\n",
    "headers = {\"Authorization\": \"Bearer YOUR_API_KEY\"}\n",
    "\n",
    "payload = {\"inputs\": \"This is awesome!\"}\n",
    "res = requests.post(API_URL, headers=headers, json=payload)\n",
    "print(res.json())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£2Ô∏è‚É£ **Widgets (No-Code Model Demo)**\n",
    "\n",
    "Each model page includes:\n",
    "\n",
    "* A browser widget\n",
    "* Zero-code testing\n",
    "\n",
    "Students can:\n",
    "\n",
    "* Type text\n",
    "* Upload image/audio\n",
    "* See prediction\n",
    "* No programming needed\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£3Ô∏è‚É£ **Deploying a Full Application + Model**\n",
    "\n",
    "Steps:\n",
    "\n",
    "```\n",
    "1. Fine-tune model\n",
    "2. Push to hub\n",
    "3. Create Gradio app using your model\n",
    "4. Deploy to Spaces\n",
    "```\n",
    "\n",
    "Example Gradio loading your HF model:\n",
    "\n",
    "```python\n",
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"username/my-model\")\n",
    "\n",
    "def predict(text):\n",
    "    return classifier(text)\n",
    "\n",
    "gr.Interface(predict, \"text\", \"label\").launch()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£4Ô∏è‚É£ **API Keys & Security**\n",
    "\n",
    "### Get API Key:\n",
    "\n",
    "Go to:\n",
    "\n",
    "```\n",
    "Settings ‚Üí Access Tokens\n",
    "```\n",
    "\n",
    "### Use securely:\n",
    "\n",
    "Store in `.env`, Secrets, or OS variables.\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£5Ô∏è‚É£ **Monitoring Models (Technical Users)**\n",
    "\n",
    "HF Dashboard allows:\n",
    "\n",
    "* Download stats\n",
    "* API usage\n",
    "* Space performance\n",
    "* Logs\n",
    "* Hardware monitoring\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£6Ô∏è‚É£ **Common Errors & Solutions**\n",
    "\n",
    "| Error              | Cause              | Fix                     |\n",
    "| ------------------ | ------------------ | ----------------------- |\n",
    "| Model not loading  | Missing tokenizer  | Upload tokenizer files  |\n",
    "| API rate limit     | Free plan exceeded | Use higher plan         |\n",
    "| File too large     | Missing Git-LFS    | Install git-lfs         |\n",
    "| Inference too slow | Large model        | Use smaller model / GPU |\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£7Ô∏è‚É£ **Beginner Activity**\n",
    "\n",
    "Ask students to:\n",
    "\n",
    "1. Fine-tune DistilBERT on IMDB\n",
    "2. Upload model to Hub\n",
    "3. Write a model card\n",
    "4. Test in Inference Widget\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£8Ô∏è‚É£ **Advanced Technical Tasks**\n",
    "\n",
    "‚úî Build API using HF Inference Endpoints\n",
    "‚úî Create private model for enterprise\n",
    "‚úî Enable GPU acceleration for inference\n",
    "‚úî Use Docker Spaces for custom backends\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£9Ô∏è‚É£ **Deployment Workflow Diagram**\n",
    "\n",
    "```\n",
    "Fine-Tuned Model\n",
    "      ‚Üì\n",
    "Save Model Locally\n",
    "      ‚Üì\n",
    "Push to Hugging Face Hub\n",
    "      ‚Üì\n",
    "Model Repository Created\n",
    "      ‚Üì\n",
    "Test via Inference API / Widget\n",
    "      ‚Üì\n",
    "Integrate into Apps / Spaces\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£0Ô∏è‚É£ **Learning Outcomes (Module 11)**\n",
    "\n",
    "After this module, students can:\n",
    "\n",
    "### ‚≠ê Beginners:\n",
    "\n",
    "‚úî Publish models to the Hub\n",
    "‚úî Write a simple model card\n",
    "‚úî Use Inference Widget\n",
    "‚úî Test model as API\n",
    "\n",
    "### ‚≠ê Technical Users:\n",
    "\n",
    "‚úî Upload custom fine-tuned models\n",
    "‚úî Use Git-LFS & versioning\n",
    "‚úî Access models via Python/REST\n",
    "‚úî Deploy complete apps in Spaces\n",
    "‚úî Manage private/public access\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e94eac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f66be0c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58077cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1ac8d4c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **Module 2: Hugging Face Hub**\n",
    "\n",
    "**Goal:** Learn to access and use models, datasets, and Spaces.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* Model Hub\n",
    "* Dataset Hub\n",
    "* Spaces Hub\n",
    "* Model cards\n",
    "* Search & filtering\n",
    "* Licensing + safe model usage\n",
    "\n",
    "### **Hands-on**\n",
    "\n",
    "```python\n",
    "from huggingface_hub import login, hf_hub_download\n",
    "login()\n",
    "hf_hub_download(repo_id=\"bert-base-uncased\", filename=\"config.json\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 3: Transformers Library**\n",
    "\n",
    "**Goal:** Understand transformer models & use pipelines.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* Importance of Transformers architecture\n",
    "* Pretrained models & checkpoints\n",
    "* Auto classes:\n",
    "\n",
    "  * `AutoModel`, `AutoTokenizer`, `AutoModelForSequenceClassification`, etc.\n",
    "* Pipelines (easy inference interface)\n",
    "\n",
    "### **Demo code**\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "sentiment = pipeline(\"sentiment-analysis\")\n",
    "sentiment(\"Hugging Face makes AI easy!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 4: Tokenizers**\n",
    "\n",
    "**Goal:** Deep understanding of tokenization.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* Why tokenization matters\n",
    "* Types:\n",
    "\n",
    "  * WordPiece\n",
    "  * BPE\n",
    "  * SentencePiece\n",
    "  * Unigram\n",
    "* Special tokens: PAD, CLS, SEP\n",
    "* Fast tokenizers (Rust-backed)\n",
    "\n",
    "### **Hands-on**\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tok(\"Hello Hugging Face!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 5: Inference with Transformers**\n",
    "\n",
    "**Goal:** Use models for real tasks.\n",
    "\n",
    "### **Tasks**\n",
    "\n",
    "* Text classification\n",
    "* Text generation\n",
    "* Named Entity Recognition\n",
    "* Question Answering\n",
    "* Translation\n",
    "* Summarization\n",
    "\n",
    "### **Quick demo**\n",
    "\n",
    "```python\n",
    "gen = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "gen(\"Explain Hugging Face in 10 words:\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 6: Fine-Tuning**\n",
    "\n",
    "**Goal:** Train your own models on custom datasets.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* Full vs. partial training\n",
    "* Trainer API\n",
    "* TrainingArguments\n",
    "* Metrics: accuracy, F1, BLEU, ROUGE\n",
    "* Building datasets (CSV/JSON/Parquet)\n",
    "\n",
    "### **Example Fine-tuning Script**\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "args = TrainingArguments(\"output\", evaluation_strategy=\"epoch\")\n",
    "trainer = Trainer(model=model, args=args)\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 7: Datasets Library**\n",
    "\n",
    "**Goal:** Learn to load, clean, and preprocess NLP datasets.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* Load from Hub\n",
    "* Map, filter, split\n",
    "* Tokenization with datasets\n",
    "* Streaming large datasets\n",
    "* Data collators\n",
    "\n",
    "### **Demo**\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"imdb\")\n",
    "print(ds[\"train\"][0])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 8: Accelerate & PEFT**\n",
    "\n",
    "**Goal:** Efficient training on low compute.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* Why efficient training? (cost + speed)\n",
    "* `accelerate` for device mapping & multi-GPU\n",
    "* PEFT:\n",
    "\n",
    "  * LoRA\n",
    "  * Prefix tuning\n",
    "  * QLoRA\n",
    "* Small models with big results\n",
    "\n",
    "### **Example**\n",
    "\n",
    "```python\n",
    "from peft import LoraConfig, get_peft_model\n",
    "config = LoraConfig(task_type=\"SEQ_CLS\")\n",
    "peft_model = get_peft_model(model, config)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 9: Hugging Face Spaces**\n",
    "\n",
    "**Goal:** Deploy real GenAI apps.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* What are Spaces?\n",
    "* Gradio apps\n",
    "* Streamlit apps\n",
    "* Repo structure:\n",
    "\n",
    "  * `app.py`\n",
    "  * `requirements.txt`\n",
    "* GPU vs CPU spaces\n",
    "* Public/Private deployments\n",
    "\n",
    "### **Gradio demo**\n",
    "\n",
    "```python\n",
    "import gradio as gr\n",
    "def greet(text): return \"Hello \" + text\n",
    "gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\").launch()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 10: Real-World GenAI Projects**\n",
    "\n",
    "**Goal:** Build industry-standard applications.\n",
    "\n",
    "### **Project Examples**\n",
    "\n",
    "1. **Sentiment Analyzer** (Transformers)\n",
    "2. **Text Summarizer** (T5/Falcon/Mistral)\n",
    "3. **Image Generator App** (Diffusers + Gradio)\n",
    "4. **RAG-based Q&A System**\n",
    "5. **Text-to-SQL chatbot**\n",
    "6. **Document classification app**\n",
    "7. **Audio Transcription App** (Whisper)\n",
    "\n",
    "---\n",
    "\n",
    "## **Module 11: Deployment & Sharing**\n",
    "\n",
    "**Goal:** Publish models, datasets, and applications.\n",
    "\n",
    "### **Topics**\n",
    "\n",
    "* Creating model cards\n",
    "* Uploading models\n",
    "* Versioning\n",
    "* Using Inference API\n",
    "* How to share Spaces\n",
    "* API rate-limits and considerations\n",
    "\n",
    "### **Upload Example**\n",
    "\n",
    "```python\n",
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"pytorch_model.bin\",\n",
    "    path_in_repo=\"pytorch_model.bin\",\n",
    "    repo_id=\"username/my-model\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ **Complete Course Outcomes**\n",
    "\n",
    "By the end, students can:\n",
    "\n",
    "‚úî Use HF Hub, datasets, and pretrained models\n",
    "‚úî Build text/image/audio GenAI apps\n",
    "‚úî Fine-tune transformer models\n",
    "‚úî Deploy apps on HuggingFace Spaces\n",
    "‚úî Publish & share professional models and demos\n",
    "\n",
    "---\n",
    "\n",
    "# Want me to prepare next?\n",
    "\n",
    "I can generate any of the following:\n",
    "\n",
    "üìå **Full teaching slides (PPT)**\n",
    "üìå **PDF notes for each module**\n",
    "üìå **Hands-on assignments + solutions**\n",
    "üìå **End-to-end real-world project notebooks**\n",
    "üìå **Full GitHub-ready course folder (code + datasets + Spaces)**\n",
    "\n",
    "Tell me what you want, and I‚Äôll create it.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
