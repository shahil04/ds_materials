{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493ec515",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbbea0ef",
   "metadata": {},
   "source": [
    "\n",
    "# üöÄ Run Open-Source LLMs in Google Colab\n",
    "\n",
    "## (LLaMA, Mistral, Falcon, Qwen)\n",
    "\n",
    "### With Real-World Business Use Cases\n",
    "\n",
    "---\n",
    "\n",
    "## üß† What You‚Äôll Achieve in Colab\n",
    "\n",
    "‚úî Run **multiple LLMs** in one notebook\n",
    "‚úî Use **free Colab GPU (T4)**\n",
    "‚úî Show **real business demos**\n",
    "‚úî No fine-tuning required\n",
    "‚úî Teaching + interview ready\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Models You‚Äôll Run in Colab\n",
    "\n",
    "![Image](https://miro.medium.com/1%2AZbnVUpK5pw5iJJeeiBa-9w.png)\n",
    "\n",
    "![Image](https://datasciencedojo.com/wp-content/uploads/Mistral-7B-Architecture-Key-Features-1030x554.png)\n",
    "\n",
    "![Image](https://media.geeksforgeeks.org/wp-content/uploads/20240305174206/Falcon-AI.png)\n",
    "\n",
    "![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/1%2A4uJOERECtpreVHFnqSrV_Q.jpeg)\n",
    "\n",
    "| Model                | Why Use It                       |\n",
    "| -------------------- | -------------------------------- |\n",
    "| **LLaMA-2 / 3 (7B)** | Enterprise-grade                 |\n",
    "| **Mistral-7B**       | Fast & lightweight               |\n",
    "| **Falcon-7B**        | Long text summarization          |\n",
    "| **Qwen-7B**          | Multilingual & structured output |\n",
    "\n",
    "All models loaded from üëá\n",
    "\n",
    "### üü¢ **Hugging Face**\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Colab Setup (One-Time)\n",
    "\n",
    "### Step 1Ô∏è‚É£: Enable GPU\n",
    "\n",
    "```\n",
    "Runtime ‚Üí Change runtime type ‚Üí GPU (T4)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2Ô∏è‚É£: Install Required Libraries\n",
    "\n",
    "```python\n",
    "!pip install -q transformers accelerate bitsandbytes sentencepiece\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Universal Model Loader (Reusable for ALL Models)\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "def load_model(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        load_in_4bit=True,\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    return tokenizer, model\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Model 1: **Mistral-7B** (Best for Live Demo)\n",
    "\n",
    "### üî• Real-World Use Case: **Customer Support Bot**\n",
    "\n",
    "```python\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer, model = load_model(model_name)\n",
    "\n",
    "prompt = \"\"\"You are a customer support assistant.\n",
    "Customer: My product is delayed. What should I do?\n",
    "Assistant:\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "output = model.generate(**inputs, max_new_tokens=150)\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "‚úÖ Fast\n",
    "‚úÖ Reliable\n",
    "‚úÖ Ideal for business demos\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Model 2: **LLaMA-2-7B**\n",
    "\n",
    "### üî• Real-World Use Case: **HR Resume Screening**\n",
    "\n",
    "```python\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer, model = load_model(model_name)\n",
    "\n",
    "prompt = \"\"\"You are an HR assistant.\n",
    "Analyze this resume and tell if candidate is suitable for Data Analyst role:\n",
    "Skills: SQL, Power BI, Python, Excel\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "output = model.generate(**inputs, max_new_tokens=200)\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "‚úÖ Enterprise tone\n",
    "‚úÖ Logical reasoning\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Model 3: **Falcon-7B**\n",
    "\n",
    "### üî• Real-World Use Case: **Long Document Summarization**\n",
    "\n",
    "```python\n",
    "model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "tokenizer, model = load_model(model_name)\n",
    "\n",
    "prompt = \"\"\"Summarize the following policy document in simple points:\n",
    "India's National Education Policy focuses on holistic development...\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "output = model.generate(**inputs, max_new_tokens=250)\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "‚úÖ Excellent for:\n",
    "\n",
    "* Policies\n",
    "* Legal docs\n",
    "* Reports\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Model 4: **Qwen-7B**\n",
    "\n",
    "### üî• Real-World Use Case: **Multilingual Assistant**\n",
    "\n",
    "```python\n",
    "model_name = \"Qwen/Qwen-7B-Chat\"\n",
    "tokenizer, model = load_model(model_name)\n",
    "\n",
    "prompt = \"\"\"Translate the following English text to Hindi:\n",
    "\"The loan will be approved within 5 working days.\"\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "output = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "‚úÖ Multilingual\n",
    "‚úÖ Structured output\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Optional: Single Dropdown to Switch Models (Teaching Trick)\n",
    "\n",
    "```python\n",
    "models = {\n",
    "    \"Mistral\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"LLaMA\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"Falcon\": \"tiiuae/falcon-7b-instruct\",\n",
    "    \"Qwen\": \"Qwen/Qwen-7B-Chat\"\n",
    "}\n",
    "```\n",
    "\n",
    "Use this to **switch models live in class** üë®‚Äçüè´üî•\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Typical Colab Limits (Be Honest in Demo)\n",
    "\n",
    "| Item      | Reality   |\n",
    "| --------- | --------- |\n",
    "| GPU       | T4 (16GB) |\n",
    "| Max model | 7B        |\n",
    "| Runtime   | 6‚Äì12 hrs  |\n",
    "| Speed     | Moderate  |\n",
    "\n",
    "üí° Use **4-bit loading** (already done).\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ How to Explain This to Non-Technical Audience\n",
    "\n",
    "> ‚ÄúWe are using **open-source AI models**, running on **Google‚Äôs free GPU**, without sending company data outside.\n",
    "> Each model behaves differently, so businesses choose based on **speed, language, and accuracy**.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Perfect Demo Flow (15‚Äì20 mins)\n",
    "\n",
    "1Ô∏è‚É£ Explain LLM concept (2 min)\n",
    "2Ô∏è‚É£ Load Mistral (fast win)\n",
    "3Ô∏è‚É£ Switch to LLaMA (enterprise tone)\n",
    "4Ô∏è‚É£ Show Falcon summarization\n",
    "5Ô∏è‚É£ Show Qwen multilingual\n",
    "6Ô∏è‚É£ Compare outputs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f339a87f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6503c1c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5547bcfb",
   "metadata": {},
   "source": [
    "# üîπ Real-World Use Case\n",
    "\n",
    "### **AI Interview Assistant**\n",
    "\n",
    "**Input:** Job Description (JD)\n",
    "**Output:**\n",
    "\n",
    "1. Interview questions\n",
    "2. Model-generated ideal answer\n",
    "3. Candidate answer evaluation\n",
    "\n",
    "This is **exactly usable for business**, LMS, HR-tech, interview bots.\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ Colab Setup (Run First)\n",
    "\n",
    "```python\n",
    "!pip install -q transformers accelerate bitsandbytes sentencepiece\n",
    "```\n",
    "\n",
    "Restart runtime after install (important).\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ Common Loader Function (Reusable for All Models)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "```\n",
    "\n",
    "```python\n",
    "def load_llm(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        load_in_4bit=True\n",
    "    )\n",
    "    \n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=300,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return pipe\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ Model Options (Choose Any)\n",
    "\n",
    "### ‚úÖ Recommended for Colab (Free GPU)\n",
    "\n",
    "| Model       | Name                                  |\n",
    "| ----------- | ------------------------------------- |\n",
    "| **Mistral** | `mistralai/Mistral-7B-Instruct-v0.2`  |\n",
    "| **LLaMA 3** | `meta-llama/Meta-Llama-3-8B-Instruct` |\n",
    "| **Falcon**  | `tiiuae/falcon-7b-instruct`           |\n",
    "| **Qwen**    | `Qwen/Qwen1.5-7B-Chat`                |\n",
    "\n",
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ Load ONE Model (Example: Mistral)\n",
    "\n",
    "```python\n",
    "llm = load_llm(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "```\n",
    "\n",
    "(You can change model name anytime.)\n",
    "\n",
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ Real-World Prompt (Interview Use Case)\n",
    "\n",
    "### üìå Job Description (Business Input)\n",
    "\n",
    "```python\n",
    "job_description = \"\"\"\n",
    "We are hiring a Python Data Scientist.\n",
    "Required skills:\n",
    "- Python\n",
    "- Pandas, NumPy\n",
    "- Machine Learning\n",
    "- SQL\n",
    "- Model evaluation\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ Generate Interview Questions\n",
    "\n",
    "```python\n",
    "prompt = f\"\"\"\n",
    "You are an AI technical interviewer.\n",
    "\n",
    "Based on the following job description:\n",
    "{job_description}\n",
    "\n",
    "Generate:\n",
    "1. 5 technical interview questions\n",
    "2. Difficulty: Medium\n",
    "3. Focus on real-world scenarios\n",
    "\"\"\"\n",
    "\n",
    "response = llm(prompt)[0][\"generated_text\"]\n",
    "print(response)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 7Ô∏è‚É£ Generate Ideal Answer (Business Value)\n",
    "\n",
    "```python\n",
    "question = \"Explain how you would handle missing values in a real-world dataset.\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Provide:\n",
    "1. Ideal interview answer\n",
    "2. Code example in Python\n",
    "\"\"\"\n",
    "\n",
    "response = llm(prompt)[0][\"generated_text\"]\n",
    "print(response)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 8Ô∏è‚É£ Candidate Answer Evaluation (HR / LMS Feature)\n",
    "\n",
    "```python\n",
    "candidate_answer = \"\"\"\n",
    "I remove missing values using dropna and sometimes fill them with mean.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are an AI interviewer.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Candidate Answer:\n",
    "{candidate_answer}\n",
    "\n",
    "Evaluate:\n",
    "1. Score out of 10\n",
    "2. Strengths\n",
    "3. Weaknesses\n",
    "4. Improvement suggestion\n",
    "\"\"\"\n",
    "\n",
    "response = llm(prompt)[0][\"generated_text\"]\n",
    "print(response)\n",
    "```\n",
    "\n",
    "‚úîÔ∏è **This is directly usable in a real interview platform**\n",
    "\n",
    "---\n",
    "\n",
    "# 9Ô∏è‚É£ Switch Model (LLaMA / Falcon / Qwen)\n",
    "\n",
    "Just change ONE line üëá\n",
    "\n",
    "```python\n",
    "llm = load_llm(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "```\n",
    "\n",
    "OR\n",
    "\n",
    "```python\n",
    "llm = load_llm(\"tiiuae/falcon-7b-instruct\")\n",
    "```\n",
    "\n",
    "OR\n",
    "\n",
    "```python\n",
    "llm = load_llm(\"Qwen/Qwen1.5-7B-Chat\")\n",
    "```\n",
    "\n",
    "No other code changes needed ‚úÖ\n",
    "\n",
    "---\n",
    "\n",
    "# üîü Business Architecture (Colab ‚Üí Production)\n",
    "\n",
    "```\n",
    "Colab (Testing)\n",
    "   ‚Üì\n",
    "Flask / FastAPI\n",
    "   ‚Üì\n",
    "FAISS (Resume, JD, Notes)\n",
    "   ‚Üì\n",
    "LLM (Frozen)\n",
    "```\n",
    "\n",
    "You‚Äôre already planning this ‚Äî **perfect alignment**.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚ö†Ô∏è Common Colab Issues + Fix\n",
    "\n",
    "### ‚ùå CUDA OOM\n",
    "\n",
    "```python\n",
    "max_new_tokens=200\n",
    "```\n",
    "\n",
    "### ‚ùå Slow\n",
    "\n",
    "* Use **Mistral or Qwen**\n",
    "* Avoid Falcon-40B\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
