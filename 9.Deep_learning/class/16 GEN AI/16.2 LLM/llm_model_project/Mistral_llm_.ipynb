{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Ti3_spo3jQB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "executionInfo": {
     "elapsed": 401,
     "status": "error",
     "timestamp": 1750250907875,
     "user": {
      "displayName": "Sahil",
      "userId": "02943187861823395646"
     },
     "user_tz": -330
    },
    "id": "re1FfEukEnbj",
    "outputId": "05ba9f2b-8059-4ebc-8094-064902c3fa6d"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# hf_\n",
    "# \n",
    "# FMDlPgKapgl\n",
    "# HzEcXNeVxZzep\n",
    "# KWyHfsFVlj\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "model_inputs = encodeds.to(device)\n",
    "model.to(device)\n",
    "\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 206601,
     "status": "ok",
     "timestamp": 1750252074669,
     "user": {
      "displayName": "Sahil",
      "userId": "02943187861823395646"
     },
     "user_tz": -330
    },
    "id": "YUTPwWowI_3t",
    "outputId": "1b41f123-2db1-4650-f7cb-5ff5878cd0f2"
   },
   "outputs": [],
   "source": [
    "## Libraries Required\n",
    "!pip install langchain-huggingface\n",
    "## For API Calls\n",
    "!pip install huggingface_hub\n",
    "!pip install transformers\n",
    "!pip install accelerate\n",
    "!pip install  bitsandbytes\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1750251344607,
     "user": {
      "displayName": "Sahil",
      "userId": "02943187861823395646"
     },
     "user_tz": -330
    },
    "id": "AQ9icPNZGPAc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_TOKEN'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 83,
     "status": "ok",
     "timestamp": 1750251345504,
     "user": {
      "displayName": "Sahil",
      "userId": "02943187861823395646"
     },
     "user_tz": -330
    },
    "id": "9UFQV_ABGiGU"
   },
   "outputs": [],
   "source": [
    "hf_token=os.environ['HF_TOKEN']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 708,
     "status": "ok",
     "timestamp": 1750251771765,
     "user": {
      "displayName": "Sahil",
      "userId": "02943187861823395646"
     },
     "user_tz": -330
    },
    "id": "n_Pgvk-ZIkZD",
    "outputId": "f83e306f-9942-4f6f-9971-b02c0948d716"
   },
   "outputs": [],
   "source": [
    "## Environment secret keys\n",
    "from google.colab import userdata\n",
    "sec_key=userdata.get(\"llm_model\")\n",
    "print(sec_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1750251813590,
     "user": {
      "displayName": "Sahil",
      "userId": "02943187861823395646"
     },
     "user_tz": -330
    },
    "id": "sk29IyFFIywC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=sec_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 104,
     "status": "ok",
     "timestamp": 1750252403853,
     "user": {
      "displayName": "Sahil",
      "userId": "02943187861823395646"
     },
     "user_tz": -330
    },
    "id": "G2NCZVFQI1-C"
   },
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,temperature=0.7,huggingfacehub_api_token=sec_key, task=\"conversational\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1750252404701,
     "user": {
      "displayName": "Sahil",
      "userId": "02943187861823395646"
     },
     "user_tz": -330
    },
    "id": "Nrngt_ujKgfa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "executionInfo": {
     "elapsed": 122,
     "status": "error",
     "timestamp": 1750252406103,
     "user": {
      "displayName": "Sahil",
      "userId": "02943187861823395646"
     },
     "user_tz": -330
    },
    "id": "yjRqNyoZJ1aV",
    "outputId": "2b5394d2-787f-4ebd-915c-0c28ef546742"
   },
   "outputs": [],
   "source": [
    "llm.invoke(\"What is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "executionInfo": {
     "elapsed": 440,
     "status": "error",
     "timestamp": 1750251378704,
     "user": {
      "displayName": "Sahil",
      "userId": "02943187861823395646"
     },
     "user_tz": -330
    },
    "id": "DddXrO_KB6MD",
    "outputId": "1890bb96-f369-4de6-8eb9-e555ac23119b"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=hf_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\", use_auth_token=hf_token)\n",
    "\n",
    "# Prompt\n",
    "prompt = \"Explain quantum physics like I'm five.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIRsEY4iDlj8"
   },
   "source": [
    "Great! Let's break down **Mistral from scratch** into a detailed, beginner-friendly guide. This will help you understand and work with **Mistral models** ‚Äî open-weight, decoder-only large language models (LLMs) released by [Mistral AI](https://mistral.ai).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ What is Mistral?\n",
    "\n",
    "**Mistral** refers to a family of **open-weight Large Language Models (LLMs)**, similar to GPT, developed by [Mistral AI](https://mistral.ai). It is:\n",
    "\n",
    "* **Decoder-only** transformer architecture.\n",
    "* **Optimized for speed and performance** (uses grouped-query attention).\n",
    "* Released under **Apache 2.0 license**.\n",
    "* Good for **text generation, chatbots, summarization, reasoning, coding**, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Prerequisites\n",
    "\n",
    "Before starting with Mistral:\n",
    "\n",
    "* ‚úÖ Python basics\n",
    "* ‚úÖ PyTorch (for working with models)\n",
    "* ‚úÖ Hugging Face Transformers\n",
    "* ‚úÖ Understanding of Transformer architecture (optional but helpful)\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Models Released\n",
    "\n",
    "1. **Mistral 7B** ‚Äì 7 billion parameter model\n",
    "2. **Mixtral (Mixture of Experts)** ‚Äì 12.9B active params, 2-of-8 MoE\n",
    "3. **Mistral-instruct** ‚Äì Fine-tuned for instruction following\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Setup Guide\n",
    "\n",
    "### üîß 1. Install Required Libraries\n",
    "\n",
    "```bash\n",
    "pip install torch transformers accelerate\n",
    "```\n",
    "\n",
    "### üîß 2. Load Mistral from Hugging Face\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "# Prompt\n",
    "prompt = \"Explain quantum physics like I'm five.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Use Cases\n",
    "\n",
    "| Use Case          | Prompt Example                            |\n",
    "| ----------------- | ----------------------------------------- |\n",
    "| **Q\\&A Bot**      | \"What is the capital of France?\"          |\n",
    "| **Summarization** | \"Summarize this article: ...\"             |\n",
    "| **Code Gen**      | \"Write a Python function to sort a list.\" |\n",
    "| **Chatbot**       | \"Hello, how can I help you today?\"        |\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Behind the Scenes ‚Äì How Mistral Works\n",
    "\n",
    "* **Decoder-only architecture**: Like GPT.\n",
    "* **Sliding window attention** (for longer context without full quadratic cost).\n",
    "* **Grouped-query attention (GQA)**: Faster inference.\n",
    "* **Rotary positional embeddings (RoPE)**: For positional awareness.\n",
    "* **Supports long context (up to 32k tokens)** in Mixtral.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Instruction Tuning (Mistral-Instruct)\n",
    "\n",
    "Mistral Instruct is trained with prompt/response data to follow instructions better:\n",
    "\n",
    "Example prompt:\n",
    "\n",
    "```\n",
    "<s>[INST] What's the fastest animal on land? [/INST]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Deployment Ideas\n",
    "\n",
    "* Host it locally using `transformers`\n",
    "* Deploy on Hugging Face Spaces with Gradio\n",
    "* Run on cloud GPUs (Google Colab, AWS, RunPod, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Tips\n",
    "\n",
    "* Use `torch_dtype=torch.float16` for lower memory usage\n",
    "* Use `device_map=\"auto\"` to utilize multiple GPUs (if available)\n",
    "* Try prompt engineering to improve results\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "* ü§ñ [Hugging Face Mistral page](https://huggingface.co/mistralai)\n",
    "* üìò [Transformers Docs](https://huggingface.co/docs/transformers/index)\n",
    "* üß† [Mistral blog](https://mistral.ai/news/)\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Sample Project Ideas\n",
    "\n",
    "1. **Chatbot UI with Gradio + Mistral**\n",
    "2. **CSV Question Answering with Mistral**\n",
    "3. **Summarizer API using FastAPI + Mistral**\n",
    "4. **Voice to Text to Mistral Response (Whisper + Mistral)**\n",
    "\n",
    "---\n",
    "\n",
    "Would you like help building a **full beginner project** using Mistral (e.g., chatbot, Q\\&A from PDF/CSV, API deployment)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpKK3-YxAskK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjCK1-DIDnSt"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBXwAX4aDnQC"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnmf9Qc7DijP"
   },
   "source": [
    "Great! Let's break down **Mistral from scratch** into a detailed, beginner-friendly guide. This will help you understand and work with **Mistral models** ‚Äî open-weight, decoder-only large language models (LLMs) released by [Mistral AI](https://mistral.ai).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ What is Mistral?\n",
    "\n",
    "**Mistral** refers to a family of **open-weight Large Language Models (LLMs)**, similar to GPT, developed by [Mistral AI](https://mistral.ai). It is:\n",
    "\n",
    "* **Decoder-only** transformer architecture.\n",
    "* **Optimized for speed and performance** (uses grouped-query attention).\n",
    "* Released under **Apache 2.0 license**.\n",
    "* Good for **text generation, chatbots, summarization, reasoning, coding**, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Prerequisites\n",
    "\n",
    "Before starting with Mistral:\n",
    "\n",
    "* ‚úÖ Python basics\n",
    "* ‚úÖ PyTorch (for working with models)\n",
    "* ‚úÖ Hugging Face Transformers\n",
    "* ‚úÖ Understanding of Transformer architecture (optional but helpful)\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Models Released\n",
    "\n",
    "1. **Mistral 7B** ‚Äì 7 billion parameter model\n",
    "2. **Mixtral (Mixture of Experts)** ‚Äì 12.9B active params, 2-of-8 MoE\n",
    "3. **Mistral-instruct** ‚Äì Fine-tuned for instruction following\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Setup Guide\n",
    "\n",
    "### üîß 1. Install Required Libraries\n",
    "\n",
    "```bash\n",
    "pip install torch transformers accelerate\n",
    "```\n",
    "\n",
    "### üîß 2. Load Mistral from Hugging Face\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "# Prompt\n",
    "prompt = \"Explain quantum physics like I'm five.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Use Cases\n",
    "\n",
    "| Use Case          | Prompt Example                            |\n",
    "| ----------------- | ----------------------------------------- |\n",
    "| **Q\\&A Bot**      | \"What is the capital of France?\"          |\n",
    "| **Summarization** | \"Summarize this article: ...\"             |\n",
    "| **Code Gen**      | \"Write a Python function to sort a list.\" |\n",
    "| **Chatbot**       | \"Hello, how can I help you today?\"        |\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Behind the Scenes ‚Äì How Mistral Works\n",
    "\n",
    "* **Decoder-only architecture**: Like GPT.\n",
    "* **Sliding window attention** (for longer context without full quadratic cost).\n",
    "* **Grouped-query attention (GQA)**: Faster inference.\n",
    "* **Rotary positional embeddings (RoPE)**: For positional awareness.\n",
    "* **Supports long context (up to 32k tokens)** in Mixtral.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Instruction Tuning (Mistral-Instruct)\n",
    "\n",
    "Mistral Instruct is trained with prompt/response data to follow instructions better:\n",
    "\n",
    "Example prompt:\n",
    "\n",
    "```\n",
    "<s>[INST] What's the fastest animal on land? [/INST]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Deployment Ideas\n",
    "\n",
    "* Host it locally using `transformers`\n",
    "* Deploy on Hugging Face Spaces with Gradio\n",
    "* Run on cloud GPUs (Google Colab, AWS, RunPod, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Tips\n",
    "\n",
    "* Use `torch_dtype=torch.float16` for lower memory usage\n",
    "* Use `device_map=\"auto\"` to utilize multiple GPUs (if available)\n",
    "* Try prompt engineering to improve results\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "* ü§ñ [Hugging Face Mistral page](https://huggingface.co/mistralai)\n",
    "* üìò [Transformers Docs](https://huggingface.co/docs/transformers/index)\n",
    "* üß† [Mistral blog](https://mistral.ai/news/)\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Sample Project Ideas\n",
    "\n",
    "1. **Chatbot UI with Gradio + Mistral**\n",
    "2. **CSV Question Answering with Mistral**\n",
    "3. **Summarizer API using FastAPI + Mistral**\n",
    "4. **Voice to Text to Mistral Response (Whisper + Mistral)**\n",
    "\n",
    "---\n",
    "\n",
    "Would you like help building a **full beginner project** using Mistral (e.g., chatbot, Q\\&A from PDF/CSV, API deployment)?\n",
    "\n",
    "\n",
    "Great choice! Here's a **step-by-step interview-ready project**:\n",
    "üîä **Voice to Text to Mistral Response using Whisper + Mistral**\n",
    "\n",
    "This project shows your expertise in:\n",
    "\n",
    "* Speech recognition (Whisper)\n",
    "* LLM text generation (Mistral)\n",
    "* Real-time interaction (optional: Gradio or Streamlit UI)\n",
    "* Integration of two AI models\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Project Title\n",
    "\n",
    "**AI Voice Assistant: Talk to an LLM using Whisper and Mistral**\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Overview\n",
    "\n",
    "1. üéôÔ∏è **Voice Input**: User speaks a question or instruction\n",
    "2. üìù **Transcription**: Whisper converts voice to text\n",
    "3. ü§ñ **Mistral Response**: Mistral generates a natural-language response\n",
    "4. üí¨ **Display/Play Output**: Text is shown or spoken back\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Required Libraries\n",
    "\n",
    "```bash\n",
    "pip install torch transformers openai-whisper gradio\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Full Python Code\n",
    "\n",
    "```python\n",
    "import whisper\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import gradio as gr\n",
    "\n",
    "# Load Whisper model (tiny, base, small, medium, large)\n",
    "whisper_model = whisper.load_model(\"base\")\n",
    "\n",
    "# Load Mistral model\n",
    "mistral_model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(mistral_model_id)\n",
    "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
    "    mistral_model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Function to convert voice to text\n",
    "def transcribe(audio):\n",
    "    print(\"Transcribing...\")\n",
    "    result = whisper_model.transcribe(audio)\n",
    "    return result['text']\n",
    "\n",
    "# Function to generate response using Mistral\n",
    "def mistral_response(text):\n",
    "    prompt = f\"<s>[INST] {text} [/INST]\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        output = mistral_model.generate(**inputs, max_new_tokens=100)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response.split(\"[/INST]\")[-1].strip()\n",
    "\n",
    "# Combined function\n",
    "def voice_to_mistral(audio):\n",
    "    text = transcribe(audio)\n",
    "    response = mistral_response(text)\n",
    "    return f\"üîà You said: {text}\\nü§ñ Mistral says: {response}\"\n",
    "\n",
    "# Gradio UI\n",
    "app = gr.Interface(\n",
    "    fn=voice_to_mistral,\n",
    "    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Voice to Mistral AI Assistant\"\n",
    ")\n",
    "\n",
    "app.launch()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Interview Talking Points\n",
    "\n",
    "**1. Why Whisper?**\n",
    "‚ÄúWhisper is a state-of-the-art open-source speech recognition model by OpenAI, capable of multilingual and noisy speech transcription.‚Äù\n",
    "\n",
    "**2. Why Mistral?**\n",
    "‚ÄúMistral is an efficient, open-weight decoder-only LLM, optimized for text generation with competitive performance and open licensing.‚Äù\n",
    "\n",
    "**3. Use Cases**\n",
    "\n",
    "* Voice Assistants\n",
    "* Accessibility Tools\n",
    "* Voice-controlled Chatbots\n",
    "* Hands-free Question Answering\n",
    "\n",
    "**4. Challenges**\n",
    "\n",
    "* Latency in large model inference\n",
    "* Handling diverse accents with Whisper\n",
    "* GPU requirements for Mistral\n",
    "\n",
    "---\n",
    "\n",
    "## ü™õ Optional Enhancements\n",
    "\n",
    "* üîä Add TTS (text-to-speech) using `gTTS` or `pyttsx3`\n",
    "* üíæ Save transcript and response as conversation logs\n",
    "* üåê Deploy on Hugging Face Spaces or Streamlit Cloud\n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ Folder Structure\n",
    "\n",
    "```\n",
    "voice_to_mistral/\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ app.py                  # Main app script\n",
    "‚îú‚îÄ‚îÄ requirements.txt        # Dependencies\n",
    "‚îî‚îÄ‚îÄ README.md               # Project overview\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Would you like:\n",
    "\n",
    "* A version with TTS response?\n",
    "* A PDF resume-ready project description?\n",
    "* Deployment tutorial (Hugging Face / Streamlit Cloud)?\n",
    "\n",
    "Let me know how you'd like to present it in your interview!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 130899,
     "status": "ok",
     "timestamp": 1750167008893,
     "user": {
      "displayName": "Sahil",
      "userId": "02943187861823395646"
     },
     "user_tz": -330
    },
    "id": "lHDmrXN_EcpC",
    "outputId": "95d4b798-8f88-4cf5-8ec0-74f923991aa9"
   },
   "outputs": [],
   "source": [
    "pip install openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 240,
     "status": "error",
     "timestamp": 1750167009277,
     "user": {
      "displayName": "Sahil",
      "userId": "02943187861823395646"
     },
     "user_tz": -330
    },
    "id": "Mu-smJEaEIaH",
    "outputId": "15eca4ec-107f-451e-829b-4c1cdfb72f8d"
   },
   "outputs": [],
   "source": [
    "import whisper\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import gradio as gr\n",
    "\n",
    "# Load Whisper model (tiny, base, small, medium, large)\n",
    "whisper_model = whisper.load_model(\"base\")\n",
    "\n",
    "# Load Mistral model\n",
    "mistral_model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(mistral_model_id)\n",
    "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
    "    mistral_model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Function to convert voice to text\n",
    "def transcribe(audio):\n",
    "    print(\"Transcribing...\")\n",
    "    result = whisper_model.transcribe(audio)\n",
    "    return result['text']\n",
    "\n",
    "# Function to generate response using Mistral\n",
    "def mistral_response(text):\n",
    "    prompt = f\"<s>[INST] {text} [/INST]\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        output = mistral_model.generate(**inputs, max_new_tokens=100)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response.split(\"[/INST]\")[-1].strip()\n",
    "\n",
    "# Combined function\n",
    "def voice_to_mistral(audio):\n",
    "    text = transcribe(audio)\n",
    "    response = mistral_response(text)\n",
    "    return f\"üîà You said: {text}\\nü§ñ Mistral says: {response}\"\n",
    "\n",
    "# Gradio UI\n",
    "app = gr.Interface(\n",
    "    fn=voice_to_mistral,\n",
    "    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Voice to Mistral AI Assistant\"\n",
    ")\n",
    "\n",
    "app.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpOc-QRhEa_3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPSaHBqydg9xeVX5FL03CN4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
