{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3618bde8",
   "metadata": {},
   "source": [
    "# MNIST Deep Learning Notebook\n",
    "\n",
    "**Goal:** Use the MNIST handwritten digits dataset to demonstrate:\n",
    "\n",
    "- Artificial Neural Network (ANN)\n",
    "- Activation Functions (ReLU, Softmax, etc.)\n",
    "- Loss Functions (Categorical Crossentropy)\n",
    "- Optimization Techniques (SGD, Adam, etc.)\n",
    "- Regularization Techniques (L2, Dropout, Early Stopping)\n",
    "- Callbacks (EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, TensorBoard)\n",
    "\n",
    "This notebook is written for **technical and non-technical learners** with **detailed comments** in each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2715c3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"MNIST Deep Learning Demo with ANN, Activations, Losses, Optimizers, Regularization, and Callbacks.\n",
    "\n",
    "This notebook can be run cell-by-cell for teaching and learning.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    ReduceLROnPlateau,\n",
    "    CSVLogger,\n",
    "    TensorBoard,\n",
    ")\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758dc92b",
   "metadata": {},
   "source": [
    "## 1. Load and Explore the MNIST Dataset\n",
    "\n",
    "MNIST is a classic dataset of **28×28 grayscale images** of handwritten digits (0–9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e766b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data from Keras datasets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test data shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# Number of classes (digits 0–9)\n",
    "num_classes = len(np.unique(y_train))\n",
    "print(\"Number of classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14231c5",
   "metadata": {},
   "source": [
    "### Visualize Sample Digits\n",
    "\n",
    "We plot a few sample images to understand how the raw data looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe8b6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some sample images with labels\n",
    "plt.figure(figsize=(8, 4))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X_train[i], cmap='gray')\n",
    "    plt.title(f\"Label: {y_train[i]}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238d7644",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "- Flatten 28×28 images into 784-dimensional vectors\n",
    "- Scale pixel values to `[0, 1]`\n",
    "- Convert labels to **one-hot encoded vectors** for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a85203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten images: (n_samples, 28, 28) -> (n_samples, 784)\n",
    "X_train_flat = X_train.reshape(-1, 28 * 28).astype(\"float32\")\n",
    "X_test_flat = X_test.reshape(-1, 28 * 28).astype(\"float32\")\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "X_train_flat /= 255.0\n",
    "X_test_flat /= 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train_cat = to_categorical(y_train, num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes)\n",
    "\n",
    "print(\"X_train_flat shape:\", X_train_flat.shape)\n",
    "print(\"y_train_cat shape:\", y_train_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb88118",
   "metadata": {},
   "source": [
    "## 3. Utility: Plot Training History\n",
    "\n",
    "We define a helper function to easily compare **training vs. validation loss and accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62290480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, title_prefix=\"Model\"):\n",
    "    \"\"\"Plot training & validation loss and accuracy from a Keras History object.\"\"\"\n",
    "    if history is None:\n",
    "        print(\"No history to plot.\")\n",
    "        return\n",
    "\n",
    "    hist = history.history\n",
    "    epochs = range(1, len(hist.get('loss', [])) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, hist.get('loss', []), label='Train Loss')\n",
    "    if 'val_loss' in hist:\n",
    "        plt.plot(epochs, hist['val_loss'], label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f\"{title_prefix} - Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy (if available)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    if 'accuracy' in hist:\n",
    "        plt.plot(epochs, hist['accuracy'], label='Train Acc')\n",
    "    if 'val_accuracy' in hist:\n",
    "        plt.plot(epochs, hist['val_accuracy'], label='Val Acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f\"{title_prefix} - Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551c9a19",
   "metadata": {},
   "source": [
    "## 4. Baseline ANN (No Regularization, Simple Optimizer)\n",
    "\n",
    "Here we build a simple fully-connected network:\n",
    "\n",
    "- Input: 784 features\n",
    "- Hidden layer: 128 neurons, **ReLU activation**\n",
    "- Output layer: 10 neurons, **Softmax activation**\n",
    "- Loss: **Categorical Crossentropy** (multi-class classification)\n",
    "- Optimizer: **SGD** (Stochastic Gradient Descent)\n",
    "\n",
    "This model helps us understand a basic ANN before adding optimizations and regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c513c558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple baseline ANN model\n",
    "baseline_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(28 * 28,)),  # ReLU activation in hidden layer\n",
    "    Dense(num_classes, activation='softmax')                # Softmax for multi-class output\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "baseline_model.compile(\n",
    "    optimizer='sgd',                      # Basic optimizer\n",
    "    loss='categorical_crossentropy',      # Standard loss for multi-class classification\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88c8975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the baseline model (few epochs for demo)\n",
    "history_baseline = baseline_model.fit(\n",
    "    X_train_flat,\n",
    "    y_train_cat,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plot_history(history_baseline, title_prefix=\"Baseline ANN (SGD)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38af9733",
   "metadata": {},
   "source": [
    "## 5. Improved ANN with Better Optimizer (Adam)\n",
    "\n",
    "Now we switch to a more powerful optimizer:\n",
    "\n",
    "- **Adam** combines the benefits of Momentum + RMSProp.\n",
    "- Often converges faster and to a better solution.\n",
    "\n",
    "We also increase the network depth slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d058178d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(28 * 28,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "adam_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "adam_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225ca0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_adam = adam_model.fit(\n",
    "    X_train_flat,\n",
    "    y_train_cat,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plot_history(history_adam, title_prefix=\"Improved ANN (Adam)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a268b2d3",
   "metadata": {},
   "source": [
    "## 6. ANN with Regularization + Callbacks\n",
    "\n",
    "Now we combine multiple **advanced deep learning concepts**:\n",
    "\n",
    "### Regularization Techniques\n",
    "- **L2 weight decay** (penalize large weights)\n",
    "- **Dropout** (randomly drop neurons during training)\n",
    "\n",
    "### Callbacks\n",
    "- `EarlyStopping` → stop when validation loss stops improving\n",
    "- `ModelCheckpoint` → save the best model weights\n",
    "- `ReduceLROnPlateau` → reduce learning rate when training stalls\n",
    "- `CSVLogger` → log training metrics to a CSV file\n",
    "- `TensorBoard` → visualize training (optional)\n",
    "\n",
    "This section demonstrates how real-world production models are trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ef2828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory setup for saving models and logs\n",
    "output_dir = \"mnist_training_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_path = os.path.join(output_dir, \"best_model.h5\")\n",
    "csv_log_path = os.path.join(output_dir, \"training_log.csv\")\n",
    "log_dir = os.path.join(output_dir, \"logs_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "print(\"Checkpoint path:\", checkpoint_path)\n",
    "print(\"CSV log path:\", csv_log_path)\n",
    "print(\"TensorBoard log dir:\", log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15da489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a regularized ANN model\n",
    "\n",
    "regularized_model = Sequential([\n",
    "    # First hidden layer with L2 regularization\n",
    "    Dense(\n",
    "        512,\n",
    "        activation='relu',\n",
    "        input_shape=(28 * 28,),\n",
    "        kernel_regularizer=regularizers.l2(1e-4)  # L2 penalty on weights\n",
    "    ),\n",
    "    Dropout(0.3),  # Dropout: randomly drop 30% of neurons\n",
    "\n",
    "    # Second hidden layer with L2 regularization\n",
    "    Dense(\n",
    "        256,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizers.l2(1e-4)\n",
    "    ),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Output layer (Softmax for multi-class classification)\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile with Adam optimizer and categorical crossentropy loss\n",
    "regularized_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "regularized_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8a8f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up callbacks\n",
    "\n",
    "early_stopping_cb = EarlyStopping(\n",
    "    monitor='val_loss',           # watch validation loss\n",
    "    patience=5,                   # epochs to wait for improvement\n",
    "    restore_best_weights=True,    # roll back to best weights\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_checkpoint_cb = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr_cb = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,           # reduce LR by half\n",
    "    patience=3,           # after 3 epochs of no improvement\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "csv_logger_cb = CSVLogger(csv_log_path)\n",
    "\n",
    "tensorboard_cb = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "callbacks_list = [\n",
    "    early_stopping_cb,\n",
    "    model_checkpoint_cb,\n",
    "    reduce_lr_cb,\n",
    "    csv_logger_cb,\n",
    "    tensorboard_cb,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e3632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the regularized model with callbacks\n",
    "history_reg = regularized_model.fit(\n",
    "    X_train_flat,\n",
    "    y_train_cat,\n",
    "    epochs=50,               # we allow up to 50 epochs, but EarlyStopping will likely cut it\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plot_history(history_reg, title_prefix=\"Regularized ANN (Adam + L2 + Dropout + Callbacks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec149bd",
   "metadata": {},
   "source": [
    "## 7. Evaluate Best Model on Test Data\n",
    "\n",
    "We now load the **best saved model** (according to validation loss) and evaluate\n",
    "it on the **test set** to see how well it generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abf50f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model from checkpoint (optional, but recommended)\n",
    "if os.path.exists(checkpoint_path):\n",
    "    best_model = load_model(checkpoint_path)\n",
    "    print(\"Loaded best model from checkpoint.\")\n",
    "else:\n",
    "    best_model = regularized_model\n",
    "    print(\"Checkpoint not found; using the last trained regularized model.\")\n",
    "\n",
    "# Evaluate on test data\n",
    "test_loss, test_acc = best_model.evaluate(X_test_flat, y_test_cat, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1871ca3f",
   "metadata": {},
   "source": [
    "## 8. Make Predictions and Visualize\n",
    "\n",
    "Let's predict some digits from the test set and compare predictions with true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7cb4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities for the first 10 test images\n",
    "y_pred_probs = best_model.predict(X_test_flat[:10])\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "print(\"Predicted labels:\", y_pred)\n",
    "print(\"True labels:     \", y_test[:10])\n",
    "\n",
    "# Visualize the corresponding images\n",
    "plt.figure(figsize=(8, 4))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X_test[i], cmap='gray')\n",
    "    plt.title(f\"Pred: {y_pred[i]}\\nTrue: {y_test[i]}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79addda2",
   "metadata": {},
   "source": [
    "## 9. Summary of Concepts Applied\n",
    "\n",
    "In this single notebook, we applied the following deep learning concepts on MNIST:\n",
    "\n",
    "1. **Artificial Neural Network (ANN)**\n",
    "   - Fully connected layers using `Dense`.\n",
    "\n",
    "2. **Activation Functions**\n",
    "   - Hidden layers: `ReLU`\n",
    "   - Output layer: `Softmax` for multi-class classification.\n",
    "\n",
    "3. **Loss Function**\n",
    "   - `categorical_crossentropy` for multi-class classification.\n",
    "\n",
    "4. **Optimization Techniques**\n",
    "   - Baseline: `SGD`\n",
    "   - Improved: `Adam` optimizer with learning rate `1e-3`.\n",
    "\n",
    "5. **Regularization Techniques**\n",
    "   - L2 weight regularization (`kernel_regularizer=regularizers.l2(...)`)\n",
    "   - Dropout layers to reduce overfitting\n",
    "   - Early stopping (via callback)\n",
    "\n",
    "6. **Callbacks**\n",
    "   - `EarlyStopping` → stop training when validation loss stops improving\n",
    "   - `ModelCheckpoint` → save the best model\n",
    "   - `ReduceLROnPlateau` → lower learning rate when learning saturates\n",
    "   - `CSVLogger` → log training history to CSV\n",
    "   - `TensorBoard` → visualize training with TensorBoard\n",
    "\n",
    "You can further extend this notebook by:\n",
    "- Trying different activation functions (e.g., `tanh`, `LeakyReLU`)\n",
    "- Changing optimizers (e.g., `RMSprop`, `AdamW`)\n",
    "- Adding Batch Normalization\n",
    "- Converting this fully connected ANN to a Convolutional Neural Network (CNN)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
