{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60307433",
   "metadata": {},
   "source": [
    "# MNIST ANN with Optuna Hyperparameter Optimization\n",
    "\n",
    "This notebook shows how to use **Optuna** to tune hyperparameters of a Keras\n",
    "Artificial Neural Network (ANN) on the **MNIST** dataset.\n",
    "\n",
    "Hyperparameters tuned:\n",
    "- Number of units in hidden layer\n",
    "- Number of hidden layers\n",
    "- Dropout rate\n",
    "- Learning rate\n",
    "- Batch size\n",
    "\n",
    "The code is heavily commented for teaching and self-study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86840092",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"MNIST + Optuna Hyperparameter Tuning Demo.\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03c7038",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess MNIST\n",
    "\n",
    "We load MNIST, flatten the images, normalize pixel values, and one‑hot encode labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d9d218",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(-1, 28 * 28).astype(\"float32\") / 255.0\n",
    "X_test = X_test.reshape(-1, 28 * 28).astype(\"float32\") / 255.0\n",
    "\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train_cat = to_categorical(y_train, num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes)\n",
    "\n",
    "print(\"Train:\", X_train.shape, y_train_cat.shape)\n",
    "print(\"Test:\", X_test.shape, y_test_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717e1188",
   "metadata": {},
   "source": [
    "### Quick Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44ad2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(X_train[i].reshape(28,28), cmap='gray')\n",
    "    plt.title(int(np.argmax(y_train_cat[i])))\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a39d96",
   "metadata": {},
   "source": [
    "## 2. Install and Import Optuna\n",
    "\n",
    "If Optuna is not installed in your environment, run the following in a notebook cell:\n",
    "\n",
    "```bash\n",
    "!pip install optuna -q\n",
    "```\n",
    "\n",
    "Then import it as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97053d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import optuna\n",
    "    print(\"Optuna version:\", optuna.__version__)\n",
    "    optuna_available = True\n",
    "except ImportError:\n",
    "    print(\"Optuna is not installed. Install it with: !pip install optuna\")\n",
    "    optuna_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1fb9c5",
   "metadata": {},
   "source": [
    "## 3. Define Objective Function for Optuna\n",
    "\n",
    "Optuna works by repeatedly calling an **objective function**.\n",
    "\n",
    "In each trial, Optuna will:\n",
    "- Suggest values for hyperparameters (units, layers, dropout, learning rate, batch size)\n",
    "- Build and train a model using those hyperparameters\n",
    "- Return a metric to **maximize** or **minimize** (here: validation accuracy to maximize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd6069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(trial):\n",
    "    \"\"\"Create a Keras ANN model whose hyperparameters are sampled by Optuna trial.\"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    # Number of hidden layers: 1–3\n",
    "    n_hidden = trial.suggest_int(\"n_hidden\", 1, 3)\n",
    "\n",
    "    for i in range(n_hidden):\n",
    "        # Units per layer: 64–512\n",
    "        units = trial.suggest_int(f\"units_{i}\", 64, 512, step=64)\n",
    "\n",
    "        if i == 0:\n",
    "            # First layer needs input_shape\n",
    "            model.add(Dense(units, activation=\"relu\", input_shape=(28*28,)))\n",
    "        else:\n",
    "            model.add(Dense(units, activation=\"relu\"))\n",
    "\n",
    "        # Dropout rate 0.0–0.5\n",
    "        dropout_rate = trial.suggest_float(f\"dropout_{i}\", 0.0, 0.5, step=0.1)\n",
    "        if dropout_rate > 0:\n",
    "            model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Output layer: 10 classes, softmax\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    # Learning rate for Adam\n",
    "    lr = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna objective: build, train, and evaluate a model; return validation accuracy.\"\"\"\n",
    "    model = create_model(trial)\n",
    "\n",
    "    # Tune batch size as well\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "\n",
    "    # We use a small number of epochs for speed; increase for better performance.\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train_cat,\n",
    "        validation_split=0.1,\n",
    "        epochs=5,\n",
    "        batch_size=batch_size,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    # Use the last validation accuracy as the objective value\n",
    "    val_acc = history.history[\"val_accuracy\"][-1]\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b11513",
   "metadata": {},
   "source": [
    "## 4. Run the Optuna Study\n",
    "\n",
    "We now create a study to **maximize validation accuracy** and run a limited\n",
    "number of trials (e.g., 10). You can increase `n_trials` for a deeper search\n",
    "if you have more compute time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b822d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if optuna_available:\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=10, show_progress_bar=True)\n",
    "\n",
    "    print(\"\\nBest trial:\")\n",
    "    best_trial = study.best_trial\n",
    "    print(\"  Value (val_accuracy):\", best_trial.value)\n",
    "    print(\"  Params:\")\n",
    "    for k, v in best_trial.params.items():\n",
    "        print(\"    {}: {}\".format(k, v))\n",
    "else:\n",
    "    print(\"Optuna not available; please install it to run the study.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc55445b",
   "metadata": {},
   "source": [
    "## 5. Train Best-Found Model on Full Training Data & Evaluate on Test Set\n",
    "\n",
    "After Optuna finishes, we rebuild the model with the best hyperparameters\n",
    "and train it a bit longer, then evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57245ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if optuna_available:\n",
    "    best_params = study.best_trial.params\n",
    "    print(\"\\nRebuilding model using best hyperparameters...\")\n",
    "\n",
    "    # Recreate a fake trial-like object to reuse our create_model() function\n",
    "    class SimpleTrial:\n",
    "        def __init__(self, params):\n",
    "            self.params = params\n",
    "\n",
    "        def suggest_int(self, name, low, high, step=1):\n",
    "            return self.params[name]\n",
    "\n",
    "        def suggest_float(self, name, low, high, step=None, log=False):\n",
    "            return self.params[name]\n",
    "\n",
    "        def suggest_categorical(self, name, choices):\n",
    "            return self.params[name]\n",
    "\n",
    "    dummy_trial = SimpleTrial(best_params)\n",
    "    best_model = create_model(dummy_trial)\n",
    "\n",
    "    best_batch_size = best_params.get(\"batch_size\", 128)\n",
    "\n",
    "    history_best = best_model.fit(\n",
    "        X_train,\n",
    "        y_train_cat,\n",
    "        validation_split=0.1,\n",
    "        epochs=10,\n",
    "        batch_size=best_batch_size,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    test_loss, test_acc = best_model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "    print(f\"\\nBest Optuna-tuned model - Test loss: {test_loss:.4f}, Test accuracy: {test_acc:.4f}\")\n",
    "else:\n",
    "    print(\"Skipping best model training because Optuna is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b161a1",
   "metadata": {},
   "source": [
    "## 6. Visualize a Few Predictions from the Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7747514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if optuna_available:\n",
    "    preds = best_model.predict(X_test[:10])\n",
    "    y_pred = np.argmax(preds, axis=1)\n",
    "\n",
    "    plt.figure(figsize=(8,3))\n",
    "    for i in range(10):\n",
    "        plt.subplot(2,5,i+1)\n",
    "        plt.imshow(X_test[i].reshape(28,28), cmap='gray')\n",
    "        plt.title(f\"Pred: {y_pred[i]}\\nTrue: {y_test[i]}\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Install Optuna and re-run the tuning section to see predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76810eea",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "In this notebook we:\n",
    "\n",
    "- Loaded and preprocessed the **MNIST** dataset.\n",
    "- Defined a flexible ANN architecture with:\n",
    "  - Tunable number of layers\n",
    "  - Tunable number of units\n",
    "  - Tunable dropout rate\n",
    "  - Tunable learning rate & batch size\n",
    "- Used **Optuna** to perform hyperparameter optimization by maximizing\n",
    "  validation accuracy.\n",
    "- Retrained the best-found model and evaluated it on the test set.\n",
    "\n",
    "You can extend this by:\n",
    "- Increasing `n_trials` for better search.\n",
    "- Adding L2 regularization as another hyperparameter.\n",
    "- Tuning different optimizers (e.g., Adam vs. RMSprop).\n",
    "- Integrating this into a bigger teaching notebook with ANN, activations,\n",
    "  loss functions, regularization, callbacks, and now Optuna-based tuning."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
