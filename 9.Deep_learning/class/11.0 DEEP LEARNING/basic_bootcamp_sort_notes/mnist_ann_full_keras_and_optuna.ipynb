{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43bde2d2",
   "metadata": {},
   "source": [
    "# MNIST Deep Learning Notebook\n",
    "\n",
    "**Goal:** Use the MNIST handwritten digits dataset to demonstrate:\n",
    "\n",
    "- Artificial Neural Network (ANN)\n",
    "- Activation Functions (ReLU, Softmax, etc.)\n",
    "- Loss Functions (Categorical Crossentropy)\n",
    "- Optimization Techniques (SGD, Adam, etc.)\n",
    "- Regularization Techniques (L2, Dropout, Early Stopping)\n",
    "- Callbacks (EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, TensorBoard)\n",
    "- Hyperparameter Tuning (learning rate, units, layers, dropout, batch size)\n",
    "\n",
    "This notebook is written for **technical and non-technical learners** with **detailed comments** in each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e616f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"MNIST Deep Learning Demo with ANN, Activations, Losses, Optimizers,\n",
    "Regularization, Callbacks, and Hyperparameter Tuning.\n",
    "\n",
    "This notebook can be run cell-by-cell for teaching and learning.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    ReduceLROnPlateau,\n",
    "    CSVLogger,\n",
    "    TensorBoard,\n",
    ")\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c91bdc1",
   "metadata": {},
   "source": [
    "## 1. Load and Explore the MNIST Dataset\n",
    "\n",
    "MNIST is a classic dataset of **28Ã—28 grayscale images** of handwritten digits (0â€“9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579c7708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data from Keras datasets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test data shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# Number of classes (digits 0â€“9)\n",
    "num_classes = len(np.unique(y_train))\n",
    "print(\"Number of classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3091029",
   "metadata": {},
   "source": [
    "### Visualize Sample Digits\n",
    "\n",
    "We plot a few sample images to understand how the raw data looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d06c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some sample images with labels\n",
    "plt.figure(figsize=(8, 4))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X_train[i], cmap='gray')\n",
    "    plt.title(f\"Label: {y_train[i]}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f31ba8c",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "- Flatten 28Ã—28 images into 784-dimensional vectors\n",
    "- Scale pixel values to `[0, 1]`\n",
    "- Convert labels to **one-hot encoded vectors** for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef623d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten images: (n_samples, 28, 28) -> (n_samples, 784)\n",
    "X_train_flat = X_train.reshape(-1, 28 * 28).astype(\"float32\")\n",
    "X_test_flat = X_test.reshape(-1, 28 * 28).astype(\"float32\")\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "X_train_flat /= 255.0\n",
    "X_test_flat /= 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train_cat = to_categorical(y_train, num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes)\n",
    "\n",
    "print(\"X_train_flat shape:\", X_train_flat.shape)\n",
    "print(\"y_train_cat shape:\", y_train_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f17e06",
   "metadata": {},
   "source": [
    "## 3. Utility: Plot Training History\n",
    "\n",
    "We define a helper function to easily compare **training vs. validation loss and accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacfd3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, title_prefix=\"Model\"):\n",
    "    \"\"\"Plot training & validation loss and accuracy from a Keras History object.\"\"\"\n",
    "    if history is None:\n",
    "        print(\"No history to plot.\")\n",
    "        return\n",
    "\n",
    "    hist = history.history\n",
    "    epochs = range(1, len(hist.get('loss', [])) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, hist.get('loss', []), label='Train Loss')\n",
    "    if 'val_loss' in hist:\n",
    "        plt.plot(epochs, hist['val_loss'], label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f\"{title_prefix} - Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy (if available)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    if 'accuracy' in hist:\n",
    "        plt.plot(epochs, hist['accuracy'], label='Train Acc')\n",
    "    if 'val_accuracy' in hist:\n",
    "        plt.plot(epochs, hist['val_accuracy'], label='Val Acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f\"{title_prefix} - Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc1c9cc",
   "metadata": {},
   "source": [
    "## 4. Baseline ANN (No Regularization, Simple Optimizer)\n",
    "\n",
    "Here we build a simple fully-connected network:\n",
    "\n",
    "- Input: 784 features\n",
    "- Hidden layer: 128 neurons, **ReLU activation**\n",
    "- Output layer: 10 neurons, **Softmax activation**\n",
    "- Loss: **Categorical Crossentropy** (multi-class classification)\n",
    "- Optimizer: **SGD** (Stochastic Gradient Descent)\n",
    "\n",
    "This model helps us understand a basic ANN before adding optimizations and regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dd1d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple baseline ANN model\n",
    "baseline_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(28 * 28,)),  # ReLU activation in hidden layer\n",
    "    Dense(num_classes, activation='softmax')                # Softmax for multi-class output\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "baseline_model.compile(\n",
    "    optimizer='sgd',                      # Basic optimizer\n",
    "    loss='categorical_crossentropy',      # Standard loss for multi-class classification\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe99462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the baseline model (few epochs for demo)\n",
    "history_baseline = baseline_model.fit(\n",
    "    X_train_flat,\n",
    "    y_train_cat,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plot_history(history_baseline, title_prefix=\"Baseline ANN (SGD)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0832bdf0",
   "metadata": {},
   "source": [
    "## 5. Improved ANN with Better Optimizer (Adam)\n",
    "\n",
    "Now we switch to a more powerful optimizer:\n",
    "\n",
    "- **Adam** combines the benefits of Momentum + RMSProp.\n",
    "- Often converges faster and to a better solution.\n",
    "\n",
    "We also increase the network depth slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aea6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(28 * 28,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "adam_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "adam_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52263d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_adam = adam_model.fit(\n",
    "    X_train_flat,\n",
    "    y_train_cat,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plot_history(history_adam, title_prefix=\"Improved ANN (Adam)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07b95f5",
   "metadata": {},
   "source": [
    "## 6. ANN with Regularization + Callbacks\n",
    "\n",
    "Now we combine multiple **advanced deep learning concepts**:\n",
    "\n",
    "### Regularization Techniques\n",
    "- **L2 weight decay** (penalize large weights)\n",
    "- **Dropout** (randomly drop neurons during training)\n",
    "\n",
    "### Callbacks\n",
    "- `EarlyStopping` â†’ stop when validation loss stops improving\n",
    "- `ModelCheckpoint` â†’ save the best model weights\n",
    "- `ReduceLROnPlateau` â†’ reduce learning rate when training stalls\n",
    "- `CSVLogger` â†’ log training metrics to a CSV file\n",
    "- `TensorBoard` â†’ visualize training (optional)\n",
    "\n",
    "This section demonstrates how real-world production models are trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2035175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory setup for saving models and logs\n",
    "output_dir = \"mnist_training_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_path = os.path.join(output_dir, \"best_model.h5\")\n",
    "csv_log_path = os.path.join(output_dir, \"training_log.csv\")\n",
    "log_dir = os.path.join(output_dir, \"logs_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "print(\"Checkpoint path:\", checkpoint_path)\n",
    "print(\"CSV log path:\", csv_log_path)\n",
    "print(\"TensorBoard log dir:\", log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220b717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a regularized ANN model\n",
    "\n",
    "regularized_model = Sequential([\n",
    "    # First hidden layer with L2 regularization\n",
    "    Dense(\n",
    "        512,\n",
    "        activation='relu',\n",
    "        input_shape=(28 * 28,),\n",
    "        kernel_regularizer=regularizers.l2(1e-4)  # L2 penalty on weights\n",
    "    ),\n",
    "    Dropout(0.3),  # Dropout: randomly drop 30% of neurons\n",
    "\n",
    "    # Second hidden layer with L2 regularization\n",
    "    Dense(\n",
    "        256,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizers.l2(1e-4)\n",
    "    ),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Output layer (Softmax for multi-class classification)\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile with Adam optimizer and categorical crossentropy loss\n",
    "regularized_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "regularized_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce54d972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up callbacks\n",
    "\n",
    "early_stopping_cb = EarlyStopping(\n",
    "    monitor='val_loss',           # watch validation loss\n",
    "    patience=5,                   # epochs to wait for improvement\n",
    "    restore_best_weights=True,    # roll back to best weights\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_checkpoint_cb = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr_cb = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,           # reduce LR by half\n",
    "    patience=3,           # after 3 epochs of no improvement\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "csv_logger_cb = CSVLogger(csv_log_path)\n",
    "\n",
    "tensorboard_cb = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "callbacks_list = [\n",
    "    early_stopping_cb,\n",
    "    model_checkpoint_cb,\n",
    "    reduce_lr_cb,\n",
    "    csv_logger_cb,\n",
    "    tensorboard_cb,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd42e210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the regularized model with callbacks\n",
    "history_reg = regularized_model.fit(\n",
    "    X_train_flat,\n",
    "    y_train_cat,\n",
    "    epochs=50,               # we allow up to 50 epochs, but EarlyStopping will likely cut it\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plot_history(history_reg, title_prefix=\"Regularized ANN (Adam + L2 + Dropout + Callbacks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeb7167",
   "metadata": {},
   "source": [
    "## 7. Evaluate Best Model on Test Data\n",
    "\n",
    "We now load the **best saved model** (according to validation loss) and evaluate\n",
    "it on the **test set** to see how well it generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50265a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model from checkpoint (optional, but recommended)\n",
    "if os.path.exists(checkpoint_path):\n",
    "    best_model = load_model(checkpoint_path)\n",
    "    print(\"Loaded best model from checkpoint.\")\n",
    "else:\n",
    "    best_model = regularized_model\n",
    "    print(\"Checkpoint not found; using the last trained regularized model.\")\n",
    "\n",
    "# Evaluate on test data\n",
    "test_loss, test_acc = best_model.evaluate(X_test_flat, y_test_cat, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ab7e40",
   "metadata": {},
   "source": [
    "## 8. Make Predictions and Visualize\n",
    "\n",
    "Let's predict some digits from the test set and compare predictions with true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3f7780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities for the first 10 test images\n",
    "y_pred_probs = best_model.predict(X_test_flat[:10])\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "print(\"Predicted labels:\", y_pred)\n",
    "print(\"True labels:     \", y_test[:10])\n",
    "\n",
    "# Visualize the corresponding images\n",
    "plt.figure(figsize=(8, 4))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X_test[i], cmap='gray')\n",
    "    plt.title(f\"Pred: {y_pred[i]}\\nTrue: {y_test[i]}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2877041e",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning\n",
    "\n",
    "Now we add **hyperparameter tuning** to search for a better architecture automatically.\n",
    "\n",
    "We will demonstrate two approaches:\n",
    "\n",
    "1. **Manual grid search** over a small set of hyperparameters.\n",
    "2. **Automated search** using `KerasTuner` (if available), tuning:\n",
    "   - Number of units in hidden layer(s)\n",
    "   - Number of hidden layers\n",
    "   - Dropout rate\n",
    "   - Learning rate\n",
    "   - Batch size\n",
    "\n",
    "> Note: For large searches, this can take time. You can reduce the number of trials/epochs for quick experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087afb78",
   "metadata": {},
   "source": [
    "### 9.1 Simple Manual Hyperparameter Search\n",
    "\n",
    "We try a few combinations of:\n",
    "\n",
    "- `learning_rate`: [0.01, 0.001]\n",
    "- `batch_size`: [64, 128]\n",
    "\n",
    "And track the validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab618ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_model(learning_rate=1e-3):\n",
    "    \"\"\"Build a simple ANN model with a configurable learning rate.\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(256, activation='relu', input_shape=(28 * 28,)),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "learning_rates = [1e-2, 1e-3]\n",
    "batch_sizes = [64, 128]\n",
    "\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        print(\"\\n=== Training with lr=\", lr, \"batch_size=\", bs, \"===\")\n",
    "        model = build_simple_model(learning_rate=lr)\n",
    "        history = model.fit(\n",
    "            X_train_flat,\n",
    "            y_train_cat,\n",
    "            epochs=5,\n",
    "            batch_size=bs,\n",
    "            validation_split=0.1,\n",
    "            verbose=0\n",
    "        )\n",
    "        val_acc = history.history['val_accuracy'][-1]\n",
    "        print(f\"Validation accuracy: {val_acc:.4f}\")\n",
    "        results.append({\n",
    "            'learning_rate': lr,\n",
    "            'batch_size': bs,\n",
    "            'val_accuracy': float(val_acc)\n",
    "        })\n",
    "\n",
    "print(\"\\nManual search results:\")\n",
    "for r in results:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9275e5",
   "metadata": {},
   "source": [
    "### 9.2 Hyperparameter Tuning with KerasTuner (Optional)\n",
    "\n",
    "We now use **KerasTuner** for a more systematic search.\n",
    "\n",
    "If KerasTuner is not installed, run in a notebook cell:\n",
    "\n",
    "```bash\n",
    "!pip install keras-tuner -q\n",
    "```\n",
    "\n",
    "Then re-run the import cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931b087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import keras_tuner as kt\n",
    "    print(\"KerasTuner version:\", kt.__version__)\n",
    "    kt_available = True\n",
    "except ImportError:\n",
    "    print(\"KerasTuner is not installed. Install it with: !pip install keras-tuner\")\n",
    "    kt_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c61eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if kt_available:\n",
    "    def build_hyper_model(hp):\n",
    "        \"\"\"Build a model for KerasTuner with multiple tunable hyperparameters.\"\"\"\n",
    "        model = Sequential()\n",
    "\n",
    "        # Tune number of hidden layers: 1 to 3\n",
    "        n_hidden = hp.Int('n_hidden', min_value=1, max_value=3, step=1)\n",
    "\n",
    "        for i in range(n_hidden):\n",
    "            # Tune units per layer: 64â€“512\n",
    "            units = hp.Int(f'units_{i}', min_value=64, max_value=512, step=64)\n",
    "            if i == 0:\n",
    "                model.add(Dense(units, activation='relu', input_shape=(28 * 28,)))\n",
    "            else:\n",
    "                model.add(Dense(units, activation='relu'))\n",
    "\n",
    "            # Tune dropout rate for each hidden layer\n",
    "            dropout_rate = hp.Float(f'dropout_{i}', min_value=0.0, max_value=0.5, step=0.1)\n",
    "            if dropout_rate > 0:\n",
    "                model.add(Dropout(dropout_rate))\n",
    "\n",
    "        # Output layer\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        # Tune learning rate for Adam optimizer\n",
    "        lr = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=opt,\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    tuner_dir = 'kt_mnist_tuner'\n",
    "    os.makedirs(tuner_dir, exist_ok=True)\n",
    "\n",
    "    tuner = kt.RandomSearch(\n",
    "        build_hyper_model,\n",
    "        objective='val_accuracy',\n",
    "        max_trials=5,          # increase for a more thorough search\n",
    "        executions_per_trial=1,\n",
    "        directory=tuner_dir,\n",
    "        project_name='mnist_ann_tuning'\n",
    "    )\n",
    "\n",
    "    tuner.search(\n",
    "        X_train_flat,\n",
    "        y_train_cat,\n",
    "        epochs=10,\n",
    "        validation_split=0.1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    print(\"\\nBest hyperparameters found:\")\n",
    "    for k in best_hp.values.keys():\n",
    "        print(k, \":\", best_hp.get(k))\n",
    "\n",
    "    best_hyper_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "    test_loss_hp, test_acc_hp = best_hyper_model.evaluate(X_test_flat, y_test_cat, verbose=0)\n",
    "    print(f\"\\n[HyperTuned Model] Test Loss: {test_loss_hp:.4f}, Test Accuracy: {test_acc_hp:.4f}\")\n",
    "else:\n",
    "    print(\"KerasTuner not available; skipping automated hyperparameter tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd32cfe4",
   "metadata": {},
   "source": [
    "## 10. Summary of Concepts Applied\n",
    "\n",
    "In this single notebook, we applied the following deep learning concepts on MNIST:\n",
    "\n",
    "1. **Artificial Neural Network (ANN)**\n",
    "   - Fully connected layers using `Dense`.\n",
    "\n",
    "2. **Activation Functions**\n",
    "   - Hidden layers: `ReLU`\n",
    "   - Output layer: `Softmax` for multi-class classification.\n",
    "\n",
    "3. **Loss Function**\n",
    "   - `categorical_crossentropy` for multi-class classification.\n",
    "\n",
    "4. **Optimization Techniques**\n",
    "   - Baseline: `SGD`\n",
    "   - Improved: `Adam` optimizer with learning rate tuning.\n",
    "\n",
    "5. **Regularization Techniques**\n",
    "   - L2 weight regularization (`kernel_regularizer=regularizers.l2(...)`)\n",
    "   - Dropout layers to reduce overfitting\n",
    "   - Early stopping (via callback)\n",
    "\n",
    "6. **Callbacks**\n",
    "   - `EarlyStopping` â†’ stop training when validation loss stops improving\n",
    "   - `ModelCheckpoint` â†’ save the best model\n",
    "   - `ReduceLROnPlateau` â†’ lower learning rate when learning saturates\n",
    "   - `CSVLogger` â†’ log training history to CSV\n",
    "   - `TensorBoard` â†’ visualize training with TensorBoard\n",
    "\n",
    "7. **Hyperparameter Tuning**\n",
    "   - Manual search over learning rate & batch size\n",
    "   - Automated search using `KerasTuner` (if installed) over:\n",
    "     - Number of layers\n",
    "     - Units per layer\n",
    "     - Dropout rate\n",
    "     - Learning rate\n",
    "\n",
    "You can further extend this notebook by:\n",
    "- Trying different activation functions (e.g., `tanh`, `LeakyReLU`)\n",
    "- Changing optimizers (e.g., `RMSprop`, `AdamW`)\n",
    "- Adding Batch Normalization\n",
    "- Converting this fully connected ANN to a Convolutional Neural Network (CNN).\n",
    "- Expanding the hyperparameter search space and trials for better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6028c94",
   "metadata": {},
   "source": [
    "---\n",
    "# ðŸ”¥ Advanced Section: Optuna Hyperparameter Optimization\n",
    "\n",
    "The following section adds **Optuna-based hyperparameter tuning** on top of the\n",
    "previous Keras-based ANN experiments. You can run it after training the earlier\n",
    "models, or independently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60307433",
   "metadata": {},
   "source": [
    "# MNIST ANN with Optuna Hyperparameter Optimization\n",
    "\n",
    "This notebook shows how to use **Optuna** to tune hyperparameters of a Keras\n",
    "Artificial Neural Network (ANN) on the **MNIST** dataset.\n",
    "\n",
    "Hyperparameters tuned:\n",
    "- Number of units in hidden layer\n",
    "- Number of hidden layers\n",
    "- Dropout rate\n",
    "- Learning rate\n",
    "- Batch size\n",
    "\n",
    "The code is heavily commented for teaching and self-study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86840092",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"MNIST + Optuna Hyperparameter Tuning Demo.\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03c7038",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess MNIST\n",
    "\n",
    "We load MNIST, flatten the images, normalize pixel values, and oneâ€‘hot encode labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d9d218",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(-1, 28 * 28).astype(\"float32\") / 255.0\n",
    "X_test = X_test.reshape(-1, 28 * 28).astype(\"float32\") / 255.0\n",
    "\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train_cat = to_categorical(y_train, num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes)\n",
    "\n",
    "print(\"Train:\", X_train.shape, y_train_cat.shape)\n",
    "print(\"Test:\", X_test.shape, y_test_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717e1188",
   "metadata": {},
   "source": [
    "### Quick Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44ad2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(X_train[i].reshape(28,28), cmap='gray')\n",
    "    plt.title(int(np.argmax(y_train_cat[i])))\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a39d96",
   "metadata": {},
   "source": [
    "## 2. Install and Import Optuna\n",
    "\n",
    "If Optuna is not installed in your environment, run the following in a notebook cell:\n",
    "\n",
    "```bash\n",
    "!pip install optuna -q\n",
    "```\n",
    "\n",
    "Then import it as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97053d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import optuna\n",
    "    print(\"Optuna version:\", optuna.__version__)\n",
    "    optuna_available = True\n",
    "except ImportError:\n",
    "    print(\"Optuna is not installed. Install it with: !pip install optuna\")\n",
    "    optuna_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1fb9c5",
   "metadata": {},
   "source": [
    "## 3. Define Objective Function for Optuna\n",
    "\n",
    "Optuna works by repeatedly calling an **objective function**.\n",
    "\n",
    "In each trial, Optuna will:\n",
    "- Suggest values for hyperparameters (units, layers, dropout, learning rate, batch size)\n",
    "- Build and train a model using those hyperparameters\n",
    "- Return a metric to **maximize** or **minimize** (here: validation accuracy to maximize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd6069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(trial):\n",
    "    \"\"\"Create a Keras ANN model whose hyperparameters are sampled by Optuna trial.\"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    # Number of hidden layers: 1â€“3\n",
    "    n_hidden = trial.suggest_int(\"n_hidden\", 1, 3)\n",
    "\n",
    "    for i in range(n_hidden):\n",
    "        # Units per layer: 64â€“512\n",
    "        units = trial.suggest_int(f\"units_{i}\", 64, 512, step=64)\n",
    "\n",
    "        if i == 0:\n",
    "            # First layer needs input_shape\n",
    "            model.add(Dense(units, activation=\"relu\", input_shape=(28*28,)))\n",
    "        else:\n",
    "            model.add(Dense(units, activation=\"relu\"))\n",
    "\n",
    "        # Dropout rate 0.0â€“0.5\n",
    "        dropout_rate = trial.suggest_float(f\"dropout_{i}\", 0.0, 0.5, step=0.1)\n",
    "        if dropout_rate > 0:\n",
    "            model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Output layer: 10 classes, softmax\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    # Learning rate for Adam\n",
    "    lr = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna objective: build, train, and evaluate a model; return validation accuracy.\"\"\"\n",
    "    model = create_model(trial)\n",
    "\n",
    "    # Tune batch size as well\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "\n",
    "    # We use a small number of epochs for speed; increase for better performance.\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train_cat,\n",
    "        validation_split=0.1,\n",
    "        epochs=5,\n",
    "        batch_size=batch_size,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    # Use the last validation accuracy as the objective value\n",
    "    val_acc = history.history[\"val_accuracy\"][-1]\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b11513",
   "metadata": {},
   "source": [
    "## 4. Run the Optuna Study\n",
    "\n",
    "We now create a study to **maximize validation accuracy** and run a limited\n",
    "number of trials (e.g., 10). You can increase `n_trials` for a deeper search\n",
    "if you have more compute time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b822d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if optuna_available:\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=10, show_progress_bar=True)\n",
    "\n",
    "    print(\"\\nBest trial:\")\n",
    "    best_trial = study.best_trial\n",
    "    print(\"  Value (val_accuracy):\", best_trial.value)\n",
    "    print(\"  Params:\")\n",
    "    for k, v in best_trial.params.items():\n",
    "        print(\"    {}: {}\".format(k, v))\n",
    "else:\n",
    "    print(\"Optuna not available; please install it to run the study.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc55445b",
   "metadata": {},
   "source": [
    "## 5. Train Best-Found Model on Full Training Data & Evaluate on Test Set\n",
    "\n",
    "After Optuna finishes, we rebuild the model with the best hyperparameters\n",
    "and train it a bit longer, then evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57245ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if optuna_available:\n",
    "    best_params = study.best_trial.params\n",
    "    print(\"\\nRebuilding model using best hyperparameters...\")\n",
    "\n",
    "    # Recreate a fake trial-like object to reuse our create_model() function\n",
    "    class SimpleTrial:\n",
    "        def __init__(self, params):\n",
    "            self.params = params\n",
    "\n",
    "        def suggest_int(self, name, low, high, step=1):\n",
    "            return self.params[name]\n",
    "\n",
    "        def suggest_float(self, name, low, high, step=None, log=False):\n",
    "            return self.params[name]\n",
    "\n",
    "        def suggest_categorical(self, name, choices):\n",
    "            return self.params[name]\n",
    "\n",
    "    dummy_trial = SimpleTrial(best_params)\n",
    "    best_model = create_model(dummy_trial)\n",
    "\n",
    "    best_batch_size = best_params.get(\"batch_size\", 128)\n",
    "\n",
    "    history_best = best_model.fit(\n",
    "        X_train,\n",
    "        y_train_cat,\n",
    "        validation_split=0.1,\n",
    "        epochs=10,\n",
    "        batch_size=best_batch_size,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    test_loss, test_acc = best_model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "    print(f\"\\nBest Optuna-tuned model - Test loss: {test_loss:.4f}, Test accuracy: {test_acc:.4f}\")\n",
    "else:\n",
    "    print(\"Skipping best model training because Optuna is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b161a1",
   "metadata": {},
   "source": [
    "## 6. Visualize a Few Predictions from the Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7747514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if optuna_available:\n",
    "    preds = best_model.predict(X_test[:10])\n",
    "    y_pred = np.argmax(preds, axis=1)\n",
    "\n",
    "    plt.figure(figsize=(8,3))\n",
    "    for i in range(10):\n",
    "        plt.subplot(2,5,i+1)\n",
    "        plt.imshow(X_test[i].reshape(28,28), cmap='gray')\n",
    "        plt.title(f\"Pred: {y_pred[i]}\\nTrue: {y_test[i]}\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Install Optuna and re-run the tuning section to see predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76810eea",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "In this notebook we:\n",
    "\n",
    "- Loaded and preprocessed the **MNIST** dataset.\n",
    "- Defined a flexible ANN architecture with:\n",
    "  - Tunable number of layers\n",
    "  - Tunable number of units\n",
    "  - Tunable dropout rate\n",
    "  - Tunable learning rate & batch size\n",
    "- Used **Optuna** to perform hyperparameter optimization by maximizing\n",
    "  validation accuracy.\n",
    "- Retrained the best-found model and evaluated it on the test set.\n",
    "\n",
    "You can extend this by:\n",
    "- Increasing `n_trials` for better search.\n",
    "- Adding L2 regularization as another hyperparameter.\n",
    "- Tuning different optimizers (e.g., Adam vs. RMSprop).\n",
    "- Integrating this into a bigger teaching notebook with ANN, activations,\n",
    "  loss functions, regularization, callbacks, and now Optuna-based tuning."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
