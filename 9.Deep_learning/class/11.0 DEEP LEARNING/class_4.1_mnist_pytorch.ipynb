{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN6a0pkAbRNSRP7b3zrtbR3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"dJH6kLf11Pcg","executionInfo":{"status":"error","timestamp":1762053357895,"user_tz":-330,"elapsed":88,"user":{"displayName":"Sahil","userId":"02943187861823395646"}},"outputId":"b7dbf399-980b-470b-f7b3-ad32cb66baba","colab":{"base_uri":"https://localhost:8080/","height":401}},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'streamlit'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2312489614.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstreamlit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'streamlit'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import streamlit as st\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, random_split\n","import matplotlib.pyplot as plt\n","\n","# -----------------------------\n","# Streamlit UI\n","# -----------------------------\n","st.title(\"Module 3: Training Deep Neural Networks (ANN)\")\n","\n","# Hyperparameters\n","batch_size = st.slider(\"Batch Size\", 8, 128, 32)\n","learning_rate = st.slider(\"Learning Rate\", 0.0001, 0.1, 0.01, step=0.001)\n","epochs = st.slider(\"Epochs\", 1, 20, 5)\n","optimizer_choice = st.selectbox(\"Optimizer\", [\"SGD\", \"Adam\"])\n","dropout_rate = st.slider(\"Dropout Rate\", 0.0, 0.5, 0.2, step=0.05)\n","l2_reg = st.slider(\"L2 Regularization (Weight Decay)\", 0.0, 0.1, 0.0, step=0.01)\n","momentum = st.slider(\"Momentum (SGD only)\", 0.0, 0.99, 0.9, step=0.01)\n","\n","# -----------------------------\n","# Load MNIST Dataset\n","# -----------------------------\n","transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n","dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n","train_size = int(0.8 * len(dataset))\n","val_size = len(dataset) - train_size\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","# -----------------------------\n","# Define Model with Weight Initialization and Dropout\n","# -----------------------------\n","class SimpleANN(nn.Module):\n","    def __init__(self, dropout_rate=0.2):\n","        super(SimpleANN, self).__init__()\n","        self.fc1 = nn.Linear(28*28, 128)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 10)\n","        # Weight initialization\n","        nn.init.kaiming_normal_(self.fc1.weight)\n","        nn.init.kaiming_normal_(self.fc2.weight)\n","        nn.init.xavier_normal_(self.fc3.weight)\n","\n","    def forward(self, x):\n","        x = x.view(-1, 28*28)\n","        x = torch.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = torch.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","model = SimpleANN(dropout_rate=dropout_rate)\n","\n","# -----------------------------\n","# Define Optimizer\n","# -----------------------------\n","if optimizer_choice == \"SGD\":\n","    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=l2_reg)\n","elif optimizer_choice == \"Adam\":\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_reg)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","# -----------------------------\n","# Training Loop with Live Plot\n","# -----------------------------\n","train_losses, val_losses, val_accuracies = [], [], []\n","\n","if st.button(\"Start Training\"):\n","    st.write(\"Training started...\")\n","    for epoch in range(1, epochs + 1):\n","        model.train()\n","        running_loss = 0.0\n","        for images, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            # Gradient Clipping to avoid exploding gradients\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n","            optimizer.step()\n","            running_loss += loss.item() * images.size(0)\n","        epoch_loss = running_loss / len(train_loader.dataset)\n","        train_losses.append(epoch_loss)\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0.0\n","        correct = 0\n","        with torch.no_grad():\n","            for images, labels in val_loader:\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item() * images.size(0)\n","                preds = outputs.argmax(dim=1)\n","                correct += (preds == labels).sum().item()\n","        val_loss /= len(val_loader.dataset)\n","        val_losses.append(val_loss)\n","        val_acc = correct / len(val_loader.dataset)\n","        val_accuracies.append(val_acc)\n","\n","        # Live Plot\n","        st.write(f\"Epoch {epoch}/{epochs} - Train Loss: {epoch_loss:.4f} - Val Loss: {val_loss:.4f} - Val Acc: {val_acc:.4f}\")\n","        fig, ax = plt.subplots(1, 2, figsize=(10,4))\n","        ax[0].plot(range(1,len(train_losses)+1), train_losses, label=\"Train Loss\")\n","        ax[0].plot(range(1,len(val_losses)+1), val_losses, label=\"Val Loss\")\n","        ax[0].set_xlabel(\"Epochs\"); ax[0].set_ylabel(\"Loss\"); ax[0].legend(); ax[0].grid(True)\n","        ax[1].plot(range(1,len(val_accuracies)+1), val_accuracies, label=\"Val Accuracy\", color=\"green\")\n","        ax[1].set_xlabel(\"Epochs\"); ax[1].set_ylabel(\"Accuracy\"); ax[1].legend(); ax[1].grid(True)\n","        st.pyplot(fig)\n","\n","    st.success(\"Training Completed!\")\n","\n","# -----------------------------\n","# Optional: Save Model Button\n","# -----------------------------\n","if st.button(\"Save Trained Model\"):\n","    torch.save(model.state_dict(), \"mnist_ann_trained.pth\")\n","    st.write(\"Model saved as mnist_ann_trained.pth\")"]},{"cell_type":"markdown","source":["Here’s a **full Python + PyTorch Streamlit demo** for **Module 3: Training Deep Neural Networks** covering **batch/mini-batch, learning rate scheduling, weight initialization, regularization, gradient issues, and live training visualization**.\n","\n","It’s interactive: learners can **train a small ANN on MNIST**, see **live-updating loss/accuracy**, and experiment with **regularization and optimizers**.\n","\n","```python\n","import streamlit as st\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, random_split\n","import matplotlib.pyplot as plt\n","\n","# -----------------------------\n","# Streamlit UI\n","# -----------------------------\n","st.title(\"Module 3: Training Deep Neural Networks (ANN)\")\n","\n","# Hyperparameters\n","batch_size = st.slider(\"Batch Size\", 8, 128, 32)\n","learning_rate = st.slider(\"Learning Rate\", 0.0001, 0.1, 0.01, step=0.001)\n","epochs = st.slider(\"Epochs\", 1, 20, 5)\n","optimizer_choice = st.selectbox(\"Optimizer\", [\"SGD\", \"Adam\"])\n","dropout_rate = st.slider(\"Dropout Rate\", 0.0, 0.5, 0.2, step=0.05)\n","l2_reg = st.slider(\"L2 Regularization (Weight Decay)\", 0.0, 0.1, 0.0, step=0.01)\n","momentum = st.slider(\"Momentum (SGD only)\", 0.0, 0.99, 0.9, step=0.01)\n","\n","# -----------------------------\n","# Load MNIST Dataset\n","# -----------------------------\n","transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n","dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n","train_size = int(0.8 * len(dataset))\n","val_size = len(dataset) - train_size\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","# -----------------------------\n","# Define Model with Weight Initialization and Dropout\n","# -----------------------------\n","class SimpleANN(nn.Module):\n","    def __init__(self, dropout_rate=0.2):\n","        super(SimpleANN, self).__init__()\n","        self.fc1 = nn.Linear(28*28, 128)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 10)\n","        # Weight initialization\n","        nn.init.kaiming_normal_(self.fc1.weight)\n","        nn.init.kaiming_normal_(self.fc2.weight)\n","        nn.init.xavier_normal_(self.fc3.weight)\n","\n","    def forward(self, x):\n","        x = x.view(-1, 28*28)\n","        x = torch.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = torch.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","model = SimpleANN(dropout_rate=dropout_rate)\n","\n","# -----------------------------\n","# Define Optimizer\n","# -----------------------------\n","if optimizer_choice == \"SGD\":\n","    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=l2_reg)\n","elif optimizer_choice == \"Adam\":\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_reg)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","# -----------------------------\n","# Training Loop with Live Plot\n","# -----------------------------\n","train_losses, val_losses, val_accuracies = [], [], []\n","\n","if st.button(\"Start Training\"):\n","    st.write(\"Training started...\")\n","    for epoch in range(1, epochs + 1):\n","        model.train()\n","        running_loss = 0.0\n","        for images, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            # Gradient Clipping to avoid exploding gradients\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n","            optimizer.step()\n","            running_loss += loss.item() * images.size(0)\n","        epoch_loss = running_loss / len(train_loader.dataset)\n","        train_losses.append(epoch_loss)\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0.0\n","        correct = 0\n","        with torch.no_grad():\n","            for images, labels in val_loader:\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item() * images.size(0)\n","                preds = outputs.argmax(dim=1)\n","                correct += (preds == labels).sum().item()\n","        val_loss /= len(val_loader.dataset)\n","        val_losses.append(val_loss)\n","        val_acc = correct / len(val_loader.dataset)\n","        val_accuracies.append(val_acc)\n","\n","        # Live Plot\n","        st.write(f\"Epoch {epoch}/{epochs} - Train Loss: {epoch_loss:.4f} - Val Loss: {val_loss:.4f} - Val Acc: {val_acc:.4f}\")\n","        fig, ax = plt.subplots(1, 2, figsize=(10,4))\n","        ax[0].plot(range(1,len(train_losses)+1), train_losses, label=\"Train Loss\")\n","        ax[0].plot(range(1,len(val_losses)+1), val_losses, label=\"Val Loss\")\n","        ax[0].set_xlabel(\"Epochs\"); ax[0].set_ylabel(\"Loss\"); ax[0].legend(); ax[0].grid(True)\n","        ax[1].plot(range(1,len(val_accuracies)+1), val_accuracies, label=\"Val Accuracy\", color=\"green\")\n","        ax[1].set_xlabel(\"Epochs\"); ax[1].set_ylabel(\"Accuracy\"); ax[1].legend(); ax[1].grid(True)\n","        st.pyplot(fig)\n","\n","    st.success(\"Training Completed!\")\n","\n","# -----------------------------\n","# Optional: Save Model Button\n","# -----------------------------\n","if st.button(\"Save Trained Model\"):\n","    torch.save(model.state_dict(), \"mnist_ann_trained.pth\")\n","    st.write(\"Model saved as mnist_ann_trained.pth\")\n","```\n","\n","---\n","\n","### **Features Implemented**\n","\n","1. Batch and mini-batch training.\n","2. Adjustable learning rate and optimizer (SGD/Adam).\n","3. Weight initialization (Kaiming/Xavier).\n","4. Regularization: L2, Dropout.\n","5. Gradient clipping to handle exploding gradients.\n","6. Live training visualization: loss curves & accuracy plots.\n","7. Save trained model.\n","\n","---\n","\n","If you want, I can **merge this with the previous MNIST drawing/prediction app**, so learners can **train the network live and test by drawing digits interactively** in one Streamlit interface.\n","\n","Do you want me to do that next?\n"],"metadata":{"id":"YwLCJVqk1TKx"}}]}