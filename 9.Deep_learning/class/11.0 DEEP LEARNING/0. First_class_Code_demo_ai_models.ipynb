{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMLzYEARIcmj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# 1. Generate some sample data\n",
        "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1) # Features\n",
        "y = np.array([2, 4, 5, 4, 5, 7, 8, 9, 10, 12]) # Target variable\n",
        "\n",
        "# 2. Define the model\n",
        "model = LinearRegression()\n",
        "\n",
        "# 3. Train the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# 4. Make a prediction\n",
        "new_X = np.array([11]).reshape(-1, 1)\n",
        "prediction = model.predict(new_X)\n",
        "\n",
        "print(f\"Trained model coefficient: {model.coef_[0]:.2f}\")\n",
        "print(f\"Trained model intercept: {model.intercept_:.2f}\")\n",
        "print(f\"Prediction for new data {new_X[0][0]}: {prediction[0]:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjD7PJ8uIcml"
      },
      "outputs": [],
      "source": [
        "# pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKD8qA2aKuDz"
      },
      "source": [
        "## 1.text-classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hw5jdqQmREBa"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImfCUuKZIcmm"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-classification\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ds6_pQI9Icmm"
      },
      "outputs": [],
      "source": [
        "data = pipe(\"This restaurant is not awesome\")\n",
        "data[0]['label']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjuG0WgbNQS8"
      },
      "source": [
        "‚≠ê 3.6 Zero-Shot Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biy_IwrpNPzD"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\"zero-shot-classification\")\n",
        "\n",
        "result = pipe(\n",
        "    \"This is a course about AI and machine learning.\",\n",
        "    candidate_labels=[\"education\", \"politics\", \"sports\"]\n",
        ")\n",
        "\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVguNJFxp-ME"
      },
      "source": [
        "https://huggingface.co/docs/transformers/v4.57.0/en/main_classes/pipelines#transformers.Pipeline.model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrrqtYppp-Iz"
      },
      "source": [
        "## 2.Text Generations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZ6Q6CFRMmAT"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "result = pipe(\n",
        "    \"Once upon a time in India, there was a data scientist\",\n",
        "    max_length=50,\n",
        "    num_return_sequences=1\n",
        ")\n",
        "\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3_heMX2MobN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmosuVEyMoGx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOWFaY2MIcmp"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipeline = pipeline(task=\"text-generation\", model=\"google/gemma-2-2b\")\n",
        "pipeline(\"the secret to baking a really good cake is \")\n",
        "[{'generated_text': 'the secret to baking a really good cake is 1. the right ingredients 2. the'}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiVKZ7dOK1Gb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Llgdg8HgK1hl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0sjrMiJK4w-"
      },
      "source": [
        "## 3. text summarizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpP0XcBGK16L"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipeline = pipeline(task=\"summarization\", model=\"google/pegasus-billsum\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEPaLoBMK90D"
      },
      "outputs": [],
      "source": [
        "summarize_text = pipeline(\"Section was formerly set out as section 44 of this title. As originally enacted, this section contained two further provisions that 'nothing in this act shall be construed as in any wise affecting the grant of lands made to the State of California by virtue of the act entitled 'An act authorizing a grant to the State of California of the Yosemite Valley, and of the land' embracing the Mariposa Big-Tree Grove, approved June thirtieth, eighteen hundred and sixty-four; or as affecting any bona-fide entry of land made within the limits above described under any law of the United States prior to the approval of this act.' The first quoted provision was omitted from the Code because the land, granted to the state of California pursuant to the Act cite, was receded to the United States. Resolution June 11, 1906, No. 27, accepted the recession.\")\n",
        "[{'summary_text': 'Instructs the Secretary of the Interior to convey to the State of California all right, title, and interest of the United States in and to specified lands which are located within the Yosemite and Mariposa National Forests, California.'}]\n",
        "\n",
        "\n",
        "summarize_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqqZ2Wb3LjbM"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "result = pipe(\"I love using Hugging Face pipelines!\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmNvJEhYLjYK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wtFigNcMxLM"
      },
      "source": [
        "‚≠ê 3.5 Translation (English ‚Üí Hindi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TosK_kKiLjVd"
      },
      "outputs": [],
      "source": [
        "\n",
        "pipe = pipeline(\"translation_en_to_hi\", model=\"Helsinki-NLP/opus-mt-en-hi\")\n",
        "print(pipe(\"I love learning transformers!\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYMKA7CSLjSu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njwOziztNarW"
      },
      "source": [
        "‚≠ê 3.7 Image Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JG8ExfAyLjNx"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\"image-classification\", model=\"google/vit-base-patch16-224\")\n",
        "\n",
        "from PIL import Image\n",
        "img = Image.open(\"/content/new_pic.jpg\")\n",
        "\n",
        "print(pipe(img))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8azSEivLjFv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4VEJJ70OBSY"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2yaHrTfOCPM"
      },
      "outputs": [],
      "source": [
        "# ‚≠ê 3.8 Speech-to-Text (ASR)\n",
        "pipe = pipeline(\"automatic-speech-recognition\",\n",
        "                model=\"openai/whisper-small\")\n",
        "\n",
        "\n",
        "# print(pipe(\"audio.wav\"))\n",
        "# https://huggingface.co/docs/transformers/en/model_doc/whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OIXFeFZOxBa"
      },
      "outputs": [],
      "source": [
        "pipe(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3xVLf9FQHO-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRO7l03rQGz6"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtZcBYubOrEK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcod1UNMQE3u"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obctXAV1QRSH"
      },
      "outputs": [],
      "source": [
        "# ‚≠ê 3.9 Embeddings / Feature Extraction\n",
        "pipe = pipeline(\"feature-extraction\",\n",
        "                model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "emb = pipe(\"This is a feature extraction example.\")\n",
        "print(len(emb[0]))   # 384 dims\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jL6MechoQQgK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QUGKdf9QQXk"
      },
      "outputs": [],
      "source": [
        " # hf_ CXeDvBPImWhKBaiJ\n",
        "    # BFfuuKpqlQKxjIgIIA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1xSCw1PSe87"
      },
      "source": [
        "## ‚≠ê 3.10 Text-to-Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzQQiLLWRo0V"
      },
      "outputs": [],
      "source": [
        "# ‚≠ê 3.10 Text-to-Image\n",
        "\n",
        "# (Pipeline does NOT support; must use Diffusers)\n",
        "\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16\n",
        ").to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZpkeYufRops"
      },
      "outputs": [],
      "source": [
        "image = pipe(\"a futuristic city in neon lights\").images[0]\n",
        "image.save(\"city.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwTJqkBSRt5D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2L7E8dVIcms"
      },
      "outputs": [],
      "source": [
        "3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEGuS8buIcmt"
      },
      "source": [
        "Alright, let‚Äôs build a **free image generation setup using Hugging Face + Transformers** üòé\n",
        "\n",
        "I‚Äôll show you two options:\n",
        "\n",
        "1. **Completely free & easiest** ‚Üí Use **Hugging Face Inference API** (no GPU needed, runs on HF servers)\n",
        "2. **Local free (needs GPU/strong CPU)** ‚Üí Download model and run via `transformers` / `diffusers`\n",
        "\n",
        "Since you wrote *‚Äúusing tranfrmer‚Äù*, I‚Äôll keep it focused on Hugging Face ecosystem (Transformers-style pipelines).\n",
        "\n",
        "---\n",
        "\n",
        "## 1Ô∏è‚É£ Setup ‚Äì Hugging Face Account & Token (One Time)\n",
        "\n",
        "1. Go to Hugging Face ‚Üí create a free account.\n",
        "2. Go to **Settings ‚Üí Access Tokens** ‚Üí create a **Read token**.\n",
        "3. Install required libraries:\n",
        "\n",
        "```bash\n",
        "pip install transformers torch pillow requests\n",
        "```\n",
        "\n",
        "If you want to use diffusion models (recommended for images):\n",
        "\n",
        "```bash\n",
        "pip install diffusers accelerate safetensors\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 2Ô∏è‚É£ Easiest Way: Use Hugging Face Inference API (Free Tier)\n",
        "\n",
        "Here we **don‚Äôt download** the model; we just call it using HTTPS.\n",
        "Example using a Stable Diffusion model (text ‚Üí image):\n",
        "\n",
        "```python\n",
        "import requests\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/stabilityai/stable-diffusion-2-1\"\n",
        "HF_TOKEN = \"YOUR_HF_API_TOKEN\"   # <--- paste your token here\n",
        "\n",
        "headers = {\"Authorization\": f\"Bearer HF_TOKEN_REPLACE\"}\n",
        "\n",
        "def generate_image(prompt, output_path=\"generated.png\"):\n",
        "    payload = {\"inputs\": prompt}\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(\"Error:\", response.status_code, response.text)\n",
        "        return\n",
        "\n",
        "    with open(output_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    print(f\"Saved image to {output_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    prompt = \"a futuristic city at sunset, ultra realistic, 4k\"\n",
        "    generate_image(prompt)\n",
        "```\n",
        "\n",
        "üëâ **Important:** Replace\n",
        "\n",
        "```python\n",
        "headers = {\"Authorization\": f\"Bearer HF_TOKEN_REPLACE\"}\n",
        "```\n",
        "\n",
        "with\n",
        "\n",
        "```python\n",
        "headers = {\"Authorization\": f\"Bearer \" + HF_TOKEN}\n",
        "```\n",
        "\n",
        "This gives you a **free image generator** using a Hugging Face model, no GPU on your side.\n",
        "\n",
        "---\n",
        "\n",
        "## 3Ô∏è‚É£ Using `diffusers` + Transformers Locally (Open-Source, Free)\n",
        "\n",
        "This uses a **Transformers-style pipeline** but via the `diffusers` library (Hugging Face official for diffusion models).\n",
        "\n",
        "```python\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "# Download any free text-to-image model, example:\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,  # use float32 if you don't have GPU\n",
        ")\n",
        "\n",
        "# If you have a GPU (recommended)\n",
        "if torch.cuda.is_available():\n",
        "    pipe = pipe.to(\"cuda\")\n",
        "\n",
        "prompt = \"a cute robot teaching data science, digital art, vibrant colors\"\n",
        "image = pipe(prompt).images[0]\n",
        "\n",
        "image.save(\"robot_ds.png\")\n",
        "print(\"Image saved as robot_ds.png\")\n",
        "```\n",
        "\n",
        "If you **don‚Äôt** have GPU, change to:\n",
        "\n",
        "```python\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float32\n",
        ").to(\"cpu\")\n",
        "```\n",
        "\n",
        "‚ö†Ô∏è CPU will be **very slow** but still free.\n",
        "\n",
        "---\n",
        "\n",
        "## 4Ô∏è‚É£ ‚ÄúTransformers‚Äù Style Pipeline (Non-diffusers models)\n",
        "\n",
        "Some image models are available directly via `transformers` pipelines (e.g., text-to-image, image-to-image). Example:\n",
        "\n",
        "```python\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# Example: Phi-3-Vision / Qwen-VL etc. can generate images or reason about them\n",
        "# (Many models are more multimodal than pure generators)\n",
        "```\n",
        "\n",
        "But for **pure image generation**, **diffusers** is the normal + recommended way in HF ecosystem, and it‚Äôs built to work **with transformers under the hood**.\n",
        "\n",
        "---\n",
        "\n",
        "## 5Ô∏è‚É£ Wrap It as a Simple ‚ÄúFree HF Image Generator‚Äù Function\n",
        "\n",
        "Here‚Äôs a neat utility that automatically uses GPU if available:\n",
        "\n",
        "```python\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "class HFImageGenerator:\n",
        "    def __init__(self, model_id=\"runwayml/stable-diffusion-v1-5\"):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        dtype = torch.float16 if self.device == \"cuda\" else torch.float32\n",
        "\n",
        "        self.pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=dtype,\n",
        "        ).to(self.device)\n",
        "\n",
        "    def generate(self, prompt: str, file_name: str = \"output.png\"):\n",
        "        image = self.pipe(prompt).images[0]\n",
        "        image.save(file_name)\n",
        "        print(f\"‚úÖ Saved: {file_name}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    gen = HFImageGenerator()\n",
        "    gen.generate(\"a fantasy landscape with floating islands, matte painting\", \"fantasy.png\")\n",
        "```\n",
        "\n",
        "This is basically a **free image generator model using Hugging Face** that you can plug into Flask, FastAPI, Streamlit, etc.\n",
        "\n",
        "---\n",
        "\n",
        "## 6Ô∏è‚É£ If You Tell Me Your Goal‚Ä¶\n",
        "\n",
        "For example:\n",
        "\n",
        "* ‚ÄúUse it in **Flask API**‚Äù\n",
        "* ‚ÄúUse it in **Streamlit app**‚Äù\n",
        "* ‚ÄúUse it in a **Jupyter Notebook** as a teaching demo‚Äù\n",
        "\n",
        "‚Ä¶I can give you **full project code** (app.py + requirements.txt + sample prompts) tailored to that.\n",
        "\n",
        "For now, you can copy-paste any of the above blocks and run them directly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HTGc22aIcmu"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aienv310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
