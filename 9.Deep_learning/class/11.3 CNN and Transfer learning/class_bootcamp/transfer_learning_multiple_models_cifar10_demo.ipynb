{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f3933fb",
   "metadata": {},
   "source": [
    "# üìò Transfer Learning Comparison ‚Äì Multiple Models on CIFAR-10 (Keras)\n",
    "\n",
    "In this notebook, you can **show students end-to-end transfer learning** on **real data**\n",
    "using **multiple pretrained CNN architectures**:\n",
    "\n",
    "- VGG16\n",
    "- ResNet50V2\n",
    "- InceptionV3\n",
    "- Xception\n",
    "- DenseNet121\n",
    "- MobileNetV2\n",
    "\n",
    "For each model we will:\n",
    "1. Prepare data (CIFAR-10 subset: `cat`, `dog`, `horse`).\n",
    "2. Build a transfer learning model (pretrained on ImageNet).\n",
    "3. Train for a few epochs (demo-level, not full training).\n",
    "4. Evaluate test accuracy.\n",
    "5. Compare models on a bar chart.\n",
    "6. Show predictions from the **best model**.\n",
    "\n",
    "This is designed as a **teaching notebook**: simple, visual, and easy to extend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee35742f",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff76d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "from tensorflow.keras.applications import (\n",
    "    VGG16,\n",
    "    ResNet50V2,\n",
    "    InceptionV3,\n",
    "    Xception,\n",
    "    DenseNet121,\n",
    "    MobileNetV2,\n",
    ")\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg16_preprocess\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input as resnet50v2_preprocess\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input as inceptionv3_preprocess\n",
    "from tensorflow.keras.applications.xception import preprocess_input as xception_preprocess\n",
    "from tensorflow.keras.applications.densenet import preprocess_input as densenet_preprocess\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenetv2_preprocess\n",
    "\n",
    "print('TensorFlow:', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fadba6a",
   "metadata": {},
   "source": [
    "## 2. Load CIFAR-10 Dataset\n",
    "\n",
    "We will:\n",
    "- Load CIFAR-10 (60k images of size 32√ó32√ó3).\n",
    "- Use only 3 classes for a fast demo:\n",
    "  - `cat` (label 3)\n",
    "  - `dog` (label 5)\n",
    "  - `horse` (label 7)\n",
    "- Subsample the dataset to keep training quick.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42b7d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "class_names_full = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                    'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "print('Original train shape:', X_train.shape, y_train.shape)\n",
    "print('Original test shape:', X_test.shape, y_test.shape)\n",
    "print('All classes:', class_names_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a75955",
   "metadata": {},
   "source": [
    "### 2.1 Visualise Some Original CIFAR-10 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d53b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "for i in range(12):\n",
    "    plt.subplot(3, 4, i + 1)\n",
    "    plt.imshow(X_train[i])\n",
    "    plt.axis('off')\n",
    "    plt.title(class_names_full[int(y_train[i])])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf85c37",
   "metadata": {},
   "source": [
    "### 2.2 Filter to 3 Classes: `cat`, `dog`, `horse`\n",
    "\n",
    "We keep only labels 3, 5, 7 and remap them to **0, 1, 2**.\n",
    "\n",
    "- `0 ‚Üí cat`\n",
    "- `1 ‚Üí dog`\n",
    "- `2 ‚Üí horse`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b92400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_classes = [3, 5, 7]  # cat, dog, horse\n",
    "\n",
    "def filter_classes(X, y, selected):\n",
    "    idx = np.isin(y, selected).flatten()\n",
    "    X_sel = X[idx]\n",
    "    y_sel = y[idx]\n",
    "    label_map = {orig: i for i, orig in enumerate(selected)}\n",
    "    y_sel_mapped = np.vectorize(label_map.get)(y_sel)\n",
    "    return X_sel, y_sel_mapped\n",
    "\n",
    "X_train_sel, y_train_sel = filter_classes(X_train, y_train, selected_classes)\n",
    "X_test_sel, y_test_sel = filter_classes(X_test, y_test, selected_classes)\n",
    "\n",
    "print('Filtered train shape:', X_train_sel.shape, y_train_sel.shape)\n",
    "print('Filtered test shape:', X_test_sel.shape, y_test_sel.shape)\n",
    "print('Unique filtered labels:', np.unique(y_train_sel))\n",
    "demo_class_names = ['cat', 'dog', 'horse']\n",
    "print('Demo classes:', demo_class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3b814e",
   "metadata": {},
   "source": [
    "### 2.3 Subsample for Fast Demo\n",
    "\n",
    "To run multiple models in one notebook live, we'll restrict the dataset size.\n",
    "You can increase these numbers on a stronger machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9204742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample(X, y, n):\n",
    "    n = min(n, X.shape[0])\n",
    "    idx = np.random.permutation(X.shape[0])[:n]\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "X_train_small, y_train_small = subsample(X_train_sel, y_train_sel, 3000)\n",
    "X_test_small, y_test_small = subsample(X_test_sel, y_test_sel, 900)\n",
    "\n",
    "print('Small train shape:', X_train_small.shape, y_train_small.shape)\n",
    "print('Small test shape:', X_test_small.shape, y_test_small.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4562b37",
   "metadata": {},
   "source": [
    "## 3. Helper Functions\n",
    "\n",
    "We now write helper functions to:\n",
    "- Resize & preprocess images for each model.\n",
    "- Build a transfer learning model.\n",
    "- Train and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917f157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 3\n",
    "\n",
    "def prepare_data_for_model(X_train, y_train, X_test, y_test, input_size, preprocess_fn):\n",
    "    \"\"\"Resize and preprocess data for a specific model.\n",
    "\n",
    "    input_size: (H, W)\n",
    "    preprocess_fn: Keras preprocess_input function for that model.\n",
    "    \"\"\"\n",
    "    H, W = input_size\n",
    "    # Resize from 32x32 -> HxW\n",
    "    X_train_resized = tf.image.resize(X_train, (H, W)).numpy()\n",
    "    X_test_resized = tf.image.resize(X_test, (H, W)).numpy()\n",
    "\n",
    "    # Apply model-specific preprocessing\n",
    "    X_train_pp = preprocess_fn(X_train_resized)\n",
    "    X_test_pp = preprocess_fn(X_test_resized)\n",
    "\n",
    "    y_train_oh = to_categorical(y_train, NUM_CLASSES)\n",
    "    y_test_oh = to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "    return X_train_pp, y_train_oh, X_test_pp, y_test_oh\n",
    "\n",
    "\n",
    "def build_tl_model(base_model_fn, input_size, preprocess_fn, name='model'):\n",
    "    \"\"\"Build a transfer learning model using a Keras applications backbone.\n",
    "\n",
    "    base_model_fn: function that returns a keras Model with include_top=False\n",
    "    input_size: (H, W)\n",
    "    preprocess_fn: kept for reference (not used here directly)\n",
    "    \"\"\"\n",
    "    H, W = input_size\n",
    "\n",
    "    base_model = base_model_fn(weights='imagenet', include_top=False,\n",
    "                               input_shape=(H, W, 3))\n",
    "    base_model.trainable = False  # start with feature extraction\n",
    "\n",
    "    inputs = layers.Input(shape=(H, W, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs, name=name)\n",
    "    model.compile(optimizer=Adam(1e-3),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def plot_history(history, title_prefix='Model'):\n",
    "    hist = history.history\n",
    "    epochs = range(1, len(hist['loss']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, hist['loss'], label='Train Loss')\n",
    "    plt.plot(epochs, hist['val_loss'], label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'{title_prefix} - Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, hist['accuracy'], label='Train Acc')\n",
    "    plt.plot(epochs, hist['val_accuracy'], label='Val Acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'{title_prefix} - Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_predictions(X_raw, X_pp, y_true, model, class_labels, n=9, title_prefix=''):\n",
    "    preds = model.predict(X_pp[:n])\n",
    "    y_pred = np.argmax(preds, axis=1)\n",
    "    y_true_flat = y_true[:n]\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(n):\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(X_raw[i])\n",
    "        plt.axis('off')\n",
    "        true_label = class_labels[int(y_true_flat[i])]\n",
    "        pred_label = class_labels[int(y_pred[i])]\n",
    "        conf = np.max(preds[i])\n",
    "        plt.title(f'T:{true_label} | P:{pred_label}\\n({conf:.2f})')\n",
    "    plt.suptitle(title_prefix)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40ea7f3",
   "metadata": {},
   "source": [
    "## 4. Define Model Configurations\n",
    "\n",
    "We prepare a list of model configurations so we can **loop over all models**:\n",
    "\n",
    "- Input size (224√ó224 or 299√ó299)\n",
    "- Backbone function\n",
    "- Preprocess function\n",
    "- A short description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a92b1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIGS = [\n",
    "    {\n",
    "        'name': 'VGG16',\n",
    "        'input_size': (224, 224),\n",
    "        'base_fn': VGG16,\n",
    "        'preprocess_fn': vgg16_preprocess,\n",
    "        'description': 'Classic deep CNN, good feature extractor but heavy.'\n",
    "    },\n",
    "    {\n",
    "        'name': 'ResNet50V2',\n",
    "        'input_size': (224, 224),\n",
    "        'base_fn': ResNet50V2,\n",
    "        'preprocess_fn': resnet50v2_preprocess,\n",
    "        'description': 'Residual network with skip connections, strong baseline.'\n",
    "    },\n",
    "    {\n",
    "        'name': 'InceptionV3',\n",
    "        'input_size': (299, 299),\n",
    "        'base_fn': InceptionV3,\n",
    "        'preprocess_fn': inceptionv3_preprocess,\n",
    "        'description': 'Multi-scale convs, good accuracy vs compute.'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Xception',\n",
    "        'input_size': (299, 299),\n",
    "        'base_fn': Xception,\n",
    "        'preprocess_fn': xception_preprocess,\n",
    "        'description': 'Depthwise separable convs, powerful but heavier.'\n",
    "    },\n",
    "    {\n",
    "        'name': 'DenseNet121',\n",
    "        'input_size': (224, 224),\n",
    "        'base_fn': DenseNet121,\n",
    "        'preprocess_fn': densenet_preprocess,\n",
    "        'description': 'Dense connections, parameter-efficient.'\n",
    "    },\n",
    "    {\n",
    "        'name': 'MobileNetV2',\n",
    "        'input_size': (224, 224),\n",
    "        'base_fn': MobileNetV2,\n",
    "        'preprocess_fn': mobilenetv2_preprocess,\n",
    "        'description': 'Lightweight, fast, great for mobile/edge.'\n",
    "    },\n",
    "]\n",
    "\n",
    "for cfg in MODEL_CONFIGS:\n",
    "    print(f\"{cfg['name']}: input_size={cfg['input_size']} - {cfg['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea99ba07",
   "metadata": {},
   "source": [
    "## 5. Train & Evaluate All Models (Feature Extraction)\n",
    "\n",
    "For each model configuration:\n",
    "\n",
    "1. Resize + preprocess CIFAR-10 subset to required input size.\n",
    "2. Build a transfer learning model with frozen backbone.\n",
    "3. Train for a **small number of epochs** (demo only).\n",
    "4. Evaluate on the test set.\n",
    "5. Store accuracy for comparison.\n",
    "\n",
    "‚ö†Ô∏è **Note:** Running all models may take a few minutes depending on your machine. You can:\n",
    "- Reduce `EPOCHS_PER_MODEL`\n",
    "- Reduce subsample sizes above\n",
    "- Comment out some configs in `MODEL_CONFIGS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa8db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_PER_MODEL = 3   # adjust as needed\n",
    "BATCH_SIZE = 32\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "results = []\n",
    "trained_models = {}\n",
    "preprocessed_data_cache = {}  # cache preprocessed data per (H,W) to save time\n",
    "\n",
    "for cfg in MODEL_CONFIGS:\n",
    "    name = cfg['name']\n",
    "    input_size = cfg['input_size']\n",
    "    base_fn = cfg['base_fn']\n",
    "    preprocess_fn = cfg['preprocess_fn']\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Training model: {name} (input_size={input_size})\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Cache preprocessed data per input size\n",
    "    if input_size in preprocessed_data_cache:\n",
    "        X_train_pp, y_train_oh, X_test_pp, y_test_oh = preprocessed_data_cache[input_size]\n",
    "    else:\n",
    "        X_train_pp, y_train_oh, X_test_pp, y_test_oh = prepare_data_for_model(\n",
    "            X_train_small, y_train_small,\n",
    "            X_test_small, y_test_small,\n",
    "            input_size,\n",
    "            preprocess_fn,\n",
    "        )\n",
    "        preprocessed_data_cache[input_size] = (X_train_pp, y_train_oh, X_test_pp, y_test_oh)\n",
    "\n",
    "    model = build_tl_model(base_fn, input_size, preprocess_fn, name=name)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_pp, y_train_oh,\n",
    "        epochs=EPOCHS_PER_MODEL,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=VAL_SPLIT,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(X_test_pp, y_test_oh, verbose=0)\n",
    "    print(f\"\\n[{name}] Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    results.append({'model': name, 'test_loss': float(test_loss), 'test_acc': float(test_acc)})\n",
    "    trained_models[name] = {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'input_size': input_size,\n",
    "        'preprocess_fn': preprocess_fn,\n",
    "    }\n",
    "\n",
    "print(\"\\nAll models trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ae1cc2",
   "metadata": {},
   "source": [
    "## 6. Compare Model Accuracies\n",
    "\n",
    "Let's show test accuracies for all models in a table and a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162feb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.sort_values('test_acc', ascending=False, inplace=True)\n",
    "display(results_df)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(results_df['model'], results_df['test_acc'])\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Transfer Learning ‚Äì CIFAR-10 (cat/dog/horse)')\n",
    "plt.xticks(rotation=30)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbbeb64",
   "metadata": {},
   "source": [
    "## 7. Visualise Training Curves & Predictions for Best Model\n",
    "\n",
    "We'll pick the **best test accuracy** model and:\n",
    "\n",
    "- Plot its training vs validation loss/accuracy curves.\n",
    "- Show **input images** with predicted vs true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e1b394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick best model by test accuracy\n",
    "best_row = results_df.iloc[0]\n",
    "best_name = best_row['model']\n",
    "print('Best model:', best_name)\n",
    "\n",
    "best_info = trained_models[best_name]\n",
    "best_model = best_info['model']\n",
    "best_history = best_info['history']\n",
    "best_input_size = best_info['input_size']\n",
    "best_preprocess_fn = best_info['preprocess_fn']\n",
    "\n",
    "plot_history(best_history, title_prefix=f'{best_name}')\n",
    "\n",
    "# Prepare data again for that model (from raw small arrays)\n",
    "X_train_pp_best, y_train_oh_best, X_test_pp_best, y_test_oh_best = prepare_data_for_model(\n",
    "    X_train_small, y_train_small,\n",
    "    X_test_small, y_test_small,\n",
    "    best_input_size,\n",
    "    best_preprocess_fn,\n",
    ")\n",
    "\n",
    "show_predictions(\n",
    "    X_test_small,\n",
    "    X_test_pp_best,\n",
    "    y_test_small,\n",
    "    best_model,\n",
    "    demo_class_names,\n",
    "    n=9,\n",
    "    title_prefix=f'Best model: {best_name}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ea0c6c",
   "metadata": {},
   "source": [
    "## 8. Optional: Fine-Tune the Best Model\n",
    "\n",
    "To show students **fine-tuning**, you can unfreeze some layers of the best model's backbone\n",
    "and retrain with a **lower learning rate**.\n",
    "\n",
    "This section is optional and may take more time.\n",
    "You can skip in live demos or run with fewer epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: fine-tune last N layers of the backbone for the best model\n",
    "# (Run only if you want to demonstrate fine-tuning.)\n",
    "\n",
    "FINE_TUNE_EPOCHS = 3\n",
    "FINE_TUNE_LR = 1e-5\n",
    "\n",
    "# Extract base model from best_model (first layer after Input)\n",
    "base_layer = best_model.layers[1]\n",
    "print('Backbone layer name:', base_layer.name)\n",
    "\n",
    "# Unfreeze backbone\n",
    "base_layer.trainable = True\n",
    "\n",
    "# Optionally, freeze all but last 30 layers\n",
    "if hasattr(base_layer, 'layers'):\n",
    "    for l in base_layer.layers[:-30]:\n",
    "        l.trainable = False\n",
    "\n",
    "best_model.compile(\n",
    "    optimizer=Adam(FINE_TUNE_LR),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print('Fine-tuning the best model...')\n",
    "history_ft = best_model.fit(\n",
    "    X_train_pp_best, y_train_oh_best,\n",
    "    epochs=FINE_TUNE_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=VAL_SPLIT,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "test_loss_ft, test_acc_ft = best_model.evaluate(X_test_pp_best, y_test_oh_best, verbose=0)\n",
    "print(f'[Fine-tuned {best_name}] Test Loss: {test_loss_ft:.4f}, Test Accuracy: {test_acc_ft:.4f}')\n",
    "\n",
    "plot_history(history_ft, title_prefix=f'{best_name} (Fine-tuned)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003919b5",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. How to Use This Notebook in Class\n",
    "\n",
    "- Start from section 2 ‚Üí explain dataset.\n",
    "- Show how we **reuse knowledge** from ImageNet models.\n",
    "- Run section 5 for a couple of models (or all, if time allows).\n",
    "- Discuss the **accuracy comparison** in section 6.\n",
    "- Use section 7 to visually show **predictions**.\n",
    "- (Optional) Run fine-tuning to show how accuracy can further improve.\n",
    "\n",
    "You can ask students to:\n",
    "- Change which CIFAR-10 classes are used.\n",
    "- Add/remove models from `MODEL_CONFIGS`.\n",
    "- Change number of epochs / batch size.\n",
    "- Add **data augmentation** for better generalisation.\n",
    "\n",
    "This gives a **unified, visual, hands-on** understanding of transfer learning across many backbones. üß†üìä"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
