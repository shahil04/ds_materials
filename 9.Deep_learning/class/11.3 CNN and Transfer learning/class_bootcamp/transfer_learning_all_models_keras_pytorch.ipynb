{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6e8e8a1",
   "metadata": {},
   "source": [
    "# üìò TRANSFER LEARNING ‚Äì PRACTICAL GUIDE (Keras + PyTorch)\n",
    "\n",
    "### Apply AlexNet, VGG, GoogLeNet, ResNet, Inception, Xception, MobileNet to Real Projects\n",
    "\n",
    "This notebook collects **ready-to-use templates** for transfer learning using multiple\n",
    "popular CNN architectures in **TensorFlow/Keras** and **PyTorch**.\n",
    "\n",
    "Use it as:\n",
    "- Teaching material for students\n",
    "- A personal reference while building image projects\n",
    "- A starting point for custom transfer learning pipelines\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669081f3",
   "metadata": {},
   "source": [
    "## üåç 1. What is Transfer Learning?\n",
    "\n",
    "**Transfer Learning** means:\n",
    "- Start from a **pretrained model** (usually trained on ImageNet: 1.2M images, 1000 classes).\n",
    "- Reuse it as a **feature extractor** or fine-tune it for a **new task**.\n",
    "\n",
    "Typical workflow:\n",
    "1. Load a pretrained backbone (e.g., ResNet50, VGG16, MobileNetV2).\n",
    "2. **Freeze** some or all of the base layers.\n",
    "3. **Replace** the final classifier head with a new one for your number of classes.\n",
    "4. Train on your custom dataset (cats vs dogs, medical images, etc.).\n",
    "5. Optionally **unfreeze deeper layers** for fine-tuning.\n",
    "\n",
    "Transfer learning works best when:\n",
    "- Your dataset is smaller than ImageNet.\n",
    "- Your data is somewhat similar to natural images.\n",
    "- You want good accuracy without training from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e4d69",
   "metadata": {},
   "source": [
    "## üß± 2. General Template ‚Äì Keras (TensorFlow)\n",
    "\n",
    "Below is a **generic transfer learning template** you can adapt for any Keras application model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a1d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Example: ResNet50 backbone\n",
    "num_classes = 5  # change to your number of classes\n",
    "\n",
    "base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base.trainable = False  # Freeze backbone for feature extraction\n",
    "\n",
    "x = GlobalAveragePooling2D()(base.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "out = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base.input, outputs=out)\n",
    "model.compile(optimizer=Adam(1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee01332",
   "metadata": {},
   "source": [
    "## üß± 3. General Template ‚Äì PyTorch\n",
    "\n",
    "Generic pattern for using a pretrained model (ResNet50 example) as a feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fabc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_classes = 5  # change to your number of classes\n",
    "\n",
    "# Load pretrained ResNet50\n",
    "model = models.resnet50(weights=\"IMAGENET1K_V2\")\n",
    "\n",
    "# Freeze all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the final fully-connected layer\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(256, num_classes)\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-3)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb065fc1",
   "metadata": {},
   "source": [
    "---\n",
    "# üß© 4. Transfer Learning per Architecture\n",
    "\n",
    "For each model below, we show:\n",
    "- Short notes / when to use\n",
    "- Keras snippet (if available)\n",
    "- PyTorch snippet\n",
    "\n",
    "‚ö† These are **skeletons** ‚Äì you still need to plug in your own dataloaders / generators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c193545a",
   "metadata": {},
   "source": [
    "## üîπ A. AlexNet (2012)\n",
    "\n",
    "**Notes**\n",
    "- Historically important; first big ImageNet winner using deep CNN.\n",
    "- Today, mainly used for teaching and small experiments.\n",
    "- Not built-in in Keras; available in PyTorch.\n",
    "\n",
    "### ‚úÖ PyTorch ‚Äì AlexNet Transfer Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d960e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "num_classes = 5  # your number of classes\n",
    "\n",
    "alexnet = models.alexnet(weights=\"IMAGENET1K_V1\")\n",
    "\n",
    "# Freeze all features\n",
    "for param in alexnet.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the final classifier layer\n",
    "in_features = alexnet.classifier[6].in_features\n",
    "alexnet.classifier[6] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(alexnet.classifier.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483b6f2b",
   "metadata": {},
   "source": [
    "### ‚úÖ Keras ‚Äì Use VGG16 as AlexNet Alternative\n",
    "\n",
    "Keras doesn‚Äôt ship AlexNet; for teaching, you can use **VGG16** as a similar\n",
    "‚Äúold-school‚Äù deep CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9443d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "num_classes = 5\n",
    "\n",
    "base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base.trainable = False\n",
    "\n",
    "x = GlobalAveragePooling2D()(base.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "out = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "vgg_model = Model(inputs=base.input, outputs=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf54c98",
   "metadata": {},
   "source": [
    "## üîπ B. ZFNet (2013)\n",
    "\n",
    "ZFNet is not directly available in Keras or torchvision. For practical work,\n",
    "we usually **replace it with VGG or ResNet**.\n",
    "\n",
    "**Recommendation**: Use **VGG16/VGG19** / **ResNet50** instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43ee9f8",
   "metadata": {},
   "source": [
    "## üîπ C. VGG16 / VGG19 (2014)\n",
    "\n",
    "**Notes**\n",
    "- Deep, simple architecture (stacked 3√ó3 convs).\n",
    "- Great as a **feature extractor**.\n",
    "- Heavy (large number of parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a04086",
   "metadata": {},
   "source": [
    "### ‚úÖ Keras ‚Äì VGG16 Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc06f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "num_classes = 5\n",
    "\n",
    "vgg_base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "vgg_base.trainable = False\n",
    "\n",
    "x = GlobalAveragePooling2D()(vgg_base.output)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "out = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "vgg_model = Model(inputs=vgg_base.input, outputs=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d2610b",
   "metadata": {},
   "source": [
    "### ‚úÖ PyTorch ‚Äì VGG16 Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b199b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "num_classes = 5\n",
    "\n",
    "vgg16 = models.vgg16(weights=\"IMAGENET1K_V1\")\n",
    "\n",
    "# Freeze features\n",
    "for param in vgg16.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace final classifier layer\n",
    "in_features = vgg16.classifier[6].in_features\n",
    "vgg16.classifier[6] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(vgg16.classifier.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8690cd37",
   "metadata": {},
   "source": [
    "## üîπ D. GoogLeNet / Inception v1\n",
    "\n",
    "**Notes**\n",
    "- Introduced **Inception modules** with multi-scale convolutions.\n",
    "- Efficient for its time.\n",
    "- In practice, we now prefer **InceptionV3** (newer) or other modern models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72c908a",
   "metadata": {},
   "source": [
    "### ‚úÖ PyTorch ‚Äì GoogLeNet Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9813b2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "googlenet = models.googlenet(weights=\"IMAGENET1K_V1\")\n",
    "\n",
    "# Replace the final FC layer\n",
    "googlenet.fc = nn.Linear(googlenet.fc.in_features, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(googlenet.fc.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437d3986",
   "metadata": {},
   "source": [
    "### ‚úÖ Keras ‚Äì Use InceptionV3 as Successor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f24b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import InceptionV3\n",
    "\n",
    "inception_base = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
    "inception_base.trainable = False\n",
    "\n",
    "x = GlobalAveragePooling2D()(inception_base.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "out = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "inception_model = Model(inputs=inception_base.input, outputs=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e743e3",
   "metadata": {},
   "source": [
    "## üîπ E. InceptionV3 (2015)\n",
    "\n",
    "**Notes**\n",
    "- High accuracy with good efficiency.\n",
    "- Uses factorized convolutions (e.g., 3√ó3 split into 1√ó3 + 3√ó1).\n",
    "- Good general-purpose backbone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748c7b5c",
   "metadata": {},
   "source": [
    "### ‚úÖ Keras ‚Äì InceptionV3 Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37c6dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import InceptionV3\n",
    "\n",
    "inception_base = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
    "inception_base.trainable = False\n",
    "\n",
    "x = GlobalAveragePooling2D()(inception_base.output)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "out = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "inception_model = Model(inputs=inception_base.input, outputs=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d28e52d",
   "metadata": {},
   "source": [
    "### ‚úÖ PyTorch ‚Äì InceptionV3 Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d02f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "inception3 = models.inception_v3(weights=\"IMAGENET1K_V1\")\n",
    "\n",
    "# Replace final FC\n",
    "inception3.fc = nn.Linear(inception3.fc.in_features, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(inception3.fc.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bcd369",
   "metadata": {},
   "source": [
    "## üîπ F. Xception (2017)\n",
    "\n",
    "**Notes**\n",
    "- Based on **depthwise separable convolutions**.\n",
    "- Very strong performance on many image tasks.\n",
    "- Officially in Keras; PyTorch uses alternative implementations (e.g., from repos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e9a1ea",
   "metadata": {},
   "source": [
    "### ‚úÖ Keras ‚Äì Xception Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9856d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import Xception\n",
    "\n",
    "xception_base = Xception(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
    "xception_base.trainable = False\n",
    "\n",
    "x = GlobalAveragePooling2D()(xception_base.output)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "out = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "xception_model = Model(inputs=xception_base.input, outputs=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2928bb8",
   "metadata": {},
   "source": [
    "### ‚úÖ PyTorch\n",
    "\n",
    "Xception is not in `torchvision.models` by default.\n",
    "- You can:\n",
    "  - Use **MobileNet** / **EfficientNet** as modern light-weight alternatives, or\n",
    "  - Install a third-party implementation from GitHub.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661ffc1a",
   "metadata": {},
   "source": [
    "## üîπ G. ResNet50 / ResNet50V2\n",
    "\n",
    "**Notes**\n",
    "- Uses **residual connections** (skip connections).\n",
    "- Stable training even when very deep.\n",
    "- Excellent general-purpose backbone.\n",
    "- Common default choice when you don't know what to pick.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6b11dd",
   "metadata": {},
   "source": [
    "### ‚úÖ Keras ‚Äì ResNet50V2 Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837d5309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50V2\n",
    "\n",
    "resnet_base = ResNet50V2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "resnet_base.trainable = False\n",
    "\n",
    "x = GlobalAveragePooling2D()(resnet_base.output)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "out = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "resnet_model = Model(inputs=resnet_base.input, outputs=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae09747",
   "metadata": {},
   "source": [
    "### ‚úÖ PyTorch ‚Äì ResNet50 Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c64a853",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = models.resnet50(weights=\"IMAGENET1K_V2\")\n",
    "\n",
    "# Freeze all layers\n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace FC\n",
    "resnet50.fc = nn.Linear(resnet50.fc.in_features, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(resnet50.fc.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af663e1e",
   "metadata": {},
   "source": [
    "## üîπ H. DenseNet\n",
    "\n",
    "**Notes**\n",
    "- Each layer receives input from **all previous layers** in a block.\n",
    "- Very parameter-efficient and strong feature extractor.\n",
    "- Good alternative when ResNet is overkill.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f907a0",
   "metadata": {},
   "source": [
    "### ‚úÖ Keras ‚Äì DenseNet121 Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d63a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import DenseNet121\n",
    "\n",
    "densenet_base = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "densenet_base.trainable = False\n",
    "\n",
    "x = GlobalAveragePooling2D()(densenet_base.output)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "out = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "densenet_model = Model(inputs=densenet_base.input, outputs=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38755bb9",
   "metadata": {},
   "source": [
    "### ‚úÖ PyTorch ‚Äì DenseNet121 Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a38808",
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet121 = models.densenet121(weights=\"IMAGENET1K_V1\")\n",
    "\n",
    "# Replace classifier\n",
    "densenet121.classifier = nn.Linear(densenet121.classifier.in_features, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(densenet121.classifier.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2141158d",
   "metadata": {},
   "source": [
    "## üîπ I. MobileNet (V1/V2/V3)\n",
    "\n",
    "**Notes**\n",
    "- Designed for **mobile / edge devices**.\n",
    "- Uses depthwise separable convolutions.\n",
    "- Excellent trade-off between speed and accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c88224",
   "metadata": {},
   "source": [
    "### ‚úÖ Keras ‚Äì MobileNetV2 Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3709d47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "mobilenet_base = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "mobilenet_base.trainable = False\n",
    "\n",
    "x = GlobalAveragePooling2D()(mobilenet_base.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "out = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "mobilenet_model = Model(inputs=mobilenet_base.input, outputs=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bd2e97",
   "metadata": {},
   "source": [
    "### ‚úÖ PyTorch ‚Äì MobileNetV2 Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc2af60",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_v2 = models.mobilenet_v2(weights=\"IMAGENET1K_V1\")\n",
    "\n",
    "# Replace classifier\n",
    "in_features = mobilenet_v2.classifier[1].in_features\n",
    "mobilenet_v2.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mobilenet_v2.classifier.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d160e",
   "metadata": {},
   "source": [
    "---\n",
    "# üß™ 5. Optional Fine-Tuning Stage\n",
    "\n",
    "After training only the top classifier head, you might want to **unfreeze some deeper layers**\n",
    "to adapt the features more to your domain. This is called **fine-tuning**.\n",
    "\n",
    "Typical workflow:\n",
    "1. Train with base frozen ‚Üí stabilize classifier.\n",
    "2. Unfreeze last N layers of the base.\n",
    "3. Retrain with a **smaller learning rate**.\n",
    "\n",
    "### üîÅ Keras Fine-Tuning Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90dacf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume `base` is your pretrained backbone (e.g., ResNet50) and `model` is built.\n",
    "\n",
    "# 1. Unfreeze the last 30 layers of the base model\n",
    "for layer in base.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "for layer in base.layers[-30:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# 2. Compile with a lower learning rate for fine-tuning\n",
    "model.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e8e211",
   "metadata": {},
   "source": [
    "### üîÅ PyTorch Fine-Tuning Example (ResNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d152c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: fine-tuning last residual block (layer4) of ResNet50\n",
    "\n",
    "# 1. Freeze all\n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2. Unfreeze layer4\n",
    "for param in resnet50.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 3. Optimize only trainable parameters\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, resnet50.parameters()), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d736779f",
   "metadata": {},
   "source": [
    "---\n",
    "# üìå 6. Data Loading ‚Äì Keras vs PyTorch\n",
    "\n",
    "Here are quick templates to load image data from **folders** for transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdd4c13",
   "metadata": {},
   "source": [
    "### üóÇ Keras ‚Äì ImageDataGenerator from Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a10ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "img_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1/255.,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    'data/train',\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_gen = val_datagen.flow_from_directory(\n",
    "    'data/val',\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e847da71",
   "metadata": {},
   "source": [
    "### üóÇ PyTorch ‚Äì Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95505cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder('data/train', transform=transform_train)\n",
    "val_dataset = datasets.ImageFolder('data/val', transform=transform_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40288857",
   "metadata": {},
   "source": [
    "---\n",
    "# üî• 7. Which Model Should I Use?\n",
    "\n",
    "| Use Case                       | Recommended Models                          |\n",
    "|-------------------------------|---------------------------------------------|\n",
    "| **Small datasets**            | VGG16, ResNet50                             |\n",
    "| **High accuracy**             | ResNet50V2, InceptionV3, Xception           |\n",
    "| **Fast & light**              | MobileNetV2, EfficientNetB0                 |\n",
    "| **Mobile / Edge deployment**  | MobileNetV2/V3, EfficientNet-Lite           |\n",
    "| **Very deep feature learning**| ResNet101/152, DenseNet121/169              |\n",
    "\n",
    "General rule of thumb:\n",
    "- Start with **ResNet50** or **MobileNetV2**.\n",
    "- If overfitting ‚Üí add regularization / data augmentation.\n",
    "- If too slow ‚Üí switch to MobileNet / smaller EfficientNet.\n",
    "- If accuracy too low ‚Üí try deeper ResNet / Xception / InceptionV3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fb1def",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Next Steps / Ideas\n",
    "\n",
    "- Wrap these templates into functions (e.g., `get_model('resnet', num_classes)`).\n",
    "- Build a **unified training script** that:\n",
    "  - Takes model name as argument\n",
    "  - Loads corresponding backbone\n",
    "  - Trains and logs metrics\n",
    "- Add **experiment tracking** (Weights & Biases, MLflow, TensorBoard).\n",
    "- Compare different models on the **same dataset** and plot:\n",
    "  - Accuracy\n",
    "  - Training time\n",
    "  - Inference speed.\n",
    "\n",
    "Use this notebook as a **base template** for your transfer learning experiments üí™."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
