{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "750cc198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting diffusers\n",
      "  Using cached diffusers-0.33.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: transformers in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (4.50.3)\n",
      "Requirement already satisfied: torch in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (2.6.0)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from diffusers) (8.6.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from diffusers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.27.0 in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from diffusers) (0.33.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from diffusers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from diffusers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from diffusers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from diffusers) (0.5.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from diffusers) (11.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from torch) (4.13.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from importlib-metadata->diffusers) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from requests->diffusers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from requests->diffusers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from requests->diffusers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\.conda\\envs\\aienv\\lib\\site-packages (from requests->diffusers) (2025.1.31)\n",
      "Using cached diffusers-0.33.1-py3-none-any.whl (3.6 MB)\n",
      "Using cached accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "Installing collected packages: diffusers, accelerate\n",
      "Successfully installed accelerate-1.7.0 diffusers-0.33.1\n"
     ]
    }
   ],
   "source": [
    "### 11. Text to Image\n",
    "\n",
    "# Text-to-image generation creates an image from a textual description. This often utilises diffusion models, which are a class of generative models. The Hugging Face **Diffusers library** is an open-source Python library focusing on diffusion models for generating images, audio, and other data types. **Stable Diffusion** is a popular latent diffusion model within the Diffusers library for high-quality image generation from text prompts.\n",
    "\n",
    "# **Example: Generating an Image from Text**\n",
    "\n",
    "\n",
    "# 1. Install required libraries\n",
    "!pip install diffusers transformers torch accelerate\n",
    "# 'accelerate' is often needed for optimisations when running diffusers models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57ea8597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hp\\.cache\\huggingface\\hub\\models--runwayml--stable-diffusion-v1-5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 15 files:  20%|██        | 3/15 [4:27:53<17:51:33, 5357.83s/it]\n"
     ]
    },
    {
     "ename": "ChunkedEncodingError",
     "evalue": "('Connection broken: IncompleteRead(681459697 bytes read, 534522133 more expected)', IncompleteRead(681459697 bytes read, 534522133 more expected))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIncompleteRead\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\urllib3\\response.py:754\u001b[39m, in \u001b[36mHTTPResponse._error_catcher\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    753\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m754\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    757\u001b[39m     \u001b[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;66;03m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\urllib3\\response.py:900\u001b[39m, in \u001b[36mHTTPResponse._raw_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    890\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    891\u001b[39m         \u001b[38;5;28mself\u001b[39m.enforce_content_length\n\u001b[32m    892\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length_remaining \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    898\u001b[39m         \u001b[38;5;66;03m# raised during streaming, so all calls with incorrect\u001b[39;00m\n\u001b[32m    899\u001b[39m         \u001b[38;5;66;03m# Content-Length are caught.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m900\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(\u001b[38;5;28mself\u001b[39m._fp_bytes_read, \u001b[38;5;28mself\u001b[39m.length_remaining)\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m read1 \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m    902\u001b[39m     (amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length_remaining == \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[32m    903\u001b[39m ):\n\u001b[32m   (...)\u001b[39m\u001b[32m    906\u001b[39m     \u001b[38;5;66;03m# `http.client.HTTPResponse`, so we close it here.\u001b[39;00m\n\u001b[32m    907\u001b[39m     \u001b[38;5;66;03m# See https://github.com/python/cpython/issues/113199\u001b[39;00m\n",
      "\u001b[31mIncompleteRead\u001b[39m: IncompleteRead(681459697 bytes read, 534522133 more expected)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mProtocolError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\requests\\models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    819\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\urllib3\\response.py:1066\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\urllib3\\response.py:983\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt, decode_content, cache_content)\u001b[39m\n\u001b[32m    979\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) < amt \u001b[38;5;129;01mand\u001b[39;00m data:\n\u001b[32m    980\u001b[39m     \u001b[38;5;66;03m# TODO make sure to initially read enough data to get past the headers\u001b[39;00m\n\u001b[32m    981\u001b[39m     \u001b[38;5;66;03m# For example, the GZ file header takes 10 bytes, we don't want to read\u001b[39;00m\n\u001b[32m    982\u001b[39m     \u001b[38;5;66;03m# it one byte at a time\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m983\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    984\u001b[39m     decoded_data = \u001b[38;5;28mself\u001b[39m._decode(data, decode_content, flush_decoder)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\urllib3\\response.py:878\u001b[39m, in \u001b[36mHTTPResponse._raw_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    876\u001b[39m fp_closed = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._fp, \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._error_catcher():\n\u001b[32m    879\u001b[39m     data = \u001b[38;5;28mself\u001b[39m._fp_read(amt, read1=read1) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\urllib3\\response.py:778\u001b[39m, in \u001b[36mHTTPResponse._error_catcher\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    777\u001b[39m         arg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConnection broken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ProtocolError(arg, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (HTTPException, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mProtocolError\u001b[39m: ('Connection broken: IncompleteRead(681459697 bytes read, 534522133 more expected)', IncompleteRead(681459697 bytes read, 534522133 more expected))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mChunkedEncodingError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load a publicly available Stable Diffusion model (no access token needed).\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Ensure you accept the model's terms if prompted, which might require logging in to Hugging Face Hub (but not an access token for public models).\u001b[39;00m\n\u001b[32m      6\u001b[39m model_id = \u001b[33m\"\u001b[39m\u001b[33mrunwayml/stable-diffusion-v1-5\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m pipeline = \u001b[43mStableDiffusionPipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# If running on Colab GPU, move pipeline to CUDA for faster inference\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# pipeline.to(\"cuda\")\u001b[39;00m\n\u001b[32m     11\u001b[39m \n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# 3. Define the text prompt\u001b[39;00m\n\u001b[32m     13\u001b[39m prompt = \u001b[33m\"\u001b[39m\u001b[33mFlying cars soar over a futuristic cityscape at sunset.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\diffusers\\pipelines\\pipeline_utils.py:773\u001b[39m, in \u001b[36mDiffusionPipeline.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    768\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pretrained_model_name_or_path.count(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m) > \u001b[32m1\u001b[39m:\n\u001b[32m    769\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    770\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mThe provided pretrained_model_name_or_path \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    771\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m is neither a valid local path nor a valid repo id. Please check the parameter.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    772\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m     cached_folder = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    774\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    775\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    776\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_onnx\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_onnx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_pipeline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdduf_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdduf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m        \u001b[49m\u001b[43mload_connected_pipeline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_connected_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    792\u001b[39m     cached_folder = pretrained_model_name_or_path\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\diffusers\\pipelines\\pipeline_utils.py:1557\u001b[39m, in \u001b[36mDiffusionPipeline.download\u001b[39m\u001b[34m(cls, pretrained_model_name, **kwargs)\u001b[39m\n\u001b[32m   1555\u001b[39m \u001b[38;5;66;03m# download all allow_patterns - ignore_patterns\u001b[39;00m\n\u001b[32m   1556\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1557\u001b[39m     cached_folder = \u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1558\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1559\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1560\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1561\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1562\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1563\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_patterns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1565\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_patterns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1566\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1567\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1569\u001b[39m     cls_name = \u001b[38;5;28mcls\u001b[39m.load_config(os.path.join(cached_folder, \u001b[33m\"\u001b[39m\u001b[33mmodel_index.json\u001b[39m\u001b[33m\"\u001b[39m)).get(\u001b[33m\"\u001b[39m\u001b[33m_class_name\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1570\u001b[39m     cls_name = cls_name[\u001b[32m4\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cls_name, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m cls_name.startswith(\u001b[33m\"\u001b[39m\u001b[33mFlax\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m cls_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\huggingface_hub\\_snapshot_download.py:327\u001b[39m, in \u001b[36msnapshot_download\u001b[39m\u001b[34m(repo_id, repo_type, revision, cache_dir, local_dir, library_name, library_version, user_agent, proxies, etag_timeout, force_download, token, local_files_only, allow_patterns, ignore_patterns, max_workers, tqdm_class, headers, endpoint, local_dir_use_symlinks, resume_download)\u001b[39m\n\u001b[32m    325\u001b[39m         _inner_hf_hub_download(file)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[43mthread_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_inner_hf_hub_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfiltered_repo_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtqdm_desc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# User can use its own tqdm class or the default one from `huggingface_hub.utils`\u001b[39;49;00m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhf_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(os.path.realpath(local_dir))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\tqdm\\contrib\\concurrent.py:69\u001b[39m, in \u001b[36mthread_map\u001b[39m\u001b[34m(fn, *iterables, **tqdm_kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[33;03mEquivalent of `list(map(fn, *iterables))`\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[33;03mdriven by `concurrent.futures.ThreadPoolExecutor`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m \u001b[33;03m    [default: max(32, cpu_count() + 4)].\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconcurrent\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfutures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_executor_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\tqdm\\contrib\\concurrent.py:51\u001b[39m, in \u001b[36m_executor_map\u001b[39m\u001b[34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ensure_lock(tqdm_class, lock_name=lock_name) \u001b[38;5;28;01mas\u001b[39;00m lk:\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# share lock in case workers are already using `tqdm`\u001b[39;00m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers=max_workers, initializer=tqdm_class.set_lock,\n\u001b[32m     50\u001b[39m                       initargs=(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\concurrent\\futures\\_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\concurrent\\futures\\_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\concurrent\\futures\\_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\concurrent\\futures\\thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\huggingface_hub\\_snapshot_download.py:301\u001b[39m, in \u001b[36msnapshot_download.<locals>._inner_hf_hub_download\u001b[39m\u001b[34m(repo_file)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_inner_hf_hub_download\u001b[39m(repo_file: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1008\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    989\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    990\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1005\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   1006\u001b[39m     )\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1008\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1161\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1158\u001b[39m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[32m   1160\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[32m-> \u001b[39m\u001b[32m1161\u001b[39m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.incomplete\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1173\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(pointer_path):\n\u001b[32m   1174\u001b[39m         _create_symlink(blob_path, pointer_path, new_blob=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1725\u001b[39m, in \u001b[36m_download_to_tmp_and_move\u001b[39m\u001b[34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[39m\n\u001b[32m   1718\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1719\u001b[39m             logger.warning(\n\u001b[32m   1720\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mXet Storage is enabled for this repo, but the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhf_xet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m package is not installed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1721\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFalling back to regular HTTP download. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1722\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFor better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1723\u001b[39m             )\n\u001b[32m-> \u001b[39m\u001b[32m1725\u001b[39m         \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1726\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1727\u001b[39m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1728\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1729\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1730\u001b[39m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1731\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1732\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1734\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1735\u001b[39m _chmod_and_move(incomplete_path, destination_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\huggingface_hub\\file_download.py:494\u001b[39m, in \u001b[36mhttp_get\u001b[39m\u001b[34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[39m\n\u001b[32m    492\u001b[39m new_resume_size = resume_size\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDOWNLOAD_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# filter out keep-alive new chunks\u001b[39;49;00m\n\u001b[32m    496\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\requests\\models.py:822\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    820\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    824\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ContentDecodingError(e)\n",
      "\u001b[31mChunkedEncodingError\u001b[39m: ('Connection broken: IncompleteRead(681459697 bytes read, 534522133 more expected)', IncompleteRead(681459697 bytes read, 534522133 more expected))"
     ]
    }
   ],
   "source": [
    "# 2. Import necessary modules and load the Stable Diffusion pipeline\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# Load a publicly available Stable Diffusion model (no access token needed).\n",
    "# Ensure you accept the model's terms if prompted, which might require logging in to Hugging Face Hub (but not an access token for public models).\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(model_id)\n",
    "\n",
    "# If running on Colab GPU, move pipeline to CUDA for faster inference\n",
    "# pipeline.to(\"cuda\")\n",
    "\n",
    "# 3. Define the text prompt\n",
    "prompt = \"Flying cars soar over a futuristic cityscape at sunset.\"\n",
    "\n",
    "# 4. Generate the image\n",
    "# The pipeline generates an image by passing the text prompt.\n",
    "image = pipeline(prompt).images\n",
    "\n",
    "# 5. Save and display the image\n",
    "image_path = \"generated_image.png\"\n",
    "image.save(image_path)\n",
    "print(f\"Image saved as {image_path}\")\n",
    "\n",
    "# To display the image in Colab (requires PIL/Pillow):\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=image_path))\n",
    "\n",
    "# The image will be saved in the Google Colab file system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fff49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Here are detailed notes with code examples for various tasks using Hugging Face, drawing upon the provided YouTube source.\n",
    "\n",
    "***\n",
    "\n",
    "### 1. Introduction to Hugging Face and its Core Libraries\n",
    "\n",
    "Hugging Face is a company and open-source community focused on Natural Language Processing (NLP) and Artificial Intelligence (AI). It is best known for its **Transformers library**, which offers tools and pre-trained models for a wide range of NLP tasks. Beyond Transformers, Hugging Face also provides other widely used libraries, including **Datasets** and **Tokenizers**. It also features a **Model Hub** for sharing and downloading pre-trained models, datasets, and other resources, and **Spaces** for hosting and sharing machine learning demos and applications.\n",
    "\n",
    "**Key Libraries:**\n",
    "*   **Transformers Library**: The core library for pre-trained models and pipelines, providing access to thousands of pre-trained models for NLP tasks like translation, text summarization, and text classification. It is simple to use with complex NLP models, offers cutting-edge models, supports customization, and has a large, active community.\n",
    "*   **Datasets Library**: Provides easy access to a wide variety of datasets for NLP and other machine learning tasks. It enables efficient work with large datasets through lazy loading and streaming, offers a unified API for processing datasets, and integrates well with the Transformers library and other ML frameworks. Hugging Face provides over 350,000 datasets on its platform.\n",
    "*   **Tokenizers Library**: A fast, efficient, and flexible library designed for tokenizing text data, a crucial step in NLP. Tokenization involves splitting text into smaller units (words, subwords, characters) and converting them into numerical representations for ML models. It is optimised for fast tokenization, supports custom tokenizers, and integrates with other Hugging Face libraries like Transformers.\n",
    "\n",
    "### 2. Hugging Face Access Token\n",
    "\n",
    "An access token (also referred to as an API key) is a secure string of characters used to access Hugging Face services and resources.\n",
    "\n",
    "**When an Access Token is Needed:**\n",
    "*   When using a private or \"gated\" model (e.g., Meta's LLaMA) or an Inference API.\n",
    "*   When uploading models, datasets, or Spaces to the Hugging Face Hub.\n",
    "\n",
    "**When an Access Token is NOT Needed:**\n",
    "*   When accessing public models (e.g., GPT-2) which are freely available to download and use without authentication.\n",
    "*   When using models via the Transformers library, as many are publicly available and downloadable without an API key.\n",
    "\n",
    "To create an access token, you need to create an account on the official Hugging Face website (`huggingface.co/join`), then navigate to your profile, select \"Access Tokens,\" and create a new token. It is crucial **not to share** your access tokens with anyone.\n",
    "\n",
    "### 3. Installing Hugging Face Libraries on Google Colab\n",
    "\n",
    "Google Colab is a free web application that can be used to run Python notebooks. You can easily install Hugging Face libraries there.\n",
    "\n",
    "**General Installation Command:**\n",
    "The general command to install libraries using `pip` (a Python package manager) on Google Colab includes an exclamation mark (`!`) at the beginning.\n",
    "\n",
    "```python\n",
    "# General command to install a library on Google Colab\n",
    "!pip install <library_name>\n",
    "```\n",
    "\n",
    "**Specific Installation Commands:**\n",
    "*   **Transformers Library**: `!pip install transformers`\n",
    "*   **Datasets Library**: `!pip install datasets`\n",
    "*   **Tokenizers Library**: `!pip install tokenizers`\n",
    "*   **PyTorch (often required for models)**: `!pip install torch`\n",
    "*   **Diffusers Library (for text-to-image/video)**: `!pip install diffusers`\n",
    "\n",
    "You can change the runtime type on Google Colab (e.g., to T4 GPU or V2-8 TPU for complex projects) for better efficiency.\n",
    "\n",
    "### 4. Downloading a Dataset\n",
    "\n",
    "The `datasets` library allows easy access to a wide variety of datasets for machine learning.\n",
    "\n",
    "**Example: Downloading the IMDB Dataset**\n",
    "\n",
    "```python\n",
    "# 1. Install the datasets library (if not already installed)\n",
    "!pip install datasets\n",
    "\n",
    "# 2. Import the necessary function\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 3. Load the IMDB dataset\n",
    "# The load_dataset function downloads datasets from the Hugging Face Hub or loads from local files.\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# 4. Print the dataset to see its structure\n",
    "# This will display an overview, including the number of samples in each split (train and test).\n",
    "print(imdb_dataset)\n",
    "```\n",
    "\n",
    "**Output Explanation:**\n",
    "The output shows a `DatasetDict` structure, typically with 'train' and 'test' splits.\n",
    "*   **`train`**: Contains 25,000 rows with features like 'text' (movie reviews) and 'label' (sentiment: positive or negative).\n",
    "*   **`test`**: Contains 25,000 rows for testing, with the same features.\n",
    "*   The 'unsupervised' split (50,000 rows) is often used for pre-training or semi-supervised learning and typically lacks labels.\n",
    "\n",
    "### 5. Downloading a Model\n",
    "\n",
    "Models can be downloaded using the `transformers` library, specifically using the `from_pretrained` method.\n",
    "\n",
    "**Example: Downloading a pre-trained BERT Model**\n",
    "\n",
    "```python\n",
    "# 1. Install the transformers library (if not already installed)\n",
    "!pip install transformers\n",
    "\n",
    "# 2. Import necessary modules\n",
    "from transformers import AutoModel # Assuming AutoModel is used, similar to the source's implied usage for BERT.\n",
    "                                      # The source mentions from_pretrained for model weights, config, and tokenizer.\n",
    "                                      # For a full example, AutoTokenizer and AutoModel would typically be used together.\n",
    "\n",
    "# 3. Download a pre-trained BERT model\n",
    "# The from_pretrained method downloads the model weights, configuration, and tokenizer from the Hugging Face Hub.\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 4. (Optional) Pass an input and check output shape (as shown in source for BERT)\n",
    "# This requires a tokenizer first to convert text to input IDs.\n",
    "# from transformers import AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# inputs = tokenizer(\"hello hugging face\", return_tensors=\"pt\")\n",
    "# outputs = model(**inputs)\n",
    "# print(outputs.last_hidden_state.shape)\n",
    "\n",
    "# Expected output shape for BERT-base-uncased with \"hello hugging face\":\n",
    "# (1, 7, 768)\n",
    "# - 1: Batch size (one input sentence)\n",
    "# - 7: Sequence length (tokenized \"hello hugging face\" including special tokens)\n",
    "# - 768: Hidden size (each token is a 768-dimensional vector, standard for BERT's base architecture)\n",
    "```\n",
    "\n",
    "### 6. Sentiment Analysis\n",
    "\n",
    "Sentiment analysis determines the sentiment (positive, negative, or neutral) expressed in a piece of text.\n",
    "\n",
    "**Types of Sentiment Analysis:**\n",
    "*   **Polarity Detection**: Classifies sentiment as positive, negative, or neutral.\n",
    "    *   *Examples:* \"I love this product\" (positive), \"The service is terrible\" (negative), \"The package arrived on time\" (neutral).\n",
    "*   **Emotion Detection**: Identifies specific emotions like anger, joy, frustration, etc..\n",
    "    *   *Examples:* \"This is pathetic, so frustrating\" (anger), \"I'm thrilled about the results\" (joy).\n",
    "*   **Aspect-Based Sentiment Analysis**: Analyses sentiment towards specific aspects of a product or service.\n",
    "    *   *Example:* \"The food was great but the service was slow\" (positive for food, negative for service).\n",
    "*   **Intent Analysis**: Detects the user's intention, e.g., to purchase or complain.\n",
    "    *   *Example:* \"Where can I buy this product?\" (purchase intent).\n",
    "\n",
    "**Example: Performing Sentiment Analysis**\n",
    "\n",
    "```python\n",
    "# 1. Install required libraries\n",
    "!pip install transformers torch\n",
    "\n",
    "# 2. Import necessary modules and load the sentiment analysis pipeline\n",
    "# The pipeline function provides a simple way to perform various NLP tasks.\n",
    "from transformers import pipeline\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "# Note: For public models, an access token is not needed.\n",
    "\n",
    "# 3. Prepare input text\n",
    "texts_to_analyze = [\n",
    "    \"I love playing and watching cricket.\",\n",
    "    \"I hate when Virat Kohli misses a century.\"\n",
    "]\n",
    "\n",
    "# 4. Perform sentiment analysis\n",
    "results = sentiment_analyzer(texts_to_analyze)\n",
    "\n",
    "# 5. Display the output\n",
    "# The output is a list of dictionaries, with each dictionary containing the sentiment 'label' and 'score' (confidence).\n",
    "for text, result in zip(texts_to_analyze, results):\n",
    "    print(f\"Text: \\\"{text}\\\"\")\n",
    "    print(f\"  Label: {result['label']}, Score: {result['score']:.4f}\\n\")\n",
    "\n",
    "# Output Explanation:\n",
    "# The 'score' is a confidence level (probability) between 0 and 1. Closer to 1 means higher confidence.\n",
    "# The 'label' indicates the predicted sentiment (e.g., 'positive', 'negative').\n",
    "# High scores (close to 1) often indicate strong, unambiguous language in the input text.\n",
    "```\n",
    "\n",
    "### 7. Text Classification\n",
    "\n",
    "Text classification categorises text into predefined classes, such as spam detection, news article classification, or intent detection.\n",
    "\n",
    "**Difference from Sentiment Analysis:**\n",
    "*   **Sentiment Analysis**: Narrow and specific to sentiment (positive, negative, neutral).\n",
    "*   **Text Classification**: Broader, labels depend on the specific task (e.g., spam/not spam, different topics like sports/technology).\n",
    "\n",
    "**Example: Detecting Spam (Text Classification)**\n",
    "\n",
    "```python\n",
    "# 1. Install required libraries\n",
    "!pip install transformers torch\n",
    "\n",
    "# 2. Import necessary modules and load a pre-trained spam detection model\n",
    "from transformers import pipeline\n",
    "# Using a model fine-tuned for sentiment analysis but adapted for spam detection.\n",
    "spam_classifier = pipeline(\"sentiment-analysis\", model=\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "# No access token is needed for this publicly available model.\n",
    "\n",
    "# 3. Prepare input text (list of strings to classify)\n",
    "texts_to_classify = [\n",
    "    \"Congratulations! You have won a 500 INR Amazon gift card! Click here to claim.\",\n",
    "    \"Hi Amit, Let's have a meeting tomorrow at 12 p.m.\",\n",
    "    \"Your Gmail account has been compromised. Click here to verify immediately.\"\n",
    "]\n",
    "\n",
    "# 4. Map labels (as the model is sentiment-based)\n",
    "# Negative sentiment can map to 'spam', neutral and positive to 'not spam'.\n",
    "label_mapping = {\n",
    "    \"NEGATIVE\": \"spam\",\n",
    "    \"NEUTRAL\": \"not spam\",\n",
    "    \"POSITIVE\": \"not spam\"\n",
    "}\n",
    "\n",
    "# 5. Perform spam detection and display results\n",
    "for text in texts_to_classify:\n",
    "    result = spam_classifier(text) # The pipeline returns a list, take the first element.\n",
    "    predicted_label = label_mapping.get(result['label'], \"unknown\") # Use .get() for safer access.\n",
    "    print(f\"Text: \\\"{text}\\\"\")\n",
    "    print(f\"  Predicted Label: {predicted_label}, Confidence Score: {result['score']:.4f}\\n\")\n",
    "\n",
    "# Output Explanation:\n",
    "# The output includes a 'label' (e.g., \"NEGATIVE\", \"POSITIVE\") and a 'score' (confidence).\n",
    "# Low confidence scores (e.g., < 0.7) indicate uncertainty in the model's prediction.\n",
    "# The model might be fine-tuned for general sentiment, so it might not be perfectly accurate for spam detection out-of-the-box.\n",
    "```\n",
    "\n",
    "### 8. Text Summarization\n",
    "\n",
    "Text summarization is used to condense long articles, documents, or research papers into shorter snippets or extract key points.\n",
    "\n",
    "**Example: Summarizing Text**\n",
    "\n",
    "```python\n",
    "# 1. Install required libraries\n",
    "!pip install transformers torch\n",
    "\n",
    "# 2. Import necessary modules and load model and tokenizer\n",
    "# AutoModelForSeq2SeqLM is used for sequence-to-sequence tasks like summarization.\n",
    "# AutoTokenizer for tokenizing text.\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load a pre-trained model for summarization (publicly available, no access token needed).\n",
    "model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Alternatively, use pipeline for simplicity:\n",
    "# summarizer = pipeline(\"summarization\", model=model_name)\n",
    "\n",
    "# 3. Set input text to summarize\n",
    "input_text = \"\"\"\n",
    "Hugging Face is a company and open-source community that focuses on Natural Language Processing and Artificial Intelligence.\n",
    "It is best known for its transformers library which provides tools and pre-trained models for a wide range of NLP tasks\n",
    "such as text classification, sentiment analysis, machine translation, and more. Hugging Face also includes a model hub\n",
    "that is a platform where users can share and download pre-trained models, datasets, and other resources.\n",
    "Additionally, it provides a library for a variety of datasets called the datasets library, and a platform for hosting\n",
    "and sharing machine learning demos and applications called Spaces. With Hugging Face, users can easily deploy and\n",
    "use models in production environments. The community is strong, with developers and AI enthusiasts contributing to the ecosystem.\n",
    "\"\"\"\n",
    "\n",
    "# 4. Tokenize the input text\n",
    "# return_tensors=\"pt\" ensures PyTorch tensors, max_length ensures truncation if needed.\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# 5. Generate the summary\n",
    "# Parameters like max_length, min_length, length_penalty, num_beams control summary length and quality.\n",
    "summary_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=150,  # Maximum number of tokens in the summary\n",
    "    min_length=30,   # Minimum number of tokens in the summary\n",
    "    length_penalty=2.0, # Encourages longer summaries (value > 1.0)\n",
    "    num_beams=4,     # Controls beam search width; higher values improve quality but slow down inference\n",
    "    early_stopping=True # Stop beam search when all beams have reached a certain stage.\n",
    ")\n",
    "\n",
    "# 6. Decode the generated tokens back to text and print the summary\n",
    "summary = tokenizer.decode(summary_ids, skip_special_tokens=True)\n",
    "print(f\"Original Text:\\n{input_text}\\n\")\n",
    "print(f\"Generated Summary:\\n{summary}\\n\")\n",
    "```\n",
    "\n",
    "### 9. Machine Translation (Text-to-Text Generation)\n",
    "\n",
    "Machine translation involves translating text from one language to another. This falls under **text-to-text generation**, which encompasses tasks where the model takes an input sequence and generates an output sequence, including summarization, paraphrasing, and question answering.\n",
    "\n",
    "**Difference from Text Generation:**\n",
    "*   **Text Generation**: Used for auto-regressive generation (one token at a time), e.g., dialogue systems.\n",
    "*   **Text-to-Text Generation**: Used for sequence-to-sequence tasks, taking an input sequence to generate an output sequence (e.g., translation, summarization).\n",
    "\n",
    "**Example: Translating English to Spanish**\n",
    "\n",
    "```python\n",
    "# 1. Install required libraries\n",
    "!pip install transformers torch\n",
    "\n",
    "# 2. Import necessary modules and load a pre-trained translation model\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "# Load the T5 model (a versatile text-to-text model).\n",
    "model_name = \"t5-small\" # A smaller version of T5 model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Alternatively, use pipeline for simplicity:\n",
    "# translator = pipeline(\"translation_en_to_es\", model=model_name)\n",
    "\n",
    "# 3. Prepare input text with a task-specific prefix\n",
    "# T5 models often require a task prefix like \"translate English to Spanish:\".\n",
    "input_text = \"translate English to Spanish: My name is Amit Diwan and I love cricket.\"\n",
    "\n",
    "# 4. Tokenize the input text\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# 5. Generate the translated text\n",
    "# max_length and num_beams can be customised.\n",
    "translated_ids = model.generate(input_ids, max_length=50, num_beams=4)\n",
    "\n",
    "# 6. Decode output tokens and print the translated text\n",
    "translated_text = tokenizer.decode(translated_ids, skip_special_tokens=True)\n",
    "print(f\"Original Text: {input_text}\\n\")\n",
    "print(f\"Translated Text: {translated_text}\\n\")\n",
    "```\n",
    "\n",
    "### 10. Question Answering\n",
    "\n",
    "Question answering involves finding the answer to a question within a given \"context\" (a paragraph or text).\n",
    "\n",
    "**Example: Performing Question Answering**\n",
    "\n",
    "```python\n",
    "# 1. Install required libraries\n",
    "!pip install transformers torch\n",
    "\n",
    "# 2. Import necessary modules and load a pre-trained QA model and tokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a publicly available QA model (no access token needed).\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "# 3. Prepare the context and question\n",
    "context = \"Amit Diwan is a software engineer based in Delhi. He works for a tech company.\"\n",
    "question = \"Where is Amit Diwan based?\"\n",
    "\n",
    "# 4. Get the model's prediction\n",
    "# The pipeline handles tokenization, model prediction, and answer extraction internally.\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "\n",
    "# 5. Display the answer\n",
    "print(f\"Context: {context}\\n\")\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"Confidence Score: {result['score']:.4f}\\n\")\n",
    "\n",
    "# Output Explanation:\n",
    "# The output directly provides the 'answer' extracted from the context and a 'score' indicating confidence.\n",
    "```\n",
    "\n",
    "### 11. Text to Image\n",
    "\n",
    "Text-to-image generation creates an image from a textual description. This often utilises diffusion models, which are a class of generative models. The Hugging Face **Diffusers library** is an open-source Python library focusing on diffusion models for generating images, audio, and other data types. **Stable Diffusion** is a popular latent diffusion model within the Diffusers library for high-quality image generation from text prompts.\n",
    "\n",
    "**Example: Generating an Image from Text**\n",
    "\n",
    "```python\n",
    "# 1. Install required libraries\n",
    "!pip install diffusers transformers torch accelerate\n",
    "# 'accelerate' is often needed for optimisations when running diffusers models.\n",
    "\n",
    "# 2. Import necessary modules and load the Stable Diffusion pipeline\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# Load a publicly available Stable Diffusion model (no access token needed).\n",
    "# Ensure you accept the model's terms if prompted, which might require logging in to Hugging Face Hub (but not an access token for public models).\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(model_id)\n",
    "\n",
    "# If running on Colab GPU, move pipeline to CUDA for faster inference\n",
    "# pipeline.to(\"cuda\")\n",
    "\n",
    "# 3. Define the text prompt\n",
    "prompt = \"Flying cars soar over a futuristic cityscape at sunset.\"\n",
    "\n",
    "# 4. Generate the image\n",
    "# The pipeline generates an image by passing the text prompt.\n",
    "image = pipeline(prompt).images\n",
    "\n",
    "# 5. Save and display the image\n",
    "image_path = \"generated_image.png\"\n",
    "image.save(image_path)\n",
    "print(f\"Image saved as {image_path}\")\n",
    "\n",
    "# To display the image in Colab (requires PIL/Pillow):\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=image_path))\n",
    "\n",
    "# The image will be saved in the Google Colab file system.\n",
    "```\n",
    "\n",
    "### 12. Text to Video\n",
    "\n",
    "Text-to-video synthesis involves generating a video from textual descriptions. This is a complex task that combines NLP models with generative models or diffusion models. Hugging Face provides models and tools for this, often leveraging the **Diffusers library**.\n",
    "\n",
    "**Example: Generating a Video from Text (Stitching Frames)**\n",
    "\n",
    "This example shows how to generate individual frames using a text-to-image model and then stitch them into a video using OpenCV.\n",
    "\n",
    "```python\n",
    "# 1. Install required libraries\n",
    "# OpenCV (cv2) and NumPy are needed for video stitching.\n",
    "!pip install diffusers transformers torch accelerate opencv-python numpy\n",
    "\n",
    "# 2. Import necessary modules and load a text-to-image model\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load a publicly available text-to-image model (Stable Diffusion).\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(model_id)\n",
    "\n",
    "# If running on Colab GPU, move pipeline to CUDA for faster inference\n",
    "# pipeline.to(\"cuda\")\n",
    "\n",
    "# 3. Define the text prompt\n",
    "prompt = \"A futuristic cityscape at night with flying cars.\"\n",
    "\n",
    "# 4. Generate individual frames based on the text description\n",
    "num_frames = 10\n",
    "frames = []\n",
    "for i in range(num_frames):\n",
    "    # Generating slightly different images for each frame based on the same prompt.\n",
    "    # In a real text-to-video model, the model itself would handle temporal consistency.\n",
    "    # Here, we're simulating by generating distinct images.\n",
    "    image = pipeline(prompt).images\n",
    "    frames.append(np.array(image)) # Convert PIL Image to NumPy array for OpenCV\n",
    "    image.save(f\"frame_{i}.png\") # Save individual frames\n",
    "    print(f\"Generated frame_{i}.png\")\n",
    "\n",
    "# 5. Stitch the frames into a video using OpenCV\n",
    "if frames:\n",
    "    height, width, layers = frames.shape\n",
    "    video_name = 'output_video.mp4'\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Codec for .mp4 files\n",
    "    video = cv2.VideoWriter(video_name, fourcc, 1, (width, height)) # 1 FPS for demonstration\n",
    "\n",
    "    for frame in frames:\n",
    "        video.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)) # Convert RGB to BGR for OpenCV\n",
    "\n",
    "    video.release() # Release the video writer object\n",
    "    print(f\"\\nVideo saved as {video_name}\")\n",
    "\n",
    "# You can download the generated frames (frame_0.png to frame_9.png) and output_video.mp4 from Colab's file browser.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebf7d8a",
   "metadata": {},
   "source": [
    "Here are detailed notes with code examples for various tasks using Hugging Face, drawing upon the provided YouTube source.\n",
    "\n",
    "***\n",
    "\n",
    "### 1. Introduction to Hugging Face and its Core Libraries\n",
    "\n",
    "Hugging Face is a company and open-source community focused on Natural Language Processing (NLP) and Artificial Intelligence (AI). It is best known for its **Transformers library**, which offers tools and pre-trained models for a wide range of NLP tasks. Beyond Transformers, Hugging Face also provides other widely used libraries, including **Datasets** and **Tokenizers**. It also features a **Model Hub** for sharing and downloading pre-trained models, datasets, and other resources, and **Spaces** for hosting and sharing machine learning demos and applications.\n",
    "\n",
    "**Key Libraries:**\n",
    "*   **Transformers Library**: The core library for pre-trained models and pipelines, providing access to thousands of pre-trained models for NLP tasks like translation, text summarization, and text classification. It is simple to use with complex NLP models, offers cutting-edge models, supports customization, and has a large, active community.\n",
    "*   **Datasets Library**: Provides easy access to a wide variety of datasets for NLP and other machine learning tasks. It enables efficient work with large datasets through lazy loading and streaming, offers a unified API for processing datasets, and integrates well with the Transformers library and other ML frameworks. Hugging Face provides over 350,000 datasets on its platform.\n",
    "*   **Tokenizers Library**: A fast, efficient, and flexible library designed for tokenizing text data, a crucial step in NLP. Tokenization involves splitting text into smaller units (words, subwords, characters) and converting them into numerical representations for ML models. It is optimised for fast tokenization, supports custom tokenizers, and integrates with other Hugging Face libraries like Transformers.\n",
    "\n",
    "### 2. Hugging Face Access Token\n",
    "\n",
    "An access token (also referred to as an API key) is a secure string of characters used to access Hugging Face services and resources.\n",
    "\n",
    "**When an Access Token is Needed:**\n",
    "*   When using a private or \"gated\" model (e.g., Meta's LLaMA) or an Inference API.\n",
    "*   When uploading models, datasets, or Spaces to the Hugging Face Hub.\n",
    "\n",
    "**When an Access Token is NOT Needed:**\n",
    "*   When accessing public models (e.g., GPT-2) which are freely available to download and use without authentication.\n",
    "*   When using models via the Transformers library, as many are publicly available and downloadable without an API key.\n",
    "\n",
    "To create an access token, you need to create an account on the official Hugging Face website (`huggingface.co/join`), then navigate to your profile, select \"Access Tokens,\" and create a new token. It is crucial **not to share** your access tokens with anyone.\n",
    "\n",
    "### 3. Installing Hugging Face Libraries on Google Colab\n",
    "\n",
    "Google Colab is a free web application that can be used to run Python notebooks. You can easily install Hugging Face libraries there.\n",
    "\n",
    "**General Installation Command:**\n",
    "The general command to install libraries using `pip` (a Python package manager) on Google Colab includes an exclamation mark (`!`) at the beginning.\n",
    "\n",
    "```python\n",
    "# General command to install a library on Google Colab\n",
    "!pip install <library_name>\n",
    "```\n",
    "\n",
    "**Specific Installation Commands:**\n",
    "*   **Transformers Library**: `!pip install transformers`\n",
    "*   **Datasets Library**: `!pip install datasets`\n",
    "*   **Tokenizers Library**: `!pip install tokenizers`\n",
    "*   **PyTorch (often required for models)**: `!pip install torch`\n",
    "*   **Diffusers Library (for text-to-image/video)**: `!pip install diffusers`\n",
    "\n",
    "You can change the runtime type on Google Colab (e.g., to T4 GPU or V2-8 TPU for complex projects) for better efficiency.\n",
    "\n",
    "### 4. Downloading a Dataset\n",
    "\n",
    "The `datasets` library allows easy access to a wide variety of datasets for machine learning.\n",
    "\n",
    "**Example: Downloading the IMDB Dataset**\n",
    "\n",
    "```python\n",
    "# 1. Install the datasets library (if not already installed)\n",
    "!pip install datasets\n",
    "\n",
    "# 2. Import the necessary function\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 3. Load the IMDB dataset\n",
    "# The load_dataset function downloads datasets from the Hugging Face Hub or loads from local files.\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# 4. Print the dataset to see its structure\n",
    "# This will display an overview, including the number of samples in each split (train and test).\n",
    "print(imdb_dataset)\n",
    "```\n",
    "\n",
    "**Output Explanation:**\n",
    "The output shows a `DatasetDict` structure, typically with 'train' and 'test' splits.\n",
    "*   **`train`**: Contains 25,000 rows with features like 'text' (movie reviews) and 'label' (sentiment: positive or negative).\n",
    "*   **`test`**: Contains 25,000 rows for testing, with the same features.\n",
    "*   The 'unsupervised' split (50,000 rows) is often used for pre-training or semi-supervised learning and typically lacks labels.\n",
    "\n",
    "### 5. Downloading a Model\n",
    "\n",
    "Models can be downloaded using the `transformers` library, specifically using the `from_pretrained` method.\n",
    "\n",
    "**Example: Downloading a pre-trained BERT Model**\n",
    "\n",
    "```python\n",
    "# 1. Install the transformers library (if not already installed)\n",
    "!pip install transformers\n",
    "\n",
    "# 2. Import necessary modules\n",
    "from transformers import AutoModel # Assuming AutoModel is used, similar to the source's implied usage for BERT.\n",
    "                                      # The source mentions from_pretrained for model weights, config, and tokenizer.\n",
    "                                      # For a full example, AutoTokenizer and AutoModel would typically be used together.\n",
    "\n",
    "# 3. Download a pre-trained BERT model\n",
    "# The from_pretrained method downloads the model weights, configuration, and tokenizer from the Hugging Face Hub.\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 4. (Optional) Pass an input and check output shape (as shown in source for BERT)\n",
    "# This requires a tokenizer first to convert text to input IDs.\n",
    "# from transformers import AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# inputs = tokenizer(\"hello hugging face\", return_tensors=\"pt\")\n",
    "# outputs = model(**inputs)\n",
    "# print(outputs.last_hidden_state.shape)\n",
    "\n",
    "# Expected output shape for BERT-base-uncased with \"hello hugging face\":\n",
    "# (1, 7, 768)\n",
    "# - 1: Batch size (one input sentence)\n",
    "# - 7: Sequence length (tokenized \"hello hugging face\" including special tokens)\n",
    "# - 768: Hidden size (each token is a 768-dimensional vector, standard for BERT's base architecture)\n",
    "```\n",
    "\n",
    "### 6. Sentiment Analysis\n",
    "\n",
    "Sentiment analysis determines the sentiment (positive, negative, or neutral) expressed in a piece of text.\n",
    "\n",
    "**Types of Sentiment Analysis:**\n",
    "*   **Polarity Detection**: Classifies sentiment as positive, negative, or neutral.\n",
    "    *   *Examples:* \"I love this product\" (positive), \"The service is terrible\" (negative), \"The package arrived on time\" (neutral).\n",
    "*   **Emotion Detection**: Identifies specific emotions like anger, joy, frustration, etc..\n",
    "    *   *Examples:* \"This is pathetic, so frustrating\" (anger), \"I'm thrilled about the results\" (joy).\n",
    "*   **Aspect-Based Sentiment Analysis**: Analyses sentiment towards specific aspects of a product or service.\n",
    "    *   *Example:* \"The food was great but the service was slow\" (positive for food, negative for service).\n",
    "*   **Intent Analysis**: Detects the user's intention, e.g., to purchase or complain.\n",
    "    *   *Example:* \"Where can I buy this product?\" (purchase intent).\n",
    "\n",
    "**Example: Performing Sentiment Analysis**\n",
    "\n",
    "```python\n",
    "# 1. Install required libraries\n",
    "!pip install transformers torch\n",
    "\n",
    "# 2. Import necessary modules and load the sentiment analysis pipeline\n",
    "# The pipeline function provides a simple way to perform various NLP tasks.\n",
    "from transformers import pipeline\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "# Note: For public models, an access token is not needed.\n",
    "\n",
    "# 3. Prepare input text\n",
    "texts_to_analyze = [\n",
    "    \"I love playing and watching cricket.\",\n",
    "    \"I hate when Virat Kohli misses a century.\"\n",
    "]\n",
    "\n",
    "# 4. Perform sentiment analysis\n",
    "results = sentiment_analyzer(texts_to_analyze)\n",
    "\n",
    "# 5. Display the output\n",
    "# The output is a list of dictionaries, with each dictionary containing the sentiment 'label' and 'score' (confidence).\n",
    "for text, result in zip(texts_to_analyze, results):\n",
    "    print(f\"Text: \\\"{text}\\\"\")\n",
    "    print(f\"  Label: {result['label']}, Score: {result['score']:.4f}\\n\")\n",
    "\n",
    "# Output Explanation:\n",
    "# The 'score' is a confidence level (probability) between 0 and 1. Closer to 1 means higher confidence.\n",
    "# The 'label' indicates the predicted sentiment (e.g., 'positive', 'negative').\n",
    "# High scores (close to 1) often indicate strong, unambiguous language in the input text.\n",
    "```\n",
    "\n",
    "### 7. Text Classification\n",
    "\n",
    "Text classification categorises text into predefined classes, such as spam detection, news article classification, or intent detection.\n",
    "\n",
    "**Difference from Sentiment Analysis:**\n",
    "*   **Sentiment Analysis**: Narrow and specific to sentiment (positive, negative, neutral).\n",
    "*   **Text Classification**: Broader, labels depend on the specific task (e.g., spam/not spam, different topics like sports/technology).\n",
    "\n",
    "**Example: Detecting Spam (Text Classification)**\n",
    "\n",
    "```python\n",
    "# 1. Install required libraries\n",
    "!pip install transformers torch\n",
    "\n",
    "# 2. Import necessary modules and load a pre-trained spam detection model\n",
    "from transformers import pipeline\n",
    "# Using a model fine-tuned for sentiment analysis but adapted for spam detection.\n",
    "spam_classifier = pipeline(\"sentiment-analysis\", model=\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "# No access token is needed for this publicly available model.\n",
    "\n",
    "# 3. Prepare input text (list of strings to classify)\n",
    "texts_to_classify = [\n",
    "    \"Congratulations! You have won a 500 INR Amazon gift card! Click here to claim.\",\n",
    "    \"Hi Amit, Let's have a meeting tomorrow at 12 p.m.\",\n",
    "    \"Your Gmail account has been compromised. Click here to verify immediately.\"\n",
    "]\n",
    "\n",
    "# 4. Map labels (as the model is sentiment-based)\n",
    "# Negative sentiment can map to 'spam', neutral and positive to 'not spam'.\n",
    "label_mapping = {\n",
    "    \"NEGATIVE\": \"spam\",\n",
    "    \"NEUTRAL\": \"not spam\",\n",
    "    \"POSITIVE\": \"not spam\"\n",
    "}\n",
    "\n",
    "# 5. Perform spam detection and display results\n",
    "for text in texts_to_classify:\n",
    "    result = spam_classifier(text) # The pipeline returns a list, take the first element.\n",
    "    predicted_label = label_mapping.get(result['label'], \"unknown\") # Use .get() for safer access.\n",
    "    print(f\"Text: \\\"{text}\\\"\")\n",
    "    print(f\"  Predicted Label: {predicted_label}, Confidence Score: {result['score']:.4f}\\n\")\n",
    "\n",
    "# Output Explanation:\n",
    "# The output includes a 'label' (e.g., \"NEGATIVE\", \"POSITIVE\") and a 'score' (confidence).\n",
    "# Low confidence scores (e.g., < 0.7) indicate uncertainty in the model's prediction.\n",
    "# The model might be fine-tuned for general sentiment, so it might not be perfectly accurate for spam detection out-of-the-box.\n",
    "```\n",
    "\n",
    "### 8. Text Summarization\n",
    "\n",
    "Text summarization is used to condense long articles, documents, or research papers into shorter snippets or extract key points.\n",
    "\n",
    "**Example: Summarizing Text**\n",
    "\n",
    "```python\n",
    "# 1. Install required libraries\n",
    "!pip install transformers torch\n",
    "\n",
    "# 2. Import necessary modules and load model and tokenizer\n",
    "# AutoModelForSeq2SeqLM is used for sequence-to-sequence tasks like summarization.\n",
    "# AutoTokenizer for tokenizing text.\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load a pre-trained model for summarization (publicly available, no access token needed).\n",
    "model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Alternatively, use pipeline for simplicity:\n",
    "# summarizer = pipeline(\"summarization\", model=model_name)\n",
    "\n",
    "# 3. Set input text to summarize\n",
    "input_text = \"\"\"\n",
    "Hugging Face is a company and open-source community that focuses on Natural Language Processing and Artificial Intelligence.\n",
    "It is best known for its transformers library which provides tools and pre-trained models for a wide range of NLP tasks\n",
    "such as text classification, sentiment analysis, machine translation, and more. Hugging Face also includes a model hub\n",
    "that is a platform where users can share and download pre-trained models, datasets, and other resources.\n",
    "Additionally, it provides a library for a variety of datasets called the datasets library, and a platform for hosting\n",
    "and sharing machine learning demos and applications called Spaces. With Hugging Face, users can easily deploy and\n",
    "use models in production environments. The community is strong, with developers and AI enthusiasts contributing to the ecosystem.\n",
    "\"\"\"\n",
    "\n",
    "# 4. Tokenize the input text\n",
    "# return_tensors=\"pt\" ensures PyTorch tensors, max_length ensures truncation if needed.\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# 5. Generate the summary\n",
    "# Parameters like max_length, min_length, length_penalty, num_beams control summary length and quality.\n",
    "summary_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=150,  # Maximum number of tokens in the summary\n",
    "    min_length=30,   # Minimum number of tokens in the summary\n",
    "    length_penalty=2.0, # Encourages longer summaries (value > 1.0)\n",
    "    num_beams=4,     # Controls beam search width; higher values improve quality but slow down inference\n",
    "    early_stopping=True # Stop beam search when all beams have reached a certain stage.\n",
    ")\n",
    "\n",
    "# 6. Decode the generated tokens back to text and print the summary\n",
    "summary = tokenizer.decode(summary_ids, skip_special_tokens=True)\n",
    "print(f\"Original Text:\\n{input_text}\\n\")\n",
    "print(f\"Generated Summary:\\n{summary}\\n\")\n",
    "```\n",
    "\n",
    "### 9. Machine Translation (Text-to-Text Generation)\n",
    "\n",
    "Machine translation involves translating text from one language to another. This falls under **text-to-text generation**, which encompasses tasks where the model takes an input sequence and generates an output sequence, including summarization, paraphrasing, and question answering.\n",
    "\n",
    "**Difference from Text Generation:**\n",
    "*   **Text Generation**: Used for auto-regressive generation (one token at a time), e.g., dialogue systems.\n",
    "*   **Text-to-Text Generation**: Used for sequence-to-sequence tasks, taking an input sequence to generate an output sequence (e.g., translation, summarization).\n",
    "\n",
    "**Example: Translating English to Spanish**\n",
    "\n",
    "```python\n",
    "# 1. Install required libraries\n",
    "!pip install transformers torch\n",
    "\n",
    "# 2. Import necessary modules and load a pre-trained translation model\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "# Load the T5 model (a versatile text-to-text model).\n",
    "model_name = \"t5-small\" # A smaller version of T5 model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Alternatively, use pipeline for simplicity:\n",
    "# translator = pipeline(\"translation_en_to_es\", model=model_name)\n",
    "\n",
    "# 3. Prepare input text with a task-specific prefix\n",
    "# T5 models often require a task prefix like \"translate English to Spanish:\".\n",
    "input_text = \"translate English to Spanish: My name is Amit Diwan and I love cricket.\"\n",
    "\n",
    "# 4. Tokenize the input text\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# 5. Generate the translated text\n",
    "# max_length and num_beams can be customised.\n",
    "translated_ids = model.generate(input_ids, max_length=50, num_beams=4)\n",
    "\n",
    "# 6. Decode output tokens and print the translated text\n",
    "translated_text = tokenizer.decode(translated_ids, skip_special_tokens=True)\n",
    "print(f\"Original Text: {input_text}\\n\")\n",
    "print(f\"Translated Text: {translated_text}\\n\")\n",
    "```\n",
    "\n",
    "### 10. Question Answering\n",
    "\n",
    "Question answering involves finding the answer to a question within a given \"context\" (a paragraph or text).\n",
    "\n",
    "**Example: Performing Question Answering**\n",
    "\n",
    "```python\n",
    "# 1. Install required libraries\n",
    "!pip install transformers torch\n",
    "\n",
    "# 2. Import necessary modules and load a pre-trained QA model and tokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a publicly available QA model (no access token needed).\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "# 3. Prepare the context and question\n",
    "context = \"Amit Diwan is a software engineer based in Delhi. He works for a tech company.\"\n",
    "question = \"Where is Amit Diwan based?\"\n",
    "\n",
    "# 4. Get the model's prediction\n",
    "# The pipeline handles tokenization, model prediction, and answer extraction internally.\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "\n",
    "# 5. Display the answer\n",
    "print(f\"Context: {context}\\n\")\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"Confidence Score: {result['score']:.4f}\\n\")\n",
    "\n",
    "# Output Explanation:\n",
    "# The output directly provides the 'answer' extracted from the context and a 'score' indicating confidence.\n",
    "```\n",
    "\n",
    "### 11. Text to Image\n",
    "\n",
    "Text-to-image generation creates an image from a textual description. This often utilises diffusion models, which are a class of generative models. The Hugging Face **Diffusers library** is an open-source Python library focusing on diffusion models for generating images, audio, and other data types. **Stable Diffusion** is a popular latent diffusion model within the Diffusers library for high-quality image generation from text prompts.\n",
    "\n",
    "**Example: Generating an Image from Text**\n",
    "\n",
    "```python\n",
    "# 1. Install required libraries\n",
    "!pip install diffusers transformers torch accelerate\n",
    "# 'accelerate' is often needed for optimisations when running diffusers models.\n",
    "\n",
    "# 2. Import necessary modules and load the Stable Diffusion pipeline\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# Load a publicly available Stable Diffusion model (no access token needed).\n",
    "# Ensure you accept the model's terms if prompted, which might require logging in to Hugging Face Hub (but not an access token for public models).\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(model_id)\n",
    "\n",
    "# If running on Colab GPU, move pipeline to CUDA for faster inference\n",
    "# pipeline.to(\"cuda\")\n",
    "\n",
    "# 3. Define the text prompt\n",
    "prompt = \"Flying cars soar over a futuristic cityscape at sunset.\"\n",
    "\n",
    "# 4. Generate the image\n",
    "# The pipeline generates an image by passing the text prompt.\n",
    "image = pipeline(prompt).images\n",
    "\n",
    "# 5. Save and display the image\n",
    "image_path = \"generated_image.png\"\n",
    "image.save(image_path)\n",
    "print(f\"Image saved as {image_path}\")\n",
    "\n",
    "# To display the image in Colab (requires PIL/Pillow):\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=image_path))\n",
    "\n",
    "# The image will be saved in the Google Colab file system.\n",
    "```\n",
    "\n",
    "### 12. Text to Video\n",
    "\n",
    "Text-to-video synthesis involves generating a video from textual descriptions. This is a complex task that combines NLP models with generative models or diffusion models. Hugging Face provides models and tools for this, often leveraging the **Diffusers library**.\n",
    "\n",
    "**Example: Generating a Video from Text (Stitching Frames)**\n",
    "\n",
    "This example shows how to generate individual frames using a text-to-image model and then stitch them into a video using OpenCV.\n",
    "\n",
    "```python\n",
    "# 1. Install required libraries\n",
    "# OpenCV (cv2) and NumPy are needed for video stitching.\n",
    "!pip install diffusers transformers torch accelerate opencv-python numpy\n",
    "\n",
    "# 2. Import necessary modules and load a text-to-image model\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load a publicly available text-to-image model (Stable Diffusion).\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(model_id)\n",
    "\n",
    "# If running on Colab GPU, move pipeline to CUDA for faster inference\n",
    "# pipeline.to(\"cuda\")\n",
    "\n",
    "# 3. Define the text prompt\n",
    "prompt = \"A futuristic cityscape at night with flying cars.\"\n",
    "\n",
    "# 4. Generate individual frames based on the text description\n",
    "num_frames = 10\n",
    "frames = []\n",
    "for i in range(num_frames):\n",
    "    # Generating slightly different images for each frame based on the same prompt.\n",
    "    # In a real text-to-video model, the model itself would handle temporal consistency.\n",
    "    # Here, we're simulating by generating distinct images.\n",
    "    image = pipeline(prompt).images\n",
    "    frames.append(np.array(image)) # Convert PIL Image to NumPy array for OpenCV\n",
    "    image.save(f\"frame_{i}.png\") # Save individual frames\n",
    "    print(f\"Generated frame_{i}.png\")\n",
    "\n",
    "# 5. Stitch the frames into a video using OpenCV\n",
    "if frames:\n",
    "    height, width, layers = frames.shape\n",
    "    video_name = 'output_video.mp4'\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Codec for .mp4 files\n",
    "    video = cv2.VideoWriter(video_name, fourcc, 1, (width, height)) # 1 FPS for demonstration\n",
    "\n",
    "    for frame in frames:\n",
    "        video.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)) # Convert RGB to BGR for OpenCV\n",
    "\n",
    "    video.release() # Release the video writer object\n",
    "    print(f\"\\nVideo saved as {video_name}\")\n",
    "\n",
    "# You can download the generated frames (frame_0.png to frame_9.png) and output_video.mp4 from Colab's file browser.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faabc706",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
