{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3784993",
   "metadata": {},
   "source": [
    " Naive Bayes\n",
    " What is Naive Bayes? \n",
    "\n",
    "Bayes Theorem \n",
    "\n",
    "Types Of Naive Bayes \n",
    "\n",
    "Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd09549",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/prashant111/naive-bayes-classifier-in-python \n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2021/09/naive-bayes-algorithm-a-complete-guide-for-data-science-enthusiasts/\n",
    "\n",
    "https://www.simplilearn.com/tutorials/machine-learning-tutorial/naive-bayes-classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd420894",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "**What is Naive Bayes?**\n",
    "\n",
    "Naive Bayes is a family of probabilistic algorithms based on Bayes' Theorem, used for classification tasks. It is called \"naive\" because it assumes that the features used for classification are independent of each other, given the class label. Despite this strong assumption, Naive Bayes can perform surprisingly well in practice, especially for text classification problems like spam detection and sentiment analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### Bayes Theorem\n",
    "\n",
    "Bayes' Theorem describes the probability of an event based on prior knowledge of conditions that might be related to the event. It can be mathematically expressed as:\n",
    "\n",
    "\\[ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} \\]\n",
    "\n",
    "Where:\n",
    "- \\( P(A|B) \\): Probability of event A occurring given that B is true (posterior probability).\n",
    "- \\( P(B|A) \\): Probability of event B occurring given that A is true (likelihood).\n",
    "- \\( P(A) \\): Probability of event A occurring (prior probability).\n",
    "- \\( P(B) \\): Probability of event B occurring (evidence).\n",
    "\n",
    "---\n",
    "\n",
    "### Types of Naive Bayes\n",
    "\n",
    "There are several variations of the Naive Bayes algorithm, depending on the type of data being processed:\n",
    "\n",
    "1. **Gaussian Naive Bayes**:\n",
    "   - Assumes that the features follow a Gaussian (normal) distribution.\n",
    "   - Suitable for continuous data.\n",
    "\n",
    "2. **Multinomial Naive Bayes**:\n",
    "   - Used for discrete count data.\n",
    "   - Commonly used in text classification (e.g., word counts).\n",
    "\n",
    "3. **Bernoulli Naive Bayes**:\n",
    "   - Similar to the multinomial variant but assumes binary features (presence/absence).\n",
    "   - Often used in text classification where features are binary (e.g., whether a word exists in a document).\n",
    "\n",
    "---\n",
    "\n",
    "### Naive Bayes Classifier\n",
    "\n",
    "The Naive Bayes Classifier uses the Bayes Theorem to predict the class label of a given instance. Here’s a basic overview of how it works:\n",
    "\n",
    "1. **Training**: \n",
    "   - Calculate prior probabilities for each class based on the training data.\n",
    "   - For each feature, calculate the likelihood of that feature given each class.\n",
    "\n",
    "2. **Prediction**:\n",
    "   - For a new instance, compute the posterior probability for each class using the calculated priors and likelihoods.\n",
    "   - Classify the instance to the class with the highest posterior probability.\n",
    "\n",
    "### Implementation Example\n",
    "\n",
    "Here’s a simple implementation of a Naive Bayes classifier using Python’s `scikit-learn` library, focusing on text classification with the Multinomial Naive Bayes variant:\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Naive Bayes is a powerful and straightforward algorithm for classification tasks, particularly suited for high-dimensional data like text. Understanding the underlying principles of Bayes' Theorem and the types of Naive Bayes classifiers allows for effective application in various domains.\n",
    "\n",
    "Naive Bayes algorithm falls under classification.\n",
    "\n",
    "##### Applications of Naive Bayes Algorithm\n",
    "- Real-time Prediction.\n",
    "- Multi-class Prediction.\n",
    "- Text classification/ Spam Filtering/ Sentiment Analysis.\n",
    "- Recommendation Systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b07ebd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.89      0.87       236\n",
      "           1       0.61      0.90      0.73       287\n",
      "           2       0.94      0.23      0.37       290\n",
      "           3       0.58      0.85      0.69       285\n",
      "           4       0.94      0.81      0.87       312\n",
      "           5       0.88      0.82      0.85       308\n",
      "           6       0.92      0.69      0.79       276\n",
      "           7       0.90      0.91      0.91       304\n",
      "           8       0.97      0.94      0.95       279\n",
      "           9       0.97      0.94      0.96       308\n",
      "          10       0.96      0.96      0.96       309\n",
      "          11       0.83      0.97      0.89       290\n",
      "          12       0.87      0.82      0.84       304\n",
      "          13       0.95      0.91      0.93       300\n",
      "          14       0.90      0.98      0.94       297\n",
      "          15       0.76      0.99      0.86       292\n",
      "          16       0.88      0.92      0.90       270\n",
      "          17       0.90      0.99      0.94       272\n",
      "          18       0.81      0.90      0.85       239\n",
      "          19       0.96      0.43      0.60       196\n",
      "\n",
      "    accuracy                           0.85      5654\n",
      "   macro avg       0.87      0.84      0.83      5654\n",
      "weighted avg       0.87      0.85      0.84      5654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_20newsgroups(subset='all')\n",
    "X = data.data  # Text data\n",
    "y = data.target  # Labels\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convert text to feature vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "X_test_counts = vectorizer.transform(X_test)\n",
    "\n",
    "# Create a Multinomial Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Train the model\n",
    "nb_classifier.fit(X_train_counts, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = nb_classifier.predict(X_test_counts)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed0d79d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f422b41",
   "metadata": {},
   "source": [
    "###### Here’s how to implement the three types of Naive Bayes classifiers—Gaussian, Multinomial, and Bernoulli—using Python’s `scikit-learn` library. We will use a sample dataset for demonstration purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71705a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 1. Gaussian Naive Bayes\n",
    "\n",
    "# Gaussian Naive Bayes is used for continuous data and assumes that the features follow a Gaussian distribution.\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a Gaussian Naive Bayes classifier\n",
    "gnb_classifier = GaussianNB()\n",
    "\n",
    "# Train the model\n",
    "gnb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gnb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Gaussian Naive Bayes:\")\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815d6b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 2. Multinomial Naive Bayes\n",
    "\n",
    "# Multinomial Naive Bayes is used for discrete count data, such as text classification.\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_20newsgroups(subset='all')\n",
    "X = data.data  # Text data\n",
    "y = data.target  # Labels\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convert text to feature vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "X_test_counts = vectorizer.transform(X_test)\n",
    "\n",
    "# Create a Multinomial Naive Bayes classifier\n",
    "mnb_classifier = MultinomialNB()\n",
    "\n",
    "# Train the model\n",
    "mnb_classifier.fit(X_train_counts, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = mnb_classifier.predict(X_test_counts)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nMultinomial Naive Bayes:\")\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc82b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### 3. Bernoulli Naive Bayes\n",
    "\n",
    "Bernoulli Naive Bayes is also used for binary/boolean features.\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Using the same dataset as Multinomial Naive Bayes but binary features\n",
    "# Convert counts to binary features (0 or 1)\n",
    "X_train_binary = (X_train_counts > 0).astype(int)\n",
    "X_test_binary = (X_test_counts > 0).astype(int)\n",
    "\n",
    "# Create a Bernoulli Naive Bayes classifier\n",
    "bnb_classifier = BernoulliNB()\n",
    "\n",
    "# Train the model\n",
    "bnb_classifier.fit(X_train_binary, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = bnb_classifier.predict(X_test_binary)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nBernoulli Naive Bayes:\")\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b916ffbf",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- **Gaussian Naive Bayes** is ideal for continuous data.\n",
    "- **Multinomial Naive Bayes** is suited for discrete count data (e.g., text classification).\n",
    "- **Bernoulli Naive Bayes** is suitable for binary/boolean features.\n",
    "\n",
    "You can run each of these code snippets separately to see how each classifier performs on their respective datasets. Adjust the datasets and parameters as needed to explore different scenarios!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba68782",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
