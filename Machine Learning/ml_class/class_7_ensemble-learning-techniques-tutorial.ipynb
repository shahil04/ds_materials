{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ensemble Techniques\n",
    " Boosting\n",
    "\n",
    "AdaBoost \n",
    "\n",
    "Gradient Boosting \n",
    "\n",
    "XGBoost \n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Techniques\n",
    "\n",
    "Ensemble techniques combine multiple models to produce a stronger overall model. They are particularly effective in improving predictive performance and reducing overfitting. The two primary categories of ensemble methods are **bagging** and **boosting**.\n",
    "\n",
    "### Boosting\n",
    "\n",
    "**Boosting** is an ensemble technique that sequentially builds models, with each new model focusing on the errors made by the previous ones. The idea is to combine weak learners (models that perform slightly better than random chance) to create a strong learner. Boosting adjusts the weights of incorrectly predicted instances, allowing subsequent models to pay more attention to these difficult cases.\n",
    "\n",
    "### AdaBoost\n",
    "\n",
    "**AdaBoost** (Adaptive Boosting) is one of the first boosting algorithms. Here’s how it works:\n",
    "\n",
    "1. **Initialization**: Assign equal weights to all training instances.\n",
    "2. **Iterative Training**: For a specified number of iterations:\n",
    "   - Train a weak learner (e.g., a shallow decision tree) on the weighted training data.\n",
    "   - Calculate the model's error rate.\n",
    "   - Update the weights of the instances based on the error rate: increase weights for misclassified instances and decrease for correctly classified ones.\n",
    "   - Compute the model’s contribution to the final prediction based on its accuracy.\n",
    "3. **Final Prediction**: Combine the predictions of all weak learners, weighted by their performance.\n",
    "\n",
    "### Gradient Boosting\n",
    "\n",
    "**Gradient Boosting** builds models sequentially, optimizing a loss function by combining weak learners. Each new model corrects the errors of the previous models by minimizing the loss function using gradient descent. Here’s a basic outline:\n",
    "\n",
    "1. **Initialization**: Start with a constant model (e.g., the mean of the target variable).\n",
    "2. **Iterative Training**:\n",
    "   - Calculate the residuals (errors) from the previous model.\n",
    "   - Train a new weak learner to predict these residuals.\n",
    "   - Update the model by adding the new learner, scaled by a learning rate.\n",
    "3. **Final Prediction**: Combine all models to make the final prediction.\n",
    "\n",
    "### XGBoost\n",
    "\n",
    "**XGBoost** (Extreme Gradient Boosting) is an optimized implementation of gradient boosting that includes regularization, parallel processing, and tree pruning. It is known for its speed and performance. Key features include:\n",
    "\n",
    "1. **Regularization**: Helps prevent overfitting by penalizing complex models.\n",
    "2. **Handling Missing Values**: Automatically learns how to handle missing data during training.\n",
    "3. **Parallel Processing**: Utilizes multiple cores to speed up the training process.\n",
    "4. **Tree Pruning**: Efficiently prunes trees using a depth-first approach, which leads to faster and more accurate models.\n",
    "\n",
    "### Comparison of Techniques\n",
    "\n",
    "- **AdaBoost** is effective when combined with weak learners and is sensitive to noisy data and outliers.\n",
    "- **Gradient Boosting** provides more flexibility and control over the model fitting process and can be tuned with various loss functions.\n",
    "- **XGBoost** often outperforms both AdaBoost and traditional gradient boosting due to its advanced features and optimizations.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Ensemble techniques, especially boosting methods like AdaBoost, Gradient Boosting, and XGBoost, have become essential in the toolkit of machine learning practitioners. They allow for the creation of powerful predictive models that can tackle complex datasets and improve accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. AdaBoost Example\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a Decision Tree as the base estimator\n",
    "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# Create AdaBoost classifier\n",
    "ada_classifier = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "ada_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = ada_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"AdaBoost Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "### 2. Gradient Boosting Example\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the Boston housing dataset\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a Gradient Boosting Regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Gradient Boosting Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R^2 Score: {r2:.2f}\")\n",
    "```\n",
    "\n",
    "### 3. XGBoost Example\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the Boston housing dataset\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create an XGBoost Regressor\n",
    "xgb_regressor = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "xgb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"XGBoost Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R^2 Score: {r2:.2f}\")\n",
    "```\n",
    "\n",
    "### Explanation of the Code\n",
    "\n",
    "1. **AdaBoost Example**:\n",
    "   - We load the Iris dataset and split it into training and testing sets.\n",
    "   - A `DecisionTreeClassifier` with a maximum depth of 1 is used as the base estimator.\n",
    "   - An `AdaBoostClassifier` is created with 50 estimators.\n",
    "   - After training, we evaluate accuracy and print the classification report.\n",
    "\n",
    "2. **Gradient Boosting Example**:\n",
    "   - The Boston housing dataset is used, and we split it similarly.\n",
    "   - A `GradientBoostingRegressor` is created with 100 estimators, a learning rate of 0.1, and a maximum depth of 3.\n",
    "   - Predictions are made, and we evaluate the model using MSE and R² score.\n",
    "\n",
    "3. **XGBoost Example**:\n",
    "   - We again use the Boston housing dataset.\n",
    "   - An `XGBRegressor` is created with similar parameters.\n",
    "   - After training, predictions are made, and we evaluate the model using MSE and R² score.\n",
    "\n",
    "Make sure you have the `xgboost` library installed. You can install it via pip:\n",
    "\n",
    "```bash\n",
    "pip install xgboost\n",
    "```\n",
    "\n",
    "Feel free to modify the parameters and datasets to see how they affect the model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3335785%2F219228e436b0e694f835f70194e45c8c%2Fmaxresdefault.jpg?generation=1589357922679234&alt=media)\n",
    "\n",
    "<font size='5' color='blue' align = 'center'>Table of Contents</font> \n",
    "<font size='3' color='White'>\n",
    "1. [Introduction](#1)\n",
    "    \n",
    "1. [**Ensemble Techniques**](#2)\n",
    "    \n",
    "    2.1 [**Max Voting / Voting Classifier**](#21)\n",
    "    \n",
    "    2.2 [**Averaging**](#22)\n",
    "    \n",
    "    2.3 [**Weighted Averaging**](#23)\n",
    "    \n",
    "    2.4 [**Stacking**](#24)\n",
    "    \n",
    "    2.5 [**Blending**](#25)\n",
    "    \n",
    "    2.6 [**Bagging**](#26)\n",
    "    \n",
    "    2.7 [**Boosting**](#27)\n",
    " \n",
    "1. [References](#3)  \n",
    "\n",
    "1. [Conclusion](#4)  \n",
    "\n",
    "    \n",
    "# 1. Introduction <a id=\"1\"></a> <br>\n",
    "    \n",
    "Suppose you wanted to purchase a car.Now by just visiting the first car company and based on the dealer's advise  will we straight away make a buy on a car? Answer is defenitely a big NO right?\n",
    "    \n",
    "![](https://thumbs.dreamstime.com/b/car-sale-4167169.jpg)\n",
    "    \n",
    "So what we do is first decide whether which car to buy ,whether it is a new or used car ,type of car,model and year of manufacture, look for list of dealers ,look for discounts/offers ,customer reviews,opinion from friends and family, performance ,fuel efficiency and obvious any car buyer will for the best price range etc.\n",
    "    \n",
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcSC0-mqf3xqr3MESGW-mGwaWQkkBjwJGbNFsQ&usqp=CAU)\n",
    "    \n",
    " In short, you wouldn’t directly reach a conclusion, but will instead make a decision considering all the above mentioned factors before we decide on the best choice.\n",
    "    \n",
    "**Ensemble models** in machine learning operate on a similar idea.\n",
    "![](https://i.pinimg.com/474x/d7/c7/9b/d7c79b0c7abc5a34e17710fe596f6834.jpg)    \n",
    "Ensemble Learning helps improve machine learning results by combining several models to improve predictive performance compared to a single model. \n",
    "![](https://i.imgur.com/L2Jaqm8.png)\n",
    "    \n",
    "# 2. Ensemble Techniques <a id=\"2\"></a> <br>\n",
    "\n",
    "## 2.1 Max Voting / Voting Classifier <a id=\"2.1\"></a> <br>\n",
    "\n",
    "The **max voting** method is generally used for classification problems. In this technique, multiple models are used to make predictions for each data point. The predictions by each model are considered as a ‘vote’. The predictions which we get from the majority of the models are used as the final prediction.\n",
    "\n",
    "A **Voting Classifier** is a machine learning model that trains on an ensemble of numerous models and predicts an output (class) based on their highest probability of chosen class as the output.\n",
    "It simply aggregates the findings of each classifier passed into Voting Classifier and predicts the output class based on the highest majority of voting. The idea is instead of creating separate dedicated models and finding the accuracy for each them, we create a single model which trains by these models and predicts output based on their combined majority of voting for each output class.\n",
    "\n",
    "Now let us use the IRIS dataset to demonstrate the Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# importing libraries \n",
    "from sklearn.ensemble import VotingClassifier ,BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score \n",
    "from numpy import mean,std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score,RepeatedStratifiedKFold,train_test_split\n",
    "from sklearn.linear_model import LogisticRegression,RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from matplotlib import pyplot\n",
    "from sklearn.datasets import load_wine,load_iris\n",
    "from matplotlib.pyplot import figure\n",
    "figure(num=2, figsize=(16, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import SelectKBest,f_regression\n",
    "from sklearn.linear_model import LinearRegression,BayesianRidge,ElasticNet,Lasso,SGDRegressor,Ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder,RobustScaler,StandardScaler\n",
    "from sklearn.pipeline import make_pipeline,Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA,KernelPCA\n",
    "from sklearn.ensemble import ExtraTreesRegressor,GradientBoostingRegressor,RandomForestRegressor,VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score,KFold,GridSearchCV,RandomizedSearchCV,StratifiedKFold,train_test_split\n",
    "from sklearn.base import BaseEstimator,clone,TransformerMixin,RegressorMixin\n",
    "from sklearn.svm import LinearSVR,SVR\n",
    "#import xgboost \n",
    "from xgboost import XGBRegressor\n",
    "#Import Pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy.stats import skew\n",
    "from scipy.stats.stats import pearsonr\n",
    "%matplotlib inline\n",
    "seed = 1075\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# loading iris dataset \n",
    "iris = load_iris() \n",
    "X = iris.data[:, :4] \n",
    "Y = iris.target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size = 0.20,random_state = 42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Ensemble of Models \n",
    "estimator = [] \n",
    "estimator.append(('LR',LogisticRegression(solver ='lbfgs',multi_class ='multinomial',max_iter = 200))) \n",
    "estimator.append(('SVC', SVC(gamma ='auto', probability = True))) \n",
    "estimator.append(('DTC', DecisionTreeClassifier())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Voting Classifier** supports two types of votings.\n",
    "\n",
    "**Hard Voting:** In hard voting, the predicted output class is a class with the highest majority of votes i.e the class which had the highest probability of being predicted by each of the classifiers. Suppose three classifiers predicted the output class(A, A, B), so here the majority predicted A as output. Hence A will be the final prediction.\n",
    "![](https://image.slidesharecdn.com/7-180514114334/95/ensemble-learning-and-random-forests-12-638.jpg?cb=1527755412)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Voting Classifier with hard voting \n",
    "hard_voting = VotingClassifier(estimators = estimator, voting ='hard') \n",
    "hard_voting.fit(X_train, y_train) \n",
    "y_pred = hard_voting.predict(X_test)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Voting Score  1\n"
     ]
    }
   ],
   "source": [
    "# accuracy_score metric to predict Accuracy \n",
    "score = accuracy_score(y_test, y_pred) \n",
    "print(\"Hard Voting Score % d\" % score) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Soft Voting:** In soft voting, the output class is the prediction based on the average of probability given to that class. Suppose given some input to three models, the prediction probability for class A = (0.30, 0.47, 0.53) and B = (0.20, 0.32, 0.40). So the average for class A is 0.4333 and B is 0.3067, the winner is clearly class A because it had the highest probability averaged by each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Voting Classifier with soft voting \n",
    "soft_voting = VotingClassifier(estimators = estimator, voting ='soft') \n",
    "soft_voting.fit(X_train, y_train) \n",
    "y_pred = soft_voting.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft Voting Score  1\n"
     ]
    }
   ],
   "source": [
    "# Using accuracy_score \n",
    "score = accuracy_score(y_test, y_pred) \n",
    "print(\"Soft Voting Score % d\" % score) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practical the output accuracy will be more for soft voting as it is the average probability of the all estimators combined, as for our basic iris dataset we are already overfitting, so there won’t be much difference in output.\n",
    "\n",
    "## 2.2 Averaging <a id=\"2.2\"></a> <br>\n",
    "\n",
    "Multiple predictions are made for each data point in averaging. In this method, we take an average of predictions from all the models and use it to make the final prediction. Averaging can be used for making predictions in regression problems or while calculating probabilities for classification problems.\n",
    "\n",
    "The simplest way to develop a model averaging ensemble in Keras is to train multiple models on the same dataset then combine the predictions from each of the trained models.\n",
    "\n",
    "We will use a small multi-class classification problem as the basis to demonstrate a model averaging ensemble.\n",
    "\n",
    "The scikit-learn class provides the make_blobs() function that can be used to create a multi-class classification problem with the prescribed number of samples, input variables, classes, and variance of samples within a class.\n",
    "\n",
    "We use this problem with 500 examples, with input variables to represent the x and y coordinates of the points and a standard deviation of 2.0 for points within each group. We will use the same random state to ensure that we always get the same 500 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=500, centers=3, n_features=2, cluster_std=2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f36222cb3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# scatter plot, dots colored by class value\n",
    "df = DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
    "colors = {0:'red', 1:'blue', 2:'green'}\n",
    "fig, ax = pyplot.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the standard deviation of 2.0 means that the classes are not linearly separable (separable by a line) causing many ambiguous points.\n",
    "\n",
    "**So in summary for every instance of test dataset, the average predictions are calculated. This method often reduces overfit and creates a smoother regression model.**\n",
    "\n",
    "## 2.3 Weighted Averaging <a id=\"2.3\"></a> <br>\n",
    "This is an extension of the averaging method. All models are assigned different weights defining the importance of each model for prediction. \n",
    "![](https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Screen-Shot-2015-08-22-at-6.40.37-pm.png)\n",
    "\n",
    "For this we will use the housing prices dataset to demonstrate as shown below\n",
    "\n",
    "Firstly import the libraries & data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv',na_values = '#NAME?')\n",
    "test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv',na_values = '#NAME?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Based on the distribution of data let us remove some of the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train.drop(train[(train['GrLivArea'] >4000) & (train['SalePrice']<300000)].index,inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us concatenate both the training and test datasets into a single dataframe for ease of data cleaning and feature engineering.'Id' feature has no significance to our modelling since it is a continuous variable ,so dropping this feature on both train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2917, 80)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full = pd.concat([train,test],ignore_index=True)\n",
    "full.drop('Id',axis = 1,inplace = True)\n",
    "full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us preprocess the data by doing some missing values treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PoolQC          2908\n",
       "MiscFeature     2812\n",
       "Alley           2719\n",
       "Fence           2346\n",
       "SalePrice       1459\n",
       "FireplaceQu     1420\n",
       "LotFrontage      486\n",
       "GarageQual       159\n",
       "GarageCond       159\n",
       "GarageFinish     159\n",
       "GarageYrBlt      159\n",
       "GarageType       157\n",
       "BsmtExposure      82\n",
       "BsmtCond          82\n",
       "BsmtQual          81\n",
       "BsmtFinType2      80\n",
       "BsmtFinType1      79\n",
       "MasVnrType        24\n",
       "MasVnrArea        23\n",
       "MSZoning           4\n",
       "BsmtFullBath       2\n",
       "BsmtHalfBath       2\n",
       "Utilities          2\n",
       "Functional         2\n",
       "Electrical         1\n",
       "BsmtUnfSF          1\n",
       "Exterior1st        1\n",
       "Exterior2nd        1\n",
       "TotalBsmtSF        1\n",
       "GarageCars         1\n",
       "BsmtFinSF2         1\n",
       "BsmtFinSF1         1\n",
       "KitchenQual        1\n",
       "SaleType           1\n",
       "GarageArea         1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values = full.isnull().sum()\n",
    "missing_values[missing_values>0].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us impute the missing values of LotFrontage based on the median of LotArea and Neighborhood. To achieve this let us first group Neighborhood and LotFrontage with respect to median,mean and count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">LotFrontage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neighborhood</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Blmngtn</th>\n",
       "      <td>46.900000</td>\n",
       "      <td>43.0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blueste</th>\n",
       "      <td>27.300000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BrDale</th>\n",
       "      <td>21.500000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BrkSide</th>\n",
       "      <td>55.789474</td>\n",
       "      <td>51.0</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ClearCr</th>\n",
       "      <td>88.150000</td>\n",
       "      <td>80.5</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CollgCr</th>\n",
       "      <td>71.336364</td>\n",
       "      <td>70.0</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Crawfor</th>\n",
       "      <td>69.951807</td>\n",
       "      <td>70.0</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edwards</th>\n",
       "      <td>65.153409</td>\n",
       "      <td>64.5</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gilbert</th>\n",
       "      <td>74.207207</td>\n",
       "      <td>64.0</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IDOTRR</th>\n",
       "      <td>62.241379</td>\n",
       "      <td>60.0</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeadowV</th>\n",
       "      <td>25.606061</td>\n",
       "      <td>21.0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mitchel</th>\n",
       "      <td>75.144444</td>\n",
       "      <td>74.0</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NAmes</th>\n",
       "      <td>75.210667</td>\n",
       "      <td>73.0</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPkVill</th>\n",
       "      <td>28.142857</td>\n",
       "      <td>24.0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NWAmes</th>\n",
       "      <td>81.517647</td>\n",
       "      <td>80.0</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NoRidge</th>\n",
       "      <td>91.629630</td>\n",
       "      <td>89.0</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NridgHt</th>\n",
       "      <td>84.184049</td>\n",
       "      <td>92.0</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OldTown</th>\n",
       "      <td>61.777293</td>\n",
       "      <td>60.0</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWISU</th>\n",
       "      <td>59.068182</td>\n",
       "      <td>60.0</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sawyer</th>\n",
       "      <td>74.551020</td>\n",
       "      <td>72.0</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SawyerW</th>\n",
       "      <td>70.669811</td>\n",
       "      <td>67.0</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Somerst</th>\n",
       "      <td>64.549383</td>\n",
       "      <td>72.5</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StoneBr</th>\n",
       "      <td>62.173913</td>\n",
       "      <td>60.0</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timber</th>\n",
       "      <td>81.157895</td>\n",
       "      <td>82.0</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Veenker</th>\n",
       "      <td>72.000000</td>\n",
       "      <td>80.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             LotFrontage             \n",
       "                    mean median count\n",
       "Neighborhood                         \n",
       "Blmngtn        46.900000   43.0    20\n",
       "Blueste        27.300000   24.0    10\n",
       "BrDale         21.500000   21.0    30\n",
       "BrkSide        55.789474   51.0    95\n",
       "ClearCr        88.150000   80.5    20\n",
       "CollgCr        71.336364   70.0   220\n",
       "Crawfor        69.951807   70.0    83\n",
       "Edwards        65.153409   64.5   176\n",
       "Gilbert        74.207207   64.0   111\n",
       "IDOTRR         62.241379   60.0    87\n",
       "MeadowV        25.606061   21.0    33\n",
       "Mitchel        75.144444   74.0    90\n",
       "NAmes          75.210667   73.0   375\n",
       "NPkVill        28.142857   24.0    21\n",
       "NWAmes         81.517647   80.0    85\n",
       "NoRidge        91.629630   89.0    54\n",
       "NridgHt        84.184049   92.0   163\n",
       "OldTown        61.777293   60.0   229\n",
       "SWISU          59.068182   60.0    44\n",
       "Sawyer         74.551020   72.0    98\n",
       "SawyerW        70.669811   67.0   106\n",
       "Somerst        64.549383   72.5   162\n",
       "StoneBr        62.173913   60.0    46\n",
       "Timber         81.157895   82.0    57\n",
       "Veenker        72.000000   80.0    16"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.groupby(['Neighborhood'])[['LotFrontage']].agg(['mean','median','count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LotArea is a continuous feature so it is best to use panda's qcut method to divide it into 10 parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">LotFrontage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LotAreaCut</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(1299.999, 4921.8]</th>\n",
       "      <td>35.741036</td>\n",
       "      <td>34.0</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(4921.8, 7007.2]</th>\n",
       "      <td>55.460674</td>\n",
       "      <td>52.0</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(7007.2, 7949.0]</th>\n",
       "      <td>62.959839</td>\n",
       "      <td>62.0</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(7949.0, 8740.4]</th>\n",
       "      <td>67.113725</td>\n",
       "      <td>65.0</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(8740.4, 9452.0]</th>\n",
       "      <td>69.959184</td>\n",
       "      <td>70.0</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(9452.0, 10148.8]</th>\n",
       "      <td>73.988235</td>\n",
       "      <td>75.0</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(10148.8, 11000.0]</th>\n",
       "      <td>73.636364</td>\n",
       "      <td>75.0</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(11000.0, 12196.8]</th>\n",
       "      <td>83.371681</td>\n",
       "      <td>82.0</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(12196.8, 14285.8]</th>\n",
       "      <td>84.973684</td>\n",
       "      <td>85.0</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(14285.8, 215245.0]</th>\n",
       "      <td>92.846535</td>\n",
       "      <td>90.0</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    LotFrontage             \n",
       "                           mean median count\n",
       "LotAreaCut                                  \n",
       "(1299.999, 4921.8]    35.741036   34.0   251\n",
       "(4921.8, 7007.2]      55.460674   52.0   267\n",
       "(7007.2, 7949.0]      62.959839   62.0   249\n",
       "(7949.0, 8740.4]      67.113725   65.0   255\n",
       "(8740.4, 9452.0]      69.959184   70.0   245\n",
       "(9452.0, 10148.8]     73.988235   75.0   255\n",
       "(10148.8, 11000.0]    73.636364   75.0   253\n",
       "(11000.0, 12196.8]    83.371681   82.0   226\n",
       "(12196.8, 14285.8]    84.973684   85.0   228\n",
       "(14285.8, 215245.0]   92.846535   90.0   202"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full['LotAreaCut'] = pd.qcut(full.LotArea,10)\n",
    "\n",
    "full.groupby([full['LotAreaCut']])[['LotFrontage']].agg(['mean','median','count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let us impute the missing values of LotFrontage as stated above with the median of LotArea and Neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "full['LotFrontage']= full.groupby(['LotAreaCut','Neighborhood'])['LotFrontage'].transform(lambda x : x.fillna(x.median()))\n",
    "full['LotFrontage']= full.groupby(['LotAreaCut'])['LotFrontage'].transform(lambda x : x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us recheck the missing values to see our LotFrontage missing values are imputed successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PoolQC          2908\n",
       "MiscFeature     2812\n",
       "Alley           2719\n",
       "Fence           2346\n",
       "SalePrice       1459\n",
       "FireplaceQu     1420\n",
       "GarageQual       159\n",
       "GarageCond       159\n",
       "GarageFinish     159\n",
       "GarageYrBlt      159\n",
       "GarageType       157\n",
       "BsmtExposure      82\n",
       "BsmtCond          82\n",
       "BsmtQual          81\n",
       "BsmtFinType2      80\n",
       "BsmtFinType1      79\n",
       "MasVnrType        24\n",
       "MasVnrArea        23\n",
       "MSZoning           4\n",
       "BsmtFullBath       2\n",
       "BsmtHalfBath       2\n",
       "Utilities          2\n",
       "Functional         2\n",
       "Electrical         1\n",
       "BsmtUnfSF          1\n",
       "Exterior1st        1\n",
       "Exterior2nd        1\n",
       "TotalBsmtSF        1\n",
       "GarageArea         1\n",
       "GarageCars         1\n",
       "BsmtFinSF2         1\n",
       "BsmtFinSF1         1\n",
       "KitchenQual        1\n",
       "SaleType           1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values = full.isnull().sum()\n",
    "\n",
    "missing_values[missing_values>0].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us focus on numerical features with one missing value and replace them with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "columns = [\"MasVnrArea\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"GarageCars\", \"BsmtFinSF2\", \"BsmtFinSF1\", \"GarageArea\"]\n",
    "for col in columns:full[col].fillna(0,inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us focus on some of the categorical features with major count of missing values and replace them with 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "columns1 = [\"PoolQC\" , \"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\", \"GarageQual\", \"GarageCond\", \"GarageFinish\",\n",
    "\"GarageYrBlt\", \"GarageType\", \"BsmtExposure\", \"BsmtCond\", \"BsmtQual\", \"BsmtFinType2\", \"BsmtFinType1\", \"MasVnrType\"]\n",
    "for col1 in columns1:full[col1].fillna('None',inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us focus on some of the categorical features with fewer missing values and replace them with the most frequently occured value which is the mode of that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "columns2 = [\"MSZoning\", \"BsmtFullBath\", \"BsmtHalfBath\", \"Utilities\", \"Functional\",\n",
    "            \"Electrical\", \"KitchenQual\", \"SaleType\",\"Exterior1st\", \"Exterior2nd\"]\n",
    "\n",
    "for col2 in columns2:\n",
    "    full[col2].fillna(full[col2].mode()[0],inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us recheck if we have any other missing values that needs to be imputed except the SalePrice for the test dataset which is the target variable to be determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SalePrice    1459\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.isnull().sum()[full.isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['1stFlrSF', '2ndFlrSF', '3SsnPorch', 'BedroomAbvGr', 'BsmtFinSF1',\n",
       "       'BsmtFinSF2', 'BsmtFullBath', 'BsmtHalfBath', 'BsmtUnfSF',\n",
       "       'EnclosedPorch', 'Fireplaces', 'FullBath', 'GarageArea', 'GarageCars',\n",
       "       'GrLivArea', 'HalfBath', 'KitchenAbvGr', 'LotArea', 'LotFrontage',\n",
       "       'LowQualFinSF', 'MSSubClass', 'MasVnrArea', 'MiscVal', 'MoSold',\n",
       "       'OpenPorchSF', 'OverallCond', 'OverallQual', 'PoolArea', 'SalePrice',\n",
       "       'ScreenPorch', 'TotRmsAbvGrd', 'TotalBsmtSF', 'WoodDeckSF', 'YearBuilt',\n",
       "       'YearRemodAdd', 'YrSold'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_features = full.select_dtypes(include=[np.number])\n",
    "numeric_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Numstr = [\"MSSubClass\",\"BsmtFullBath\",\"BsmtHalfBath\",\"HalfBath\",\"BedroomAbvGr\",\"KitchenAbvGr\",\"MoSold\",\n",
    "          \"YrSold\",\"YearBuilt\",\"YearRemodAdd\",\"LowQualFinSF\",\"GarageYrBlt\"]\n",
    "\n",
    "for i in Numstr:\n",
    "    full[i]=full[i].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">SalePrice</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSSubClass</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>200779.080460</td>\n",
       "      <td>192000.0</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>138647.380952</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>102300.000000</td>\n",
       "      <td>88500.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>129613.333333</td>\n",
       "      <td>128250.0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>185224.811567</td>\n",
       "      <td>159250.0</td>\n",
       "      <td>536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>95829.724638</td>\n",
       "      <td>99900.0</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>156125.000000</td>\n",
       "      <td>142500.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>108591.666667</td>\n",
       "      <td>107500.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>143302.972222</td>\n",
       "      <td>132000.0</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>240403.542088</td>\n",
       "      <td>216000.0</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>166772.416667</td>\n",
       "      <td>156000.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>192437.500000</td>\n",
       "      <td>163500.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>169736.551724</td>\n",
       "      <td>166500.0</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>147810.000000</td>\n",
       "      <td>140750.0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>133541.076923</td>\n",
       "      <td>135980.0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                SalePrice                \n",
       "                     mean    median count\n",
       "MSSubClass                               \n",
       "120         200779.080460  192000.0    87\n",
       "150                   NaN       NaN     0\n",
       "160         138647.380952  146000.0    63\n",
       "180         102300.000000   88500.0    10\n",
       "190         129613.333333  128250.0    30\n",
       "20          185224.811567  159250.0   536\n",
       "30           95829.724638   99900.0    69\n",
       "40          156125.000000  142500.0     4\n",
       "45          108591.666667  107500.0    12\n",
       "50          143302.972222  132000.0   144\n",
       "60          240403.542088  216000.0   297\n",
       "70          166772.416667  156000.0    60\n",
       "75          192437.500000  163500.0    16\n",
       "80          169736.551724  166500.0    58\n",
       "85          147810.000000  140750.0    20\n",
       "90          133541.076923  135980.0    52"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.groupby(['MSSubClass'])[['SalePrice']].agg(['mean','median','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def map_values():\n",
    "    full[\"oMSSubClass\"] = full.MSSubClass.map({'180':1, \n",
    "                                        '30':2, '45':2, \n",
    "                                        '190':3, '50':3, '90':3, \n",
    "                                        '85':4, '40':4, '160':4, \n",
    "                                        '70':5, '20':5, '75':5, '80':5, '150':5,\n",
    "                                        '120': 6, '60':6})\n",
    "    \n",
    "    full[\"oMSZoning\"] = full.MSZoning.map({'C (all)':1, 'RH':2, 'RM':2, 'RL':3, 'FV':4})\n",
    "    full[\"oNeighborhood\"] = full.Neighborhood.map({'MeadowV':1,\n",
    "                                               'IDOTRR':2, 'BrDale':2,\n",
    "                                               'OldTown':3, 'Edwards':3, 'BrkSide':3,\n",
    "                                               'Sawyer':4, 'Blueste':4, 'SWISU':4, 'NAmes':4,\n",
    "                                               'NPkVill':5, 'Mitchel':5,\n",
    "                                               'SawyerW':6, 'Gilbert':6, 'NWAmes':6,\n",
    "                                               'Blmngtn':7, 'CollgCr':7, 'ClearCr':7, 'Crawfor':7,\n",
    "                                               'Veenker':8, 'Somerst':8, 'Timber':8,\n",
    "                                               'StoneBr':9,\n",
    "                                               'NoRidge':10, 'NridgHt':10})\n",
    "    \n",
    "    full[\"oCondition1\"] = full.Condition1.map({'Artery':1,\n",
    "                                           'Feedr':2, 'RRAe':2,\n",
    "                                           'Norm':3, 'RRAn':3,\n",
    "                                           'PosN':4, 'RRNe':4,\n",
    "                                           'PosA':5 ,'RRNn':5})\n",
    "    \n",
    "    full[\"oBldgType\"] = full.BldgType.map({'2fmCon':1, 'Duplex':1, 'Twnhs':1, '1Fam':2, 'TwnhsE':2})\n",
    "    \n",
    "    full[\"oHouseStyle\"] = full.HouseStyle.map({'1.5Unf':1, \n",
    "                                           '1.5Fin':2, '2.5Unf':2, 'SFoyer':2, \n",
    "                                           '1Story':3, 'SLvl':3,\n",
    "                                           '2Story':4, '2.5Fin':4})\n",
    "    \n",
    "    full[\"oExterior1st\"] = full.Exterior1st.map({'BrkComm':1,\n",
    "                                             'AsphShn':2, 'CBlock':2, 'AsbShng':2,\n",
    "                                             'WdShing':3, 'Wd Sdng':3, 'MetalSd':3, 'Stucco':3, 'HdBoard':3,\n",
    "                                             'BrkFace':4, 'Plywood':4,\n",
    "                                             'VinylSd':5,\n",
    "                                             'CemntBd':6,\n",
    "                                             'Stone':7, 'ImStucc':7})\n",
    "    \n",
    "    full[\"oMasVnrType\"] = full.MasVnrType.map({'BrkCmn':1, 'None':1, 'BrkFace':2, 'Stone':3})\n",
    "    \n",
    "    full[\"oExterQual\"] = full.ExterQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\n",
    "    \n",
    "    full[\"oFoundation\"] = full.Foundation.map({'Slab':1, \n",
    "                                           'BrkTil':2, 'CBlock':2, 'Stone':2,\n",
    "                                           'Wood':3, 'PConc':4})\n",
    "    \n",
    "    full[\"oBsmtQual\"] = full.BsmtQual.map({'Fa':2, 'None':1, 'TA':3, 'Gd':4, 'Ex':5})\n",
    "    \n",
    "    full[\"oBsmtExposure\"] = full.BsmtExposure.map({'None':1, 'No':2, 'Av':3, 'Mn':3, 'Gd':4})\n",
    "    \n",
    "    full[\"oHeating\"] = full.Heating.map({'Floor':1, 'Grav':1, 'Wall':2, 'OthW':3, 'GasW':4, 'GasA':5})\n",
    "    \n",
    "    full[\"oHeatingQC\"] = full.HeatingQC.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n",
    "    \n",
    "    full[\"oKitchenQual\"] = full.KitchenQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\n",
    "    \n",
    "    full[\"oFunctional\"] = full.Functional.map({'Maj2':1, 'Maj1':2, 'Min1':2, 'Min2':2, 'Mod':2, 'Sev':2, 'Typ':3})\n",
    "    \n",
    "    full[\"oFireplaceQu\"] = full.FireplaceQu.map({'None':1, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n",
    "    \n",
    "    full[\"oGarageType\"] = full.GarageType.map({'CarPort':1, 'None':1,\n",
    "                                           'Detchd':2,\n",
    "                                           '2Types':3, 'Basment':3,\n",
    "                                           'Attchd':4, 'BuiltIn':5})\n",
    "    \n",
    "    full[\"oGarageFinish\"] = full.GarageFinish.map({'None':1, 'Unf':2, 'RFn':3, 'Fin':4})\n",
    "    \n",
    "    full[\"oPavedDrive\"] = full.PavedDrive.map({'N':1, 'P':2, 'Y':3})\n",
    "    \n",
    "    full[\"oSaleType\"] = full.SaleType.map({'COD':1, 'ConLD':1, 'ConLI':1, 'ConLw':1, 'Oth':1, 'WD':1,\n",
    "                                       'CWD':2, 'Con':3, 'New':3})\n",
    "    \n",
    "    full[\"oSaleCondition\"] = full.SaleCondition.map({'AdjLand':1, 'Abnorml':2, 'Alloca':2, 'Family':2, 'Normal':3, 'Partial':4})            \n",
    "                \n",
    "                        \n",
    "                        \n",
    "    \n",
    "    return \"Done!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Done!'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# drop two unwanted columns\n",
    "full.drop(\"LotAreaCut\",axis=1,inplace=True)\n",
    "\n",
    "full.drop(['SalePrice'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>GarageYrBlt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>2001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>1998.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  YearBuilt YearRemodAdd GarageYrBlt\n",
       "0      2003         2003      2003.0\n",
       "1      1976         1976      1976.0\n",
       "2      2001         2002      2001.0\n",
       "3      1915         1970      1998.0\n",
       "4      2000         2000      2000.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full[['YearBuilt','YearRemodAdd','GarageYrBlt']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a class for the LabelEncoder to fit and transform some of the identified features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class labenc(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        label = LabelEncoder()\n",
    "        X['YearBuilt']=label.fit_transform(X['YearBuilt'])\n",
    "        X['YearRemodAdd']=label.fit_transform(X['YearRemodAdd'])\n",
    "        X['GarageYrBlt']=label.fit_transform(X['GarageYrBlt'])\n",
    "        return X\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class skewness(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,skew=0.5):\n",
    "        self.skew = skew\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        X_numeric=X.select_dtypes(exclude=[\"object\"])\n",
    "        skewness = X_numeric.apply(lambda x: skew(x))\n",
    "        skewness_features = skewness[abs(skewness) >= self.skew].index\n",
    "        X[skewness_features] = np.log1p(X[skewness_features])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class dummies(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        X = pd.get_dummies(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use pipeline to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. Pipeline serves two purposes here:\n",
    "\n",
    "Convenience: You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n",
    "Joint parameter selection: You can grid search over parameters of all estimators in the pipeline at once.\n",
    "All estimators in a pipeline, except the last one, must be transformers (i.e. must have a transform method). The last estimator may be any type (transformer, classifier, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('labenc',labenc()),('skewness',skewness(skew =1)),('dummies',dummies())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "full_copy = full.copy()\n",
    "data_pipeline = pipeline.fit_transform(full_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "robust_scaler = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1458"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train = train.shape[0]\n",
    "n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1458, 405), (1458,), (1459, 405))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X= data_pipeline[:n_train]\n",
    "y = train.SalePrice\n",
    "test_X = data_pipeline[n_train:]\n",
    "X.shape,y.shape,test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_scaled = robust_scaler.fit(X).transform(X)\n",
    "y_log = np.log(train.SalePrice)\n",
    "test_X_scaled = robust_scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1458, 405), (1458,), (1459, 405))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled.shape,y_log.shape,test_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will perform some feature selection like Lasso "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class add_feature(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,additional=1):\n",
    "        self.additional = additional\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        if self.additional==1:\n",
    "            X[\"TotalHouse\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]   \n",
    "            X[\"TotalArea\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"]\n",
    "            \n",
    "        else:\n",
    "            X[\"TotalHouse\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]   \n",
    "            X[\"TotalArea\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"]\n",
    "            \n",
    "            X[\"+_TotalHouse_OverallQual\"] = X[\"TotalHouse\"] * X[\"OverallQual\"]\n",
    "            X[\"+_GrLivArea_OverallQual\"] = X[\"GrLivArea\"] * X[\"OverallQual\"]\n",
    "            X[\"+_oMSZoning_TotalHouse\"] = X[\"oMSZoning\"] * X[\"TotalHouse\"]\n",
    "            X[\"+_oMSZoning_OverallQual\"] = X[\"oMSZoning\"] + X[\"OverallQual\"]\n",
    "            X[\"+_oMSZoning_YearBuilt\"] = X[\"oMSZoning\"] + X[\"YearBuilt\"]\n",
    "            X[\"+_oNeighborhood_TotalHouse\"] = X[\"oNeighborhood\"] * X[\"TotalHouse\"]\n",
    "            X[\"+_oNeighborhood_OverallQual\"] = X[\"oNeighborhood\"] + X[\"OverallQual\"]\n",
    "            X[\"+_oNeighborhood_YearBuilt\"] = X[\"oNeighborhood\"] + X[\"YearBuilt\"]\n",
    "            X[\"+_BsmtFinSF1_OverallQual\"] = X[\"BsmtFinSF1\"] * X[\"OverallQual\"]\n",
    "            \n",
    "            X[\"-_oFunctional_TotalHouse\"] = X[\"oFunctional\"] * X[\"TotalHouse\"]\n",
    "            X[\"-_oFunctional_OverallQual\"] = X[\"oFunctional\"] + X[\"OverallQual\"]\n",
    "            X[\"-_LotArea_OverallQual\"] = X[\"LotArea\"] * X[\"OverallQual\"]\n",
    "            X[\"-_TotalHouse_LotArea\"] = X[\"TotalHouse\"] + X[\"LotArea\"]\n",
    "            X[\"-_oCondition1_TotalHouse\"] = X[\"oCondition1\"] * X[\"TotalHouse\"]\n",
    "            X[\"-_oCondition1_OverallQual\"] = X[\"oCondition1\"] + X[\"OverallQual\"]\n",
    "            \n",
    "           \n",
    "            X[\"Bsmt\"] = X[\"BsmtFinSF1\"] + X[\"BsmtFinSF2\"] + X[\"BsmtUnfSF\"]\n",
    "            X[\"Rooms\"] = X[\"FullBath\"]+X[\"TotRmsAbvGrd\"]\n",
    "            X[\"PorchArea\"] = X[\"OpenPorchSF\"]+X[\"EnclosedPorch\"]+X[\"3SsnPorch\"]+X[\"ScreenPorch\"]\n",
    "            X[\"TotalPlace\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"] + X[\"OpenPorchSF\"]+X[\"EnclosedPorch\"]+X[\"3SsnPorch\"]+X[\"ScreenPorch\"]\n",
    "\n",
    "    \n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2917, 426)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([('labenc',labenc()),('add_feature', add_feature(additional=2)),\n",
    "                     ('skewness',skewness(skew =1)),('dummies',dummies())])\n",
    "\n",
    "full_pipe = pipeline.fit_transform(full)\n",
    "full_pipe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n_train=train.shape[0]\n",
    "X = full_pipe[:n_train]\n",
    "test_X = full_pipe[n_train:]\n",
    "y= train.SalePrice\n",
    "\n",
    "X_scaled = robust_scaler.fit(X).transform(X)\n",
    "y_log = np.log(train.SalePrice)\n",
    "test_X_scaled = robust_scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1458, 426)\n"
     ]
    }
   ],
   "source": [
    "print(X_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Now let us define Root Mean Square Error \n",
    "def rmse_cv(model,X,y):\n",
    "    rmse = np.sqrt(-cross_val_score(model,X,y,scoring=\"neg_mean_squared_error\",cv=5))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose 4 models and use 5-folds cross-calidation to evaluate these models.\n",
    "\n",
    "### Models include:\n",
    "\n",
    "   - LinearRegression\n",
    "   - Ridge\n",
    "   - Lasso\n",
    "   - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "models = [LinearRegression(),\n",
    "             Ridge(),\n",
    "             Lasso(alpha=0.01,max_iter=10000),\n",
    "             RandomForestRegressor(),\n",
    "             GradientBoostingRegressor(),\n",
    "             SVR(),\n",
    "             LinearSVR(),\n",
    "             ElasticNet(alpha = 0.001,max_iter=10000),\n",
    "             SGDRegressor(max_iter=1000, tol = 1e-3),\n",
    "             BayesianRidge(),\n",
    "             KernelRidge(alpha=0.6,kernel='polynomial',degree = 2,coef0=2.5),\n",
    "             ExtraTreesRegressor(),\n",
    "             XGBRegressor()\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "names = ['LR','Ridge','Lasso','RF','GBR','SVR','LSVR','ENet','SGDR','BayRidge','Kernel','XTreeR','XGBR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 1759961752.499758, 1626373557.062564\n",
      "Ridge: 0.117596, 0.009054\n",
      "Lasso: 0.120931, 0.005810\n",
      "RF: 0.135688, 0.006663\n",
      "GBR: 0.121049, 0.004380\n",
      "SVR: 0.112700, 0.004770\n",
      "LSVR: 0.125693, 0.007966\n",
      "ENet: 0.108729, 0.005421\n",
      "SGDR: 0.292973, 0.010462\n",
      "BayRidge: 0.110577, 0.005997\n",
      "Kernel: 0.109421, 0.005545\n",
      "XTreeR: 0.134061, 0.007801\n",
      "XGBR: 0.121919, 0.006182\n"
     ]
    }
   ],
   "source": [
    "for model,name in zip(models,names):\n",
    "    score = rmse_cv(model,X_scaled,y_log)\n",
    "    print(\"{}: {:.6f}, {:4f}\".format(name,score.mean(),score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# To define the average weight \n",
    "class AverageWeight(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self,model,weight):\n",
    "        self.model = model\n",
    "        self.weight = weight\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        self.models_ = [clone(x) for x in self.model]\n",
    "        for model in self.models_:\n",
    "            model.fit(X,y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        w = list()\n",
    "        pred = np.array([model.predict(X) for model in self.models_])\n",
    "        # for every data point, single model prediction times weight, then add them together\n",
    "        for data in range(pred.shape[1]):\n",
    "            single = [pred[model,data]*weight for model,weight in zip(range(pred.shape[0]),self.weight)]\n",
    "            w.append(np.sum(single))\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha= 0.0005, max_iter= 10000)\n",
    "ridge = Ridge(alpha=45, max_iter= 10000)\n",
    "svr = SVR(C = 0.2, epsilon= 0.025, gamma = 0.0004, kernel = 'rbf')\n",
    "ker = KernelRidge(alpha=0.15 ,kernel='polynomial',degree=3 , coef0=0.9)\n",
    "ela = ElasticNet(alpha=0.0065,l1_ratio=0.075,max_iter=10000)\n",
    "bay = BayesianRidge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally to calculate the average weights let us look at the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10924395500309354\n"
     ]
    }
   ],
   "source": [
    "# Assign weights to all the above 6 models\n",
    "w1 = 0.047\n",
    "w2 = 0.2\n",
    "w3 = 0.25\n",
    "w4 = 0.3\n",
    "w5 = 0.003\n",
    "w6 = 0.2\n",
    "\n",
    "weight_avg = AverageWeight(model = [lasso,ridge,svr,ker,ela,bay],weight=[w1,w2,w3,w4,w5,w6])\n",
    "score = rmse_cv(weight_avg,X_scaled,y_log)\n",
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider only two models then the score will vary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11166439558199057\n"
     ]
    }
   ],
   "source": [
    "weight_avg = AverageWeight(model = [svr,ker],weight=[0.50,0.50])\n",
    "score = rmse_cv(weight_avg,X_scaled,y_log)\n",
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in summary Weighted averaging is a slightly modified version of simple averaging, where the prediction of each model is multiplied by the weight and then their average is calculated. \n",
    "\n",
    "## 2.4 Stacking <a id=\"2.4\"></a> <br>\n",
    "![](https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/05/21160015/shutterstock_1159836664-696x464.jpg)\n",
    "Stacking is an ensemble learning technique that uses predictions from multiple models (for example decision tree, knn or svm) to build a new model. This model is used for making predictions on the test set.\n",
    "\n",
    "Stacking, also known as Stacked Generalization is an ensemble technique that combines multiple classifications or regression models via a meta-classifier or a meta-regressor. The base-level models are trained on a complete training set, then the meta-model is trained on the features that are outputs of the base-level model. The base-level often consists of different learning algorithms and therefore stacking ensembles are often heterogeneous. Here is a diagram illustrating the process\n",
    "\n",
    "Below is a step-wise explanation for a simple stacked ensemble:\n",
    "\n",
    "![](https://www.researchgate.net/publication/324552457/figure/fig3/AS:616245728645121@1523935839872/An-example-scheme-of-stacking-ensemble-learning.png)\n",
    "\n",
    "Step 1:The train set is split into 10 parts.\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/05/image-11-300x217.png)\n",
    "Step 2:A base model (suppose a decision tree) is fitted on 9 parts and predictions are made for the 10th part. This is done for each part of the train set.\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/05/image-10-300x249.png)\n",
    "Step 3:The base model (in this case, decision tree) is then fitted on the whole train dataset.\n",
    "\n",
    "Step 4:Using this model, predictions are made on the test set.\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/05/image-2-300x225.png)\n",
    "Step 5:Steps 2 to 4 are repeated for another base model (say knn) resulting in another set of predictions for the train set and test set.\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/05/image-3-300x224.png)\n",
    "Step 6:The predictions from the train set are used as features to build a new model.\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/05/image12-292x300.png)\n",
    "\n",
    "Step 7:This model is used to make final predictions on the test prediction set.\n",
    "\n",
    "\n",
    "In order to simplify the above explanation, the stacking model we have created has only two levels. The decision tree and knn models are built at level zero, while a logistic regression model is built at level one. Feel free to create multiple levels in a stacking model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class stacking(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self,mod,meta_model):\n",
    "        self.mod = mod\n",
    "        self.meta_model = meta_model\n",
    "        self.kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        self.saved_model = [list() for i in self.mod]\n",
    "        oof_train = np.zeros((X.shape[0], len(self.mod)))\n",
    "        \n",
    "        for i,model in enumerate(self.mod):\n",
    "            for train_index, val_index in self.kf.split(X,y):\n",
    "                renew_model = clone(model)\n",
    "                renew_model.fit(X[train_index], y[train_index])\n",
    "                self.saved_model[i].append(renew_model)\n",
    "                oof_train[val_index,i] = renew_model.predict(X[val_index])\n",
    "        \n",
    "        self.meta_model.fit(oof_train,y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        whole_test = np.column_stack([np.column_stack(model.predict(X) for model in single_model).mean(axis=1) \n",
    "                                      for single_model in self.saved_model]) \n",
    "        return self.meta_model.predict(whole_test)\n",
    "    \n",
    "    def get_oof(self,X,y,test_X):\n",
    "        oof = np.zeros((X.shape[0],len(self.mod)))\n",
    "        test_single = np.zeros((test_X.shape[0],5))\n",
    "        test_mean = np.zeros((test_X.shape[0],len(self.mod)))\n",
    "        for i,model in enumerate(self.mod):\n",
    "            for j, (train_index,val_index) in enumerate(self.kf.split(X,y)):\n",
    "                clone_model = clone(model)\n",
    "                clone_model.fit(X[train_index],y[train_index])\n",
    "                oof[val_index,i] = clone_model.predict(X[val_index])\n",
    "                test_single[:,j] = clone_model.predict(test_X)\n",
    "            test_mean[:,i] = test_single.mean(axis=1)\n",
    "        return oof, test_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_scaled_imputed = SimpleImputer().fit_transform(X_scaled)\n",
    "y_log_imputed = SimpleImputer().fit_transform(y_log.values.reshape(-1,1)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "stack_model = stacking(mod=[lasso,ridge,svr,ker,ela,bay],meta_model=ker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "score = rmse_cv(stack_model,X_scaled_imputed,y_log_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10770838136721914\n"
     ]
    }
   ],
   "source": [
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Blending <a id=\"2.5\"></a> <br>\n",
    "\n",
    "Blending follows the same approach as stacking but uses only a holdout (validation) set from the train set to make predictions. In other words, unlike stacking, the predictions are made on the holdout set only. The holdout set and the predictions are used to build a model which is run on the test set. Here is a detailed explanation of the blending process:\n",
    "\n",
    "Step 1: The train set is split into training and validation sets\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/05/image-7-300x226.png)\n",
    "\n",
    "Step 2: Model(s) are fitted on the training set.\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/05/image-5-300x228.png)\n",
    "\n",
    "Step 3: The predictions are made on the validation set and the test set.\n",
    "\n",
    "Step 4: The validation set and its predictions are used as features to build a new model.\n",
    "\n",
    "Step 5: This model is used to make final predictions on the test and meta-features.\n",
    "\n",
    "\n",
    "The difference between stacking and blending is that Stacking uses out-of-fold predictions for the train set of the next layer (i.e meta-model), and Blending uses a validation set (let’s say, 10-15% of the training set) to train the next layer.\n",
    "\n",
    "We’ll build two models, decision tree and knn, on the train set in order to make predictions on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "# define dataset\n",
    "X,y = load_wine().data,load_wine().target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x_val=pd.DataFrame(X_val)\n",
    "x_test=pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model1 = DecisionTreeClassifier()\n",
    "model1.fit(X_train, y_train)\n",
    "val_pred1=model1.predict(X_val)\n",
    "test_pred1=model1.predict(X_test)\n",
    "val_pred1=pd.DataFrame(val_pred1)\n",
    "test_pred1=pd.DataFrame(test_pred1)\n",
    "\n",
    "model2 = KNeighborsClassifier()\n",
    "model2.fit(X_train,y_train)\n",
    "val_pred2=model2.predict(X_val)\n",
    "test_pred2=model2.predict(X_test)\n",
    "val_pred2=pd.DataFrame(val_pred2)\n",
    "test_pred2=pd.DataFrame(test_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the meta-features and the validation set, a logistic regression model is built to make predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val=pd.concat([x_val, val_pred1,val_pred2],axis=1)\n",
    "df_test=pd.concat([x_test, test_pred1,test_pred2],axis=1)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(df_val,y_val)\n",
    "model.score(df_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Bagging <a id=\"2.6\"></a> <br>\n",
    "![](https://miro.medium.com/max/700/1*DFHUbdz6EyOuMYP4pDnFlw.jpeg)\n",
    "\n",
    "**Bagging**, is shorthand for the combination of bootstrapping and aggregating. Bootstrapping is a method to help decrease the variance of the classifier and reduce overfitting, by resampling data from the training set with the same cardinality as the original set. The model created should be less overfitted than a single individual model.\n",
    "\n",
    "A high variance for a model is not good, suggesting its performance is sensitive to the training data provided. So, even if more the training data is provided, the model may still perform poorly. And, may not even reduce the variance of our model.\n",
    "\n",
    "Bagging is an effective method when you have limited data, and by using samples you’re able to get an estimate by aggregating the scores over many samples.\n",
    "\n",
    "The simplest approach with bagging is to use a couple of small subsamples and bag them, if the ensemble accuracy is much higher than the base models, it’s working; if not, use larger subsamples.Using larger subsamples is not guaranteed to improve your results. In bagging there is a tradeoff between base model accuracy and the gain you get through bagging. The aggregation from bagging may improve the ensemble greatly when you have an unstable model, yet when your base models are more stable — been trained on larger subsamples with higher accuracy — improvements from bagging reduces.\n",
    "\n",
    "Once the bagging is done, and all the models have been created on (mostly) different data, a weighted average is then used to determine the final score.\n",
    "\n",
    "![](https://miro.medium.com/max/866/1*JksRZ1E72Rsx2s8lQbNR1w.jpeg)\n",
    "\n",
    "There are three main terms describing the ensemble (combination) of various models into one more effective model:\n",
    "\n",
    "* **Bagging** to decrease the model’s variance;\n",
    "* **Boosting** to decreasing the model’s bias, and;\n",
    "* **Stacking** to increasing the predictive force of the classifier.\n",
    "\n",
    "The idea behind bagging is combining the results of multiple models (for instance, all decision trees) to get a generalized result. Here’s a question: If you create all the models on the same set of data and combine it, will it be useful? There is a high chance that these models will give the same result since they are getting the same input. So how can we solve this problem? One of the techniques is bootstrapping.\n",
    "\n",
    "Bootstrapping is a sampling technique in which we create subsets of observations from the original dataset, with replacement. The size of the subsets is the same as the size of the original set.\n",
    "\n",
    "Bagging (or Bootstrap Aggregating) technique uses these subsets (bags) to get a fair idea of the distribution (complete set). The size of subsets created for bagging may be less than the original set.\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/05/image20-768x289.png)\n",
    "\n",
    "Step 1: Multiple subsets are created from the original dataset, selecting observations with replacement.\n",
    "\n",
    "Step 2: A base model (weak model) is created on each of these subsets.\n",
    "\n",
    "Step 3: The models run in parallel and are independent of each other.\n",
    "\n",
    "Step 4: The final predictions are determined by combining the predictions from all the models.\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/05/Screenshot-from-2018-05-08-13-11-49-768x580.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "# define dataset\n",
    "X,y = load_wine().data,load_wine().target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of: 0.967, std: (+/-) 0.037 [RandomForestClassifier]\n",
      "Mean of: 0.967, std: (+/-) 0.036 [Bagging RandomForestClassifier]\n",
      "\n",
      "Mean of: 0.978, std: (+/-) 0.037 [ExtraTreesClassifier]\n",
      "Mean of: 0.978, std: (+/-) 0.037 [Bagging ExtraTreesClassifier]\n",
      "\n",
      "Mean of: 0.676, std: (+/-) 0.084 [KNeighborsClassifier]\n",
      "Mean of: 0.759, std: (+/-) 0.059 [Bagging KNeighborsClassifier]\n",
      "\n",
      "Mean of: 0.439, std: (+/-) 0.050 [SVC]\n",
      "Mean of: 0.417, std: (+/-) 0.039 [Bagging SVC]\n",
      "\n",
      "Mean of: 0.984, std: (+/-) 0.025 [RidgeClassifier]\n",
      "Mean of: 0.978, std: (+/-) 0.037 [Bagging RidgeClassifier]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create classifiers\n",
    "rf = RandomForestClassifier()\n",
    "et = ExtraTreesClassifier()\n",
    "knn = KNeighborsClassifier()\n",
    "svc = SVC()\n",
    "rg = RidgeClassifier()\n",
    "clf_array = [rf, et, knn, svc, rg]\n",
    "for clf in clf_array:\n",
    "    vanilla_scores = cross_val_score(clf, X, y, cv=10, n_jobs=-1)\n",
    "    bagging_clf = BaggingClassifier(clf,max_samples=0.4, max_features=10, random_state=seed)\n",
    "    bagging_scores = cross_val_score(bagging_clf, X, y, cv=10,n_jobs=-1)\n",
    "    \n",
    "    print (\"Mean of: {1:.3f}, std: (+/-) {2:.3f} [{0}]\".format(clf.__class__.__name__,vanilla_scores.mean(), vanilla_scores.std()))\n",
    "    print (\"Mean of: {1:.3f}, std: (+/-) {2:.3f} [Bagging {0}]\\n\".format(clf.__class__.__name__,bagging_scores.mean(), bagging_scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all but one of the classifiers, we had lower variance as shown above . As well, the accuracy of classifiers all increased except for SVC. Looks like this bagging thing actually works. \n",
    "\n",
    "So our bagged individual classifiers are (mostly) better, but which one do we choose?\n",
    "\n",
    "**Let’s Vote!**\n",
    "\n",
    "Sklearn’s **VotingClassifier** allows you to combine different machine learning classifiers, and perform a vote on what the predicted class label(s) are for a record.\n",
    "\n",
    "There are two types of voting you can do for classifiers: hard and soft.\n",
    "\n",
    "With hard voting, you just need a majority of classifiers to determine what the result could be. As with the image below, the various bagged models are shown with H, and the results of the classifiers are shown on the rows. On the far right, H1 and H3 vote for the first record to be “no” (purple) while H2 votes for “yes” (yellow). Because 2 of the models vote for “no”, the ensemble classifies that record as a “no”.\n",
    "\n",
    "\n",
    "With soft (weighted), we compute a percentage weight with each classifier. A predicted class probability from each model for each record is collected and multiplied by the classifier weight, and finally averaged. The final class label is then derived from the class label with the highest average probability.\n",
    "\n",
    "In reality weights are hard to find if you’re just providing your best guesses to which model you think should be weighted more or less. To counter this subjective process, a linear optimization equation or neural net could be constructed to find the correct weighting for each of the models to optimize the accuracy of the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96 (+/- 0.06) [Random Forest]\n",
      "Accuracy: 0.96 (+/- 0.06) [Extra Trees]\n",
      "Accuracy: 0.68 (+/- 0.08) [KNeighbors]\n",
      "Accuracy: 0.44 (+/- 0.05) [SVC]\n",
      "Accuracy: 0.98 (+/- 0.03) [Ridge Classifier]\n",
      "Accuracy: 0.97 (+/- 0.03) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "clf = [rf, et, knn, svc, rg]\n",
    "eclf = VotingClassifier(estimators=[('Random Forests', rf), ('Extra Trees', et), ('KNeighbors', knn), ('SVC', svc), ('Ridge Classifier', rg)], voting='hard')\n",
    "for clf, label in zip([rf, et, knn, svc, rg, eclf], ['Random Forest', 'Extra Trees', 'KNeighbors', 'SVC', 'Ridge Classifier', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X, y, cv=10, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our bagged ensemble results shown above, we have an increase in accuracy and a decrease in variance, so our ensemble model is working as expected after we’ve combined all the various models into one.\n",
    "\n",
    "Now that we know how well our model(s) are doing individually and together, does that actually look. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Boosting <a id=\"2.7\"></a> <br>\n",
    "![](https://miro.medium.com/max/2936/1*jbncjeM4CfpobEnDO0ZTjw.png)\n",
    "\n",
    "The main idea of boosting is to add additional models to the overall ensemble model sequentially. Previously with bagging, we averaged each individual model created. This time with each iteration of boosting, a new model is created and the new base-learner model is trained (updated) from the errors of the previous learners.\n",
    "\n",
    "The algorithm creates multiple weak models whose output is added together to get an overall prediction. This is ensemble modelling from earlier. The now boosted gradient shifts the current prediction nudging it to the true target, in a similar fashion to how gradient descent moves toward the true values. The gradient descent optimization occurs on the output of the varies models, and not their individual parameters.\n",
    "\n",
    "There are different methods to optimize boosting algorithms.\n",
    "Unlike the bagging examples above, classical boosting the subset creation is not random and performance will depend upon the performance of previous models. As, each new subset which is iterated upon contains elements which could have been misclassified by previous models. We will also be using the same hard voting we used previously to ensemble the models together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "# define dataset\n",
    "X,y = load_wine().data,load_wine().target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adaptive boosting** or **AdaBoost** is one of the simplest boosting algorithms. Usually, decision trees are used for modelling. Multiple sequential models are created, each correcting the errors from the last model. AdaBoost assigns weights to the observations which are incorrectly predicted and the subsequent model works to predict these values correctly.\n",
    "\n",
    "Below are the steps for performing the AdaBoost algorithm:\n",
    "\n",
    "* Initially, all observations in the dataset are given equal weights.\n",
    "* A model is built on a subset of data.\n",
    "* Using this model, predictions are made on the whole dataset.\n",
    "* Errors are calculated by comparing the predictions and actual values.\n",
    "* While creating the next model, higher weights are given to the data points which were predicted incorrectly.\n",
    "* Weights can be determined using the error value. For instance, higher the error more is the weight assigned to the observation.\n",
    "* This process is repeated until the error function does not change, or the maximum limit of the number of estimators is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_boost = AdaBoostClassifier(random_state=1)\n",
    "ada_boost.fit(X_train, y_train)\n",
    "ada_boost.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters**\n",
    "\n",
    "**base_estimators**:\n",
    "\n",
    "* It helps to specify the type of base estimator, that is, the machine learning algorithm to be used as base learner.\n",
    "\n",
    "**n_estimators**:\n",
    "\n",
    "* It defines the number of base estimators.\n",
    "* The default value is 10, but you should keep a higher value to get better performance.\n",
    "\n",
    "**learning_rate**:\n",
    "\n",
    "* This parameter controls the contribution of the estimators in the final combination.\n",
    "* There is a trade-off between learning_rate and n_estimators.\n",
    "\n",
    "**max_depth**:\n",
    "\n",
    "* Defines the maximum depth of the individual estimator.\n",
    "* Tune this parameter for best performance.\n",
    "\n",
    "**n_jobs**\n",
    "\n",
    "* Specifies the number of processors it is allowed to use.\n",
    "* Set value to -1 for maximum processors allowed.\n",
    "\n",
    "**random_state** :\n",
    "\n",
    "* An integer value to specify the random data split.\n",
    "* A definite value of random_state will always produce same results if given with same parameters and training data.\n",
    "\n",
    "**Gradient Boosting or GBM ** \n",
    "\n",
    "It is another ensemble machine learning algorithm that works for both regression and classification problems. GBM uses the boosting technique, combining a number of weak learners to form a strong learner. Regression trees used as a base learner, each subsequent tree in series is built on the errors calculated by the previous tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9444444444444444"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_boost= GradientBoostingClassifier(learning_rate=0.01,random_state=1)\n",
    "grad_boost.fit(X_train, y_train)\n",
    "grad_boost.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters**\n",
    "\n",
    "**min_samples_split**\n",
    "\n",
    "* Defines the minimum number of samples (or observations) which are required in a node to be considered for splitting.\n",
    "* Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    " \n",
    "**min_samples_leaf**\n",
    "\n",
    "* Defines the minimum samples required in a terminal or leaf node.\n",
    "* Generally, lower values should be chosen for imbalanced class problems because the regions in which the minority class will be in the majority will be very small.\n",
    "\n",
    "**min_weight_fraction_leaf**\n",
    "\n",
    "* Similar to min_samples_leaf but defined as a fraction of the total number of observations instead of an integer.\n",
    "\n",
    "**max_depth**\n",
    "\n",
    "* The maximum depth of a tree.\n",
    "* Used to control over-fitting as higher depth will allow the model to learn relations very specific to a particular sample.\n",
    "* Should be tuned using CV.\n",
    " \n",
    "**max_leaf_nodes**\n",
    "\n",
    "* The maximum number of terminal nodes or leaves in a tree.\n",
    "* Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "* If this is defined, GBM will ignore max_depth.\n",
    "\n",
    "**max_features**\n",
    "\n",
    "* The number of features to consider while searching for the best split. These will be randomly selected.\n",
    "* As a thumb-rule, the square root of the total number of features works great but we should check up to 30-40% of the total number of features.\n",
    "* Higher values can lead to over-fitting but it generally depends on a case to case scenario.\n",
    " \n",
    "\n",
    "**XGBoost** (extreme Gradient Boosting) is an advanced implementation of the gradient boosting algorithm. XGBoost has proved to be a highly effective ML algorithm, extensively used in machine learning competitions and hackathons. XGBoost has high predictive power and is almost 10 times faster than the other gradient boosting techniques. It also includes a variety of regularization which reduces overfitting and improves overall performance. Hence it is also known as ‘**regularized boosting**‘ technique.\n",
    "\n",
    "Let us see how XGBoost is comparatively better than other techniques:\n",
    "\n",
    "**Regularization:**\n",
    "\n",
    "Standard GBM implementation has no regularisation like XGBoost.\n",
    "Thus XGBoost also helps to reduce overfitting.\n",
    "\n",
    "**Parallel Processing:**\n",
    "* XGBoost implements parallel processing and is faster than GBM .\n",
    "* XGBoost also supports implementation on Hadoop.\n",
    "\n",
    "**High Flexibility:**\n",
    "XGBoost allows users to define custom optimization objectives and evaluation criteria adding a whole new dimension to the model.\n",
    "\n",
    "**Handling Missing Values:**\n",
    "XGBoost has an in-built routine to handle missing values.\n",
    "\n",
    "**Tree Pruning:**\n",
    "XGBoost makes splits up to the max_depth specified and then starts pruning the tree backwards and removes splits beyond which there is no positive gain.\n",
    "\n",
    "**Built-in Cross-Validation:**\n",
    "XGBoost allows a user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9444444444444444"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_boost=xgb.XGBClassifier(random_state=1,learning_rate=0.01)\n",
    "xgb_boost.fit(X_train, y_train)\n",
    "xgb_boost.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters**\n",
    "\n",
    "**nthread**\n",
    "\n",
    "* This is used for parallel processing and the number of cores in the system should be entered..\n",
    "* If you wish to run on all cores, do not input this value. The algorithm will detect it automatically.\n",
    "\n",
    "**eta**\n",
    "\n",
    "Analogous to learning rate in GBM.\n",
    "Makes the model more robust by shrinking the weights on each step.\n",
    "\n",
    "**min_child_weight**\n",
    "\n",
    "* Defines the minimum sum of weights of all observations required in a child.\n",
    "* Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "\n",
    "**max_depth**\n",
    "\n",
    "* It is used to define the maximum depth.\n",
    "* Higher depth will allow the model to learn relations very specific to a particular sample.\n",
    "\n",
    "**max_leaf_nodes**\n",
    "\n",
    "* The maximum number of terminal nodes or leaves in a tree.\n",
    "* Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "* If this is defined, GBM will ignore max_depth.\n",
    "\n",
    "**gamma**\n",
    "\n",
    "* A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "* Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "\n",
    "**subsample**\n",
    "\n",
    "* Same as the subsample of GBM. Denotes the fraction of observations to be randomly sampled for each tree.\n",
    "* Lower values make the algorithm more conservative and prevent overfitting but values that are too small might lead to under-fitting.\n",
    "\n",
    "**colsample_bytree**\n",
    "\n",
    "* It is similar to max_features in GBM.\n",
    "* Denotes the fraction of columns to be randomly sampled for each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.87 (+/- 0.14) [Ada Boost]\n",
      "Accuracy: 0.91 (+/- 0.07) [Grad Boost]\n",
      "Accuracy: 0.93 (+/- 0.07) [XG Boost]\n",
      "Accuracy: 0.92 (+/- 0.09) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "eclf = VotingClassifier(estimators=[('Ada Boost', ada_boost), ('Grad Boost', grad_boost), ('XG Boost', xgb_boost)], voting='hard')\n",
    "clf = [rf, et, knn, svc, rg]\n",
    "for clf, label in zip([ada_boost, grad_boost, xgb_boost,eclf], ['Ada Boost','Grad Boost','XG Boost','Ensemble']):\n",
    "    scores = cross_val_score(clf, X, y, cv=10, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "62d0c612885169d9c6471c797406e6c3e0ae2bbd",
    "collapsed": true,
    "trusted": true
   },
   "source": [
    "# References<a id=\"3\"></a> <br>\n",
    "\n",
    "1. https://www.mygreatlearning.com/blog/ensemble-learning\n",
    "2. https://machinelearningmastery.com/model-averaging-ensemble-for-deep-learning-neural-networks/\n",
    "3. https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/\n",
    "4. https://towardsdatascience.com/ensemble-methods-in-machine-learning-what-are-they-and-why-use-them-68ec3f9fef5f\n",
    "5. https://medium.com/@rrfd/boosting-bagging-and-stacking-ensemble-methods-with-sklearn-and-mlens-a455c0c982de\n",
    "6. https://www.toptal.com/machine-learning/ensemble-methods-machine-learning\n",
    "\n",
    "# Conclusion<a id=\"4\"></a> <br>\n",
    "\n",
    "# I hope by now you had a fair understanding of what is Ensemble Learning Methods. \n",
    "\n",
    "# Please do leave your comments /suggestions and if you like this kernel greatly appreciate to<font color ='red'> UPVOTE ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
