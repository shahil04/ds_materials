{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to Classification - INSURANCE CLAIM ANALYSIS\n",
    "Introduction to Logistic Regression\n",
    "\n",
    "Sigmoid Function\n",
    "\n",
    "Confusion Matrix\n",
    "\n",
    "Classification Evaluation Metrics\n",
    "\n",
    "https://www.kaggle.com/code/mohamedbakrey/make-a-prediction-for-insurance-claim-report\n",
    "\n",
    "https://youtu.be/ntBa7YKc9XM?si=1V1RL0wCmxM_gjor  Regression price\n",
    "\n",
    "https://youtu.be/OOLhKLXCJiU?si=dWxvFk82mVO8WxEP DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.kaggle.com/code/pavansanagapati/ensemble-learning-techniques-tutorial\n",
    "Boosting\n",
    "Introduction to Boosting\n",
    "https://www.kaggle.com/code/prashant111/bagging-vs-boosting \n",
    "Adaptive Boosting\n",
    "https://www.kaggle.com/discussions/accomplishments/485459#2708350 \n",
    "Gradient Boosting\n",
    "\n",
    "XG Boosting\n",
    "\n",
    "Assignment\n",
    "\n",
    " Support Vector Machine\n",
    "https://www.kaggle.com/code/faressayah/support-vector-machine-pca-tutorial-for-beginner\n",
    "\n",
    "https://www.kaggle.com/code/egazakharenko/support-vector-machines-svm-from-scratch\n",
    "Introduction to SVM\n",
    "\n",
    "Support Vectors and Hyperplane\n",
    "\n",
    "Hard and Soft Margin\n",
    "\n",
    "Polynomial and Radial Basis Kernel Function\n",
    "\n",
    "SVM for multi-class Classification\n",
    "\n",
    "SVM for Regression\n",
    "https://www.kaggle.com/code/prashant111/svm-classifier-tutorial\n",
    "\n",
    "=====================\n",
    " Unsupervised Learning - CUSTOMER PARTTERN MATCHING\n",
    "https://www.youtube.com/watch?v=afPJeQuVeuY \n",
    "Understand Customers with KMeans Clustering in Python\n",
    "Introduction to Clustering\n",
    "https://www.datacamp.com/blog/introduction-to-unsupervised-learning\n",
    "K-Means Clustering\n",
    "\n",
    "Principal Component Analysis\n",
    "\n",
    "Assignment\n",
    "https://youtu.be/JtgSrB5J0_w?si=1mnGkPv50hfyBtLn\n",
    "================\n",
    "\n",
    " CAPSTONE PROJECT - CREDIT CARD FRAUD DETECTION\n",
    "https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud/discussion/277570\n",
    "https://data-flair.training/blogs/credit-card-fraud-detection-python-machine-learning/\n",
    "==========================\n",
    "Decision Tree and Random Forest - HEART DISEASE PREDICTION\n",
    "https://www.kaggle.com/code/nayeem091/a-guide-to-any-classification-problem\n",
    "Introduction to Decision Tree\n",
    "https://www.kaggle.com/code/prashant111/decision-tree-classifier-tutorial\n",
    "Gini Index and Entropy\n",
    "\n",
    "Advantage and Disadvantage\n",
    "\n",
    "Pruning Techniques\n",
    "\n",
    "Decision Tree for Regression\n",
    "\n",
    "Introduction to Random Forest\n",
    "https://www.kaggle.com/code/prashant111/random-forest-classifier-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "data.keys()\n",
    "print(data.DESCR)\n",
    "\n",
    "data.data\n",
    "\n",
    "data.target\n",
    "\n",
    "data.feature_names\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame(data.data, columns = data.feature_names)\n",
    "df['target'] = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "df.sample(2)\n",
    "\n",
    "df.target.unique()\n",
    "\n",
    "\n",
    "df = df[df.target != 2] #for implementing log regression for two class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "\n",
    "X_train.shape, y_train.shape\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "y_pred\n",
    "\n",
    "classifier.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "y_pred_proba = classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_pred_proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))  # Create a new figure with a specified size\n",
    "plt.plot(fpr, tpr, color='darkorange', linewidth=2, label='ROC curve (area = %0.2f)' % roc_auc)  # Plot ROC curve,\n",
    "plt.plot([0, 1], [0, 1], color='navy', linewidth=2, linestyle='--')  # Plot the diagonal line representing random guessing\n",
    "plt.xlim([0.0, 1.0])  # Set x-axis limits\n",
    "plt.ylim([0.0, 1.05])  # Set y-axis limits\n",
    "plt.xlabel('False Positive Rate')  # Set x-axis label\n",
    "plt.ylabel('True Positive Rate')  # Set y-axis label\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')  # Set plot title\n",
    "plt.legend(loc=\"lower right\")  # Add legend to the plot\n",
    "plt.show()  # Show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5)\n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(classifier, X_train, y_train, cv = cv, scoring = \"accuracy\")\n",
    "\n",
    "scores\n",
    "np.mean(scores)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
