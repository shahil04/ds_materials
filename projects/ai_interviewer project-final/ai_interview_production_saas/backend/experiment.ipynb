{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d5214dd",
   "metadata": {},
   "source": [
    "1. upload resume and extract text using ai and create\n",
    "    a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07691e4",
   "metadata": {},
   "source": [
    "## resume code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90529fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Streamlit Resume, LinkedIn & GitHub Analyzer with Gemini API Integration\n",
    "------------------------------------------------\n",
    "Features:\n",
    "- Upload resume (PDF, DOCX, TXT). Extract text and analyze.\n",
    "- Paste job description / role keywords to compare against resume.\n",
    "- Detect LinkedIn & GitHub URLs and give recommendations.\n",
    "- Send resume + job description to Gemini (Generative API) for scoring, detailed feedback, and an updated resume in DOCX format following a recruiter-friendly format.\n",
    "- Produce downloadable feedback report and updated resume (.docx).\n",
    "\n",
    "Notes / Requirements:\n",
    "- Uses: streamlit, requests, python-docx, PyPDF2, docx2txt\n",
    "- Install (if needed): pip install streamlit requests python-docx PyPDF2 docx2txt\n",
    "- You MUST provide a working Gemini/Generative API key and (optionally) endpoint details. This app uses a flexible HTTP call so you can adapt to your provider (Google Gemini / Vertex AI / other compatible endpoints).\n",
    "- The app sends a structured prompt to the model asking for a JSON response with: overall_score (0-100), suggestions (list), updated_resume (full text in resume format), highlights (skills matched).\n",
    "\n",
    "Security / Privacy:\n",
    "- Resume text is sent to the external API you configure. Do not use sensitive personal data if you do not want it transmitted.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import streamlit as st\n",
    "import re\n",
    "import io\n",
    "import base64\n",
    "from collections import Counter\n",
    "import requests\n",
    "import json\n",
    "from docx import Document\n",
    "\n",
    "try:\n",
    "    import PyPDF2\n",
    "except Exception:\n",
    "    PyPDF2 = None\n",
    "\n",
    "try:\n",
    "    import docx2txt\n",
    "except Exception:\n",
    "    docx2txt = None\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Configuration & Constants\n",
    "# --------------------------------------------------\n",
    "\n",
    "SKILLS_DB = [\n",
    "    'python','pandas','numpy','scikit-learn','sklearn','tensorflow','pytorch',\n",
    "    'sql','excel','power bi','tableau','matplotlib','seaborn','nlp','computer vision',\n",
    "    'regression','classification','clustering','random forest','xgboost','lightgbm',\n",
    "    'git','github','docker','aws','gcp','azure','bigquery','spark','hadoop'\n",
    "]\n",
    "\n",
    "SECTION_HEADERS = [\n",
    "    'experience','education','projects','skills','certifications','summary','objective','publications','contact'\n",
    "]\n",
    "\n",
    "URL_PATTERN = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "EMAIL_PATTERN = re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
    "PHONE_PATTERN = re.compile(r'(?:\\+?\\d{1,3}[\\s-]?)?(?:\\(?\\d{3}\\)?[\\s-]?|\\d{3}[\\s-]?)\\d{3}[\\s-]?\\d{4}')\n",
    "LINKEDIN_PATTERN = re.compile(r'(?:https?://)?(?:www\\.)?linkedin\\.com/\\S+', re.IGNORECASE)\n",
    "GITHUB_PATTERN = re.compile(r'(?:https?://)?(?:www\\.)?github\\.com/\\S+', re.IGNORECASE)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Helper functions\n",
    "# --------------------------------------------------\n",
    "\n",
    "def extract_text_from_pdf(file_stream):\n",
    "    if PyPDF2 is None:\n",
    "        return \"\"  # PyPDF2 not installed\n",
    "    try:\n",
    "        reader = PyPDF2.PdfReader(file_stream)\n",
    "        text = []\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text() or ''\n",
    "            text.append(page_text)\n",
    "        return '\\n'.join(text)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_text_from_docx(file_stream):\n",
    "    if docx2txt is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        tmp = file_stream.read()\n",
    "        with open('/tmp/temp_resume.docx', 'wb') as f:\n",
    "            f.write(tmp)\n",
    "        text = docx2txt.process('/tmp/temp_resume.docx')\n",
    "        return text\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_text_from_txt(file_stream):\n",
    "    try:\n",
    "        raw = file_stream.read()\n",
    "        if isinstance(raw, bytes):\n",
    "            raw = raw.decode('utf-8', errors='ignore')\n",
    "        return raw\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    t = text.lower()\n",
    "    t = re.sub(r'[^a-z0-9\\s]', ' ', t)\n",
    "    tokens = [w for w in t.split() if len(w) > 1]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def detect_sections(text):\n",
    "    text_lower = text.lower()\n",
    "    found = {}\n",
    "    for sec in SECTION_HEADERS:\n",
    "        found[sec] = sec in text_lower\n",
    "    return found\n",
    "\n",
    "\n",
    "def generate_local_feedback(resume_text, jd_text=None):\n",
    "    tokens = clean_and_tokenize(resume_text)\n",
    "    sections = detect_sections(resume_text)\n",
    "    skills_found = [s for s in SKILLS_DB if s in resume_text.lower()]\n",
    "\n",
    "    urls = URL_PATTERN.findall(resume_text)\n",
    "    emails = EMAIL_PATTERN.findall(resume_text)\n",
    "    phones = PHONE_PATTERN.findall(resume_text)\n",
    "    linkedin = LINKEDIN_PATTERN.findall(resume_text)\n",
    "    github = GITHUB_PATTERN.findall(resume_text)\n",
    "\n",
    "    length_words = len(tokens)\n",
    "    length_score = 1.0 if 200 <= length_words <= 800 else max(0.2, min(1.0, length_words / 800))\n",
    "    contact_score = 1.0 if (emails or phones) else 0.0\n",
    "    section_score = sum(1 for v in sections.values() if v) / len(sections)\n",
    "    skills_score = min(1.0, len(skills_found) / 8)\n",
    "\n",
    "    overall = round((0.3 * length_score + 0.2 * contact_score + 0.2 * section_score + 0.3 * skills_score) * 100)\n",
    "\n",
    "    feedback = {\n",
    "        'overall_score': overall,\n",
    "        'length_words': length_words,\n",
    "        'length_score': round(length_score, 2),\n",
    "        'contact_found': bool(emails or phones),\n",
    "        'emails': emails,\n",
    "        'phones': phones,\n",
    "        'sections': sections,\n",
    "        'section_score': round(section_score, 2),\n",
    "        'skills_found': skills_found,\n",
    "        'skills_score': round(skills_score, 2),\n",
    "        'linkedin': linkedin,\n",
    "        'github': github,\n",
    "        'urls': urls,\n",
    "    }\n",
    "\n",
    "    if jd_text:\n",
    "        resume_tokens = set(clean_and_tokenize(resume_text))\n",
    "        jd_tokens = set(clean_and_tokenize(jd_text))\n",
    "        common = resume_tokens.intersection(jd_tokens)\n",
    "        if len(jd_tokens) == 0:\n",
    "            jd_score = 0.0\n",
    "        else:\n",
    "            jd_score = round(min(len(common)/len(jd_tokens), 1.0), 3)\n",
    "        feedback['job_match_score'] = int(jd_score*100)\n",
    "        feedback['job_common_terms'] = len(common)\n",
    "        feedback['job_skill_overlap'] = [s for s in SKILLS_DB if s in jd_tokens and s in resume_tokens]\n",
    "    return feedback\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Gemini API interaction\n",
    "# --------------------------------------------------\n",
    "\n",
    "def call_gemini_api(api_key, model_endpoint, prompt, timeout=60):\n",
    "    \"\"\"\n",
    "    Flexible POST to a generative text endpoint. The default expectation is a JSON POST with {\"prompt\": \"...\"}\n",
    "    If you use Google Generative Language API (v1beta2), set model_endpoint to:\n",
    "    https://generativelanguage.googleapis.com/v1beta2/models/text-bison-001:generateText?key=YOUR_API_KEY\n",
    "\n",
    "    If you use another provider, set endpoint accordingly and modify payload structure in this function.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    payload = {\n",
    "        'prompt': prompt,\n",
    "        'max_output_tokens': 1024\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # If model_endpoint already contains ?key=..., the api_key may be omitted\n",
    "        if 'key=' in model_endpoint:\n",
    "            resp = requests.post(model_endpoint, headers=headers, json=payload, timeout=timeout)\n",
    "        else:\n",
    "            # try adding api_key as query param\n",
    "            url = model_endpoint\n",
    "            if api_key:\n",
    "                if '?' in url:\n",
    "                    url = f\"{url}&key={api_key}\"\n",
    "                else:\n",
    "                    url = f\"{url}?key={api_key}\"\n",
    "            resp = requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
    "        if resp.status_code != 200:\n",
    "            return False, f\"API Error {resp.status_code}: {resp.text}\"\n",
    "        return True, resp.json()\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "# Helper to build prompt for Gemini\n",
    "\n",
    "def build_gemini_prompt(resume_text, jd_text=None, desired_format='recruiter_resume'):\n",
    "    \"\"\"\n",
    "    Instruct the generative model to return a JSON object with these keys:\n",
    "    - overall_score: integer 0-100\n",
    "    - suggestions: list of bullet suggestions\n",
    "    - updated_resume: full resume text in recruiter-friendly format (one-page for freshers)\n",
    "    - highlights: list of matched skills\n",
    "\n",
    "    The model is asked to output ONLY valid JSON. If the model outputs extra text, the app will try to extract the JSON block.\n",
    "    \"\"\"\n",
    "    instruction = (\n",
    "        \"You are an expert career coach for Data Science & ML candidates.\\n\"\n",
    "        \"INPUTS:\\n- resume_text: the candidate's resume plain text.\\n\"\n",
    "        \"- jd_text (optional): job description or role keywords.\\n\\n\"\n",
    "        \"TASK:\\n1) Evaluate the resume for recruiter-readiness and produce an overall_score (0-100).\\n\"\n",
    "        \"2) Provide a short list of suggestions to improve the resume (max 8 bullets).\\n\"\n",
    "        \"3) Produce an UPDATED_RESUME in a clear, recruiter-friendly format (one page for freshers, 1-2 pages for experienced).\\n\"\n",
    "        \"4) Provide a highlights list of matched skills found in the resume and JD.\\n\\n\"\n",
    "        \"OUTPUT FORMAT:\\nReturn ONLY a JSON object (no extra commentary) with keys: overall_score (int), suggestions (array of strings), updated_resume (string), highlights (array of strings).\\n\\n\"\n",
    "        \"Make the updated_resume concise, use bullet points under each job/project with measurable impact, include contact header, skills section, projects, and education. If jd_text is provided, tailor the resume to include relevant keywords from the JD while preserving truthfulness.\\n\\n\"\n",
    "        \"Begin JSON now.\\n\\n\"\n",
    "    )\n",
    "\n",
    "    # Build body\n",
    "    body = {\n",
    "        'resume_text': resume_text,\n",
    "        'jd_text': jd_text or ''\n",
    "    }\n",
    "\n",
    "    prompt = instruction + \"INPUT_JSON:\" + json.dumps(body)\n",
    "    return prompt\n",
    "\n",
    "# Utility: extract JSON from model output (robust)\n",
    "\n",
    "def extract_json_from_text(text):\n",
    "    # find first { ... }\n",
    "    try:\n",
    "        start = text.index('{')\n",
    "        end = text.rindex('}')\n",
    "        candidate = text[start:end+1]\n",
    "        return json.loads(candidate)\n",
    "    except Exception:\n",
    "        # fallback: try to find line that starts with { and ends with }\n",
    "        matches = re.findall(r'\\{.*\\}', text, flags=re.DOTALL)\n",
    "        for m in matches:\n",
    "            try:\n",
    "                return json.loads(m)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "# --------------------------------------------------\n",
    "# DOCX creation helper\n",
    "# --------------------------------------------------\n",
    "\n",
    "def create_docx_from_text(resume_text):\n",
    "    doc = Document()\n",
    "    lines = resume_text.split('\\n')\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        # If line looks like a heading (ALL CAPS or ends with ':'), make it bold\n",
    "        if len(line) < 60 and (line.isupper() or line.endswith(':')):\n",
    "            p = doc.add_paragraph()\n",
    "            run = p.add_run(line)\n",
    "            run.bold = True\n",
    "        else:\n",
    "            doc.add_paragraph(line)\n",
    "    # Save to bytes\n",
    "    bio = io.BytesIO()\n",
    "    doc.save(bio)\n",
    "    bio.seek(0)\n",
    "    return bio\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Streamlit UI\n",
    "# --------------------------------------------------\n",
    "\n",
    "st.set_page_config(page_title='Resume & Profile Analyzer (Gemini)', layout='wide')\n",
    "st.title('Resume, LinkedIn & GitHub Analyzer (Gemini-powered)')\n",
    "st.markdown('Upload your resume and optionally paste a job description. Provide your Gemini/Generative API key and endpoint, then click Analyze to get model-driven scoring, suggestions, and an updated resume (DOCX).')\n",
    "\n",
    "col1, col2 = st.columns([1,2])\n",
    "with col1:\n",
    "    uploaded_file = st.file_uploader('Upload Resume', type=['pdf','docx','txt'])\n",
    "    jd_text = st.text_area('Paste Job Description / Role Keywords (optional)', height=200)\n",
    "    api_key = st.text_input('Generative API Key (Gemini/Vertex API Key)', type='password')\n",
    "    model_endpoint = st.text_input('Model Endpoint URL', value='https://generativelanguage.googleapis.com/v1beta2/models/text-bison-001:generateText')\n",
    "    analyze_btn = st.button('Analyze with Gemini')\n",
    "\n",
    "with col2:\n",
    "    st.info('Tips: If using Google Generative Language API, you can use the text-bison-001 model endpoint and provide the API key either in the query string or here. Adjust the Model Endpoint if you use a different provider.')\n",
    "    st.subheader('Sample Skills (Data Science):')\n",
    "    st.write(', '.join(SKILLS_DB))\n",
    "\n",
    "resume_text = ''\n",
    "if uploaded_file is not None:\n",
    "    st.write('Uploaded file:', uploaded_file.name)\n",
    "    if uploaded_file.name.lower().endswith('.pdf'):\n",
    "        resume_text = extract_text_from_pdf(uploaded_file)\n",
    "    elif uploaded_file.name.lower().endswith('.docx'):\n",
    "        resume_text = extract_text_from_docx(uploaded_file)\n",
    "    else:\n",
    "        resume_text = extract_text_from_txt(uploaded_file)\n",
    "\n",
    "if analyze_btn:\n",
    "    if not resume_text:\n",
    "        st.error('No resume text found. Please upload a supported file with readable text.')\n",
    "    elif not api_key:\n",
    "        st.error('Please provide your Generative API Key to call Gemini/Generative API.')\n",
    "    else:\n",
    "        with st.spinner('Calling Gemini and preparing feedback...'):\n",
    "            # Local quick analysis first\n",
    "            local_feedback = generate_local_feedback(resume_text, jd_text)\n",
    "\n",
    "            # Build prompt and call external API\n",
    "            prompt = build_gemini_prompt(resume_text, jd_text)\n",
    "            ok, resp = call_gemini_api(api_key, model_endpoint, prompt)\n",
    "            if not ok:\n",
    "                st.error(f'API call failed: {resp}')\n",
    "            else:\n",
    "                # Try to parse response JSON from model\n",
    "                # Generative API responses differ by provider; attempt to extract text\n",
    "                model_text = ''\n",
    "                # If response has 'candidates' or 'output' fields adaptively pick\n",
    "                if isinstance(resp, dict):\n",
    "                    # Attempt to find text in common fields\n",
    "                    if 'candidates' in resp and isinstance(resp['candidates'], list) and len(resp['candidates'])>0:\n",
    "                        model_text = resp['candidates'][0].get('content', '') or json.dumps(resp['candidates'][0])\n",
    "                    elif 'output' in resp:\n",
    "                        model_text = resp['output'] if isinstance(resp['output'], str) else json.dumps(resp['output'])\n",
    "                    else:\n",
    "                        # fallback to stringifying full response\n",
    "                        model_text = json.dumps(resp)\n",
    "                else:\n",
    "                    model_text = str(resp)\n",
    "\n",
    "                # try extract JSON\n",
    "                parsed = extract_json_from_text(model_text)\n",
    "                if parsed is None:\n",
    "                    st.warning('Could not extract structured JSON from model output. Showing raw output for review.')\n",
    "                    st.code(model_text[:10000])\n",
    "                else:\n",
    "                    # Merge local feedback and model feedback where appropriate\n",
    "                    overall = parsed.get('overall_score', local_feedback['overall_score'])\n",
    "                    suggestions = parsed.get('suggestions', [])\n",
    "                    updated_resume = parsed.get('updated_resume', '')\n",
    "                    highlights = parsed.get('highlights', local_feedback['skills_found'])\n",
    "\n",
    "                    st.metric('Overall Score (model)', f\"{overall} / 100\")\n",
    "                    st.subheader('Top Suggestions (from model)')\n",
    "                    for s in suggestions:\n",
    "                        st.write('-', s)\n",
    "\n",
    "                    st.subheader('Highlights (skills matched)')\n",
    "                    st.write(', '.join(highlights) if highlights else 'None')\n",
    "\n",
    "                    st.subheader('Updated Resume (Model-generated)')\n",
    "                    st.text_area('Updated Resume', value=updated_resume, height=500)\n",
    "\n",
    "                    # Create DOCX and provide download\n",
    "                    docx_bytes = create_docx_from_text(updated_resume)\n",
    "                    b64 = base64.b64encode(docx_bytes.getvalue()).decode()\n",
    "                    href = f\"data:application/vnd.openxmlformats-officedocument.wordprocessingml.document;base64,{b64}\"\n",
    "                    st.markdown(f\"[Download Updated Resume (DOCX)]({href})\")\n",
    "\n",
    "                    # Also produce a plain feedback report\n",
    "                    report_lines = []\n",
    "                    report_lines.append(f\"Overall Score: {overall} / 100\")\n",
    "                    report_lines.append('\\nSuggestions:')\n",
    "                    report_lines.extend(suggestions)\n",
    "                    report_text = '\\n'.join(report_lines)\n",
    "                    b = report_text.encode('utf-8')\n",
    "                    href2 = f\"data:file/txt;base64,{base64.b64encode(b).decode()}\"\n",
    "                    st.markdown(f\"[Download Feedback Report]({href2})\")\n",
    "\n",
    "st.markdown('---')\n",
    "st.caption('This app demonstrates how to combine local heuristics with a generative model (Gemini/Vertex/other). Adjust prompts and endpoint according to your provider. Do not send sensitive personal data unless you accept the privacy implications.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8f5afc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mextract_text\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mc:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mUsers\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mhp\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mDownloads\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mNikhil.pdf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mextract_text\u001b[39m\u001b[34m(file)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m file_path = \u001b[43mfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\n\u001b[32m     22\u001b[39m ext = file_path.split(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m].lower()\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# -------- PDF --------\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'name'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "extract_text(r'c:\\Users\\hp\\Downloads\\Nikhil.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df963d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Prompt Builder (IMPORTANT)\n",
    "# ----------------------------\n",
    "def build_prompt(resume_text, job_role, job_desc):\n",
    "    return f\"\"\"\n",
    "You are an expert ATS system and technical interviewer.\n",
    "\n",
    "INPUTS:\n",
    "Resume Text:\n",
    "\\\"\\\"\\\"{resume_text}\\\"\\\"\\\"\n",
    "\n",
    "Job Role:\n",
    "\\\"\\\"\\\"{job_role}\\\"\\\"\\\"\n",
    "\n",
    "Job Description:\n",
    "\\\"\\\"\\\"{job_desc}\\\"\\\"\\\"\n",
    "\n",
    "TASKS:\n",
    "1. Calculate an ATS compatibility score (0‚Äì100).\n",
    "2. Generate 10‚Äì15 interview questions tailored to:\n",
    "   - resume\n",
    "   - job role\n",
    "   - job description\n",
    "3. Questions should include:\n",
    "   - Technical\n",
    "   - Project-based\n",
    "   - Behavioral\n",
    "   - Scenario-based\n",
    "\n",
    "OUTPUT FORMAT (STRICT JSON ONLY):\n",
    "{{\n",
    "  \"ats_score\": number,\n",
    "  \"interview_questions\": [\n",
    "    \"Question 1\",\n",
    "    \"Question 2\"\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Do not add explanations. Return JSON only.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Gemini API Call\n",
    "# ----------------------------\n",
    "def call_gemini(api_key='AIzaSyBbW3UmL22kiw4FppXyVrgTCS0V2Ko1N4s', prompt=\"summarize the the data science skills\"):\n",
    "    url = (\n",
    "        \"https://generativelanguage.googleapis.com/v1beta/models/\"\n",
    "        \"gemini-1.5-flash:generateContent\"\n",
    "        f\"?key={api_key}\"\n",
    "    )\n",
    "\n",
    "    payload = {\n",
    "        \"contents\": [\n",
    "            {\n",
    "                \"parts\": [{\"text\": prompt}]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload, timeout=60)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        return f\"API Error: {response.text}\", None\n",
    "\n",
    "    try:\n",
    "        text = response.json()[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "        data = json.loads(text)\n",
    "        return None, data\n",
    "    except Exception as e:\n",
    "        return \"Failed to parse Gemini response\", None\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Main Gradio Function\n",
    "# ----------------------------\n",
    "def analyze_resume(file, job_role, job_desc, api_key):\n",
    "    resume_text = extract_text(file)\n",
    "\n",
    "    if not resume_text.strip():\n",
    "        return \"‚ùå Could not extract resume text\", \"\", None\n",
    "\n",
    "    prompt = build_prompt(resume_text, job_role, job_desc)\n",
    "    error, result = call_gemini(api_key, prompt)\n",
    "\n",
    "    if error:\n",
    "        return error, \"\", None\n",
    "\n",
    "    ats = result[\"ats_score\"]\n",
    "    questions = result[\"interview_questions\"]\n",
    "\n",
    "    questions_text = \"\\n\".join(\n",
    "        [f\"{i+1}. {q}\" for i, q in enumerate(questions)]\n",
    "    )\n",
    "\n",
    "    # Save questions to file\n",
    "    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".txt\")\n",
    "    tmp.write(questions_text.encode(\"utf-8\"))\n",
    "    tmp.close()\n",
    "\n",
    "    summary = f\"‚úÖ ATS Score: {ats} / 100\"\n",
    "\n",
    "    return summary, questions_text, tmp.name\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Gradio UI\n",
    "# ----------------------------\n",
    "with gr.Blocks(title=\"AI Resume ATS & Interview Generator\") as demo:\n",
    "\n",
    "    gr.Markdown(\"\"\"\n",
    "    # üìÑ AI Resume ATS & Interview Question Generator (Gemini)\n",
    "    Upload resume ‚Üí add job role & JD ‚Üí get ATS score + interview questions\n",
    "    \"\"\")\n",
    "\n",
    "    resume = gr.File(label=\"Upload Resume (PDF / DOCX / TXT)\")\n",
    "    job_role = gr.Textbox(label=\"Job Role\", placeholder=\"Data Scientist\")\n",
    "    job_desc = gr.Textbox(label=\"Job Description\", lines=6)\n",
    "    api_key = gr.Textbox(label=\"Gemini API Key\", type=\"password\")\n",
    "\n",
    "    submit = gr.Button(\"Analyze Resume üöÄ\")\n",
    "\n",
    "    ats_output = gr.Textbox(label=\"ATS Score\")\n",
    "    questions_output = gr.Textbox(\n",
    "        label=\"Generated Interview Questions\", lines=15\n",
    "    )\n",
    "    download = gr.File(label=\"Download Interview Questions\")\n",
    "\n",
    "    submit.click(\n",
    "        analyze_resume,\n",
    "        inputs=[resume, job_role, job_desc, api_key],\n",
    "        outputs=[ats_output, questions_output, download]\n",
    "    )\n",
    "\n",
    "    gr.Markdown(\"\"\"\n",
    "    **Demo Notes**\n",
    "    - ATS logic handled by Gemini\n",
    "    - Easily replace Gemini with OpenAI / Claude\n",
    "    - Can extend to feedback, resume rewrite, mock interview\n",
    "    \"\"\")\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b044934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Access is denied.\n"
     ]
    }
   ],
   "source": [
    "# ModuleNotFoundError: No module named 'pdfplumber'\n",
    "# !pip install pdfplumber\n",
    "# pip install docx2txt\n",
    "# AIzaSyBbW3UmL22kiw4FppXyVrgTCS0V2Ko1N4s\n",
    "# gemini-1.5-flash gemini-1.5-flash\n",
    "# AIzaSyBbW3UmL22kiw4FppXyVrgTCS0V2Ko1N4s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5221f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n",
      "{\n",
      "  \"error\": {\n",
      "    \"code\": 404,\n",
      "    \"message\": \"models/gemini-1.5-flash is not found for API version v1, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\",\n",
      "    \"status\": \"NOT_FOUND\"\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "\n",
    "API_KEY = \"AIzaSyDHXP-hKN5b4QFXLAope3kYOCUiie-BA2o\"\n",
    "\n",
    "url = \"https://generativelanguage.googleapis.com/v1/models/gemini-1.5-flash:generateContent\"\n",
    "\n",
    "payload = {\n",
    "    \"contents\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"parts\": [{\"text\": \"Return JSON {\\\"status\\\":\\\"ok\\\"}\"}]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "res = requests.post(\n",
    "    url,\n",
    "    headers={\"Content-Type\": \"application/json\"},\n",
    "    params={\"key\": API_KEY},\n",
    "    json=payload\n",
    ")\n",
    "\n",
    "print(res.status_code)\n",
    "print(res.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9118a900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\queueing.py\", line 763, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\blocks.py\", line 2125, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<8 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\blocks.py\", line 1607, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        fn, *processed_input, limiter=self.limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 1005, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\utils.py\", line 1066, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_9968\\3648929735.py\", line 162, in analyze_resume\n",
      "    tmp.write(file.read())\n",
      "              ^^^^^^^^^\n",
      "AttributeError: 'NamedString' object has no attribute 'read'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\queueing.py\", line 763, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\blocks.py\", line 2125, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<8 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\blocks.py\", line 1607, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        fn, *processed_input, limiter=self.limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 1005, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\utils.py\", line 1066, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_9968\\3648929735.py\", line 162, in analyze_resume\n",
      "    tmp.write(file.read())\n",
      "              ^^^^^^^^^\n",
      "AttributeError: 'NamedString' object has no attribute 'read'\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import re\n",
    "import io\n",
    "import tempfile\n",
    "from collections import Counter\n",
    "\n",
    "# ------------------------------\n",
    "# Optional imports\n",
    "# ------------------------------\n",
    "try:\n",
    "    import PyPDF2\n",
    "except Exception:\n",
    "    PyPDF2 = None\n",
    "\n",
    "try:\n",
    "    import docx2txt\n",
    "except Exception:\n",
    "    docx2txt = None\n",
    "\n",
    "# ------------------------------\n",
    "# Constants\n",
    "# ------------------------------\n",
    "SKILLS_DB = [\n",
    "    'python','pandas','numpy','scikit-learn','sklearn','tensorflow','pytorch',\n",
    "    'sql','excel','power bi','tableau','matplotlib','seaborn','nlp','computer vision',\n",
    "    'regression','classification','clustering','random forest','xgboost','lightgbm',\n",
    "    'git','github','docker','aws','gcp','azure','bigquery','spark','hadoop'\n",
    "]\n",
    "\n",
    "SECTION_HEADERS = [\n",
    "    'experience','education','projects','skills','certifications',\n",
    "    'summary','objective','publications','contact'\n",
    "]\n",
    "\n",
    "URL_PATTERN = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "EMAIL_PATTERN = re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
    "PHONE_PATTERN = re.compile(r'(?:\\+?\\d{1,3}[\\s-]?)?(?:\\(?\\d{3}\\)?[\\s-]?|\\d{3}[\\s-]?)\\d{3}[\\s-]?\\d{4}')\n",
    "LINKEDIN_PATTERN = re.compile(r'(?:https?://)?(?:www\\.)?linkedin\\.com/\\S+', re.IGNORECASE)\n",
    "GITHUB_PATTERN = re.compile(r'(?:https?://)?(?:www\\.)?github\\.com/\\S+', re.IGNORECASE)\n",
    "\n",
    "# ------------------------------\n",
    "# Utility Functions\n",
    "# ------------------------------\n",
    "def extract_text_from_pdf(file_path):\n",
    "    if PyPDF2 is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        reader = PyPDF2.PdfReader(file_path)\n",
    "        return \"\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_docx(file_path):\n",
    "    if docx2txt is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return docx2txt.process(file_path)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_txt(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            return f.read()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    return [w for w in text.split() if len(w) > 1]\n",
    "\n",
    "def detect_sections(text):\n",
    "    text = text.lower()\n",
    "    return {sec: sec in text for sec in SECTION_HEADERS}\n",
    "\n",
    "def simple_keyword_match(resume_text, jd_text):\n",
    "    resume_tokens = set(clean_and_tokenize(resume_text))\n",
    "    jd_tokens = set(clean_and_tokenize(jd_text))\n",
    "\n",
    "    if not jd_tokens:\n",
    "        return 0, 0, []\n",
    "\n",
    "    common = resume_tokens & jd_tokens\n",
    "    skill_overlap = [s for s in SKILLS_DB if s in resume_tokens and s in jd_tokens]\n",
    "    score = min(1.0, len(common) / len(jd_tokens))\n",
    "\n",
    "    return int(score * 100), len(common), skill_overlap\n",
    "\n",
    "def generate_feedback(resume_text, jd_text):\n",
    "    tokens = clean_and_tokenize(resume_text)\n",
    "    sections = detect_sections(resume_text)\n",
    "    skills_found = [s for s in SKILLS_DB if s in resume_text.lower()]\n",
    "\n",
    "    emails = EMAIL_PATTERN.findall(resume_text)\n",
    "    phones = PHONE_PATTERN.findall(resume_text)\n",
    "    linkedin = LINKEDIN_PATTERN.findall(resume_text)\n",
    "    github = GITHUB_PATTERN.findall(resume_text)\n",
    "\n",
    "    length_words = len(tokens)\n",
    "    length_score = 1.0 if 200 <= length_words <= 800 else max(0.2, min(1.0, length_words / 800))\n",
    "    contact_score = 1.0 if (emails or phones) else 0.0\n",
    "    section_score = sum(sections.values()) / len(sections)\n",
    "    skills_score = min(1.0, len(skills_found) / 8)\n",
    "\n",
    "    overall = round((0.3*length_score + 0.2*contact_score +\n",
    "                     0.2*section_score + 0.3*skills_score) * 100)\n",
    "\n",
    "    feedback = {\n",
    "        \"overall\": overall,\n",
    "        \"words\": length_words,\n",
    "        \"emails\": emails,\n",
    "        \"phones\": phones,\n",
    "        \"linkedin\": linkedin,\n",
    "        \"github\": github,\n",
    "        \"sections\": sections,\n",
    "        \"skills\": skills_found\n",
    "    }\n",
    "\n",
    "    if jd_text.strip():\n",
    "        jm, common, overlap = simple_keyword_match(resume_text, jd_text)\n",
    "        feedback[\"job_match\"] = jm\n",
    "        feedback[\"job_common\"] = common\n",
    "        feedback[\"job_overlap\"] = overlap\n",
    "\n",
    "    return feedback\n",
    "\n",
    "def create_report(feedback):\n",
    "    lines = [\n",
    "        f\"Overall Profile Score: {feedback['overall']} / 100\",\n",
    "        f\"Word Count: {feedback['words']}\",\n",
    "        f\"Emails: {', '.join(feedback['emails']) or 'None'}\",\n",
    "        f\"Phones: {', '.join(feedback['phones']) or 'None'}\",\n",
    "        f\"LinkedIn: {', '.join(feedback['linkedin']) or 'Not Found'}\",\n",
    "        f\"GitHub: {', '.join(feedback['github']) or 'Not Found'}\",\n",
    "        \"\",\n",
    "        \"Sections Found:\",\n",
    "        \", \".join([k for k,v in feedback[\"sections\"].items() if v]),\n",
    "        \"\",\n",
    "        f\"Skills Detected: {', '.join(feedback['skills']) or 'None'}\"\n",
    "    ]\n",
    "\n",
    "    if \"job_match\" in feedback:\n",
    "        lines += [\n",
    "            \"\",\n",
    "            f\"Job Match Score: {feedback['job_match']} / 100\",\n",
    "            f\"Common Terms: {feedback['job_common']}\",\n",
    "            f\"Skill Overlap: {', '.join(feedback['job_overlap']) or 'None'}\"\n",
    "        ]\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# ------------------------------\n",
    "# Gradio App Logic\n",
    "# ------------------------------\n",
    "def analyze_resume(file, jd_text):\n",
    "    if file is None:\n",
    "        return \"‚ùå Please upload a resume file.\", None\n",
    "\n",
    "    suffix = file.name.split(\".\")[-1].lower()\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=f\".{suffix}\") as tmp:\n",
    "        tmp.write(file.read())\n",
    "        path = tmp.name\n",
    "\n",
    "    if suffix == \"pdf\":\n",
    "        resume_text = extract_text_from_pdf(path)\n",
    "    elif suffix == \"docx\":\n",
    "        resume_text = extract_text_from_docx(path)\n",
    "    else:\n",
    "        resume_text = extract_text_from_txt(path)\n",
    "\n",
    "    if not resume_text.strip():\n",
    "        return \"‚ùå Unable to extract text from resume.\", None\n",
    "\n",
    "    feedback = generate_feedback(resume_text, jd_text)\n",
    "    report = create_report(feedback)\n",
    "\n",
    "    report_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".txt\")\n",
    "    report_file.write(report.encode(\"utf-8\"))\n",
    "    report_file.close()\n",
    "\n",
    "    return report, report_file.name\n",
    "\n",
    "# ------------------------------\n",
    "# Gradio UI\n",
    "# ------------------------------\n",
    "with gr.Blocks(title=\"Resume & Profile Analyzer\") as demo:\n",
    "\n",
    "    gr.Markdown(\"\"\"\n",
    "    # üìÑ Resume, LinkedIn & GitHub Analyzer  \n",
    "    Upload your resume and optionally paste a job description  \n",
    "    to get instant, explainable feedback.\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "        resume_file = gr.File(label=\"Upload Resume (PDF / DOCX / TXT)\")\n",
    "        jd_input = gr.Textbox(label=\"Job Description (Optional)\", lines=8)\n",
    "\n",
    "    analyze_btn = gr.Button(\"Analyze Resume üöÄ\")\n",
    "\n",
    "    output_text = gr.Textbox(label=\"Analysis Report\", lines=18)\n",
    "    download_file = gr.File(label=\"Download Feedback Report\")\n",
    "\n",
    "    analyze_btn.click(\n",
    "        analyze_resume,\n",
    "        inputs=[resume_file, jd_input],\n",
    "        outputs=[output_text, download_file]\n",
    "    )\n",
    "\n",
    "    gr.Markdown(\"\"\"\n",
    "    **Demo Notes**\n",
    "    - Rule-based & explainable (great for teaching)\n",
    "    - Extend with NLP, embeddings, ATS scoring, RAG, or LLMs\n",
    "    \"\"\")\n",
    "\n",
    "# ------------------------------\n",
    "# Launch\n",
    "# ------------------------------\n",
    "demo.launch(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67229d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca61c20c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
