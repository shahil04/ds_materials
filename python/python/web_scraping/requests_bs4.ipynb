{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Web Scraping?\n",
    "\n",
    "All you need to know about web scraping as a method of extracting data from websites.\n",
    "\n",
    "Web scraping is the method of extracting structured information from a web page. It means that web scraping automates manually finding and saving the information on a website you find valuable. Although web scraping can be done manually, in most cases, automatic tools are favored when scraping web data as they can be less costly and work at a faster rate.\n",
    "\n",
    "##### How does Web Scraping work?\n",
    "3 simple steps can represent a general web scraping case:\n",
    "\n",
    "- Request a web page content by URL (open the web page to get the HTML content) or via the direct API call.\n",
    "- Convert the mess of the HTML tags into extracted and structured data by parsing (like we do it when a copy-pasting particular text part).\n",
    "- Store the extracted data into preferred storage: database, text file, CSV, Excel, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://docs.scrapingant.com/web-scraping-101/what-is-web-scraping\n",
    "- https://pokeapi.co/  practice api call\n",
    "- https://httpbin.org/#/  https://youtu.be/Xi1F2ZMAZ7Q?feature=shared Requests Library in Python - Beginner Crash Course\n",
    "- Using urllib to Make a HTTP Request With Python https://youtu.be/00fMyoSLKek?feature=shared\n",
    "- handle api https://freeapi.app/ -- https://youtu.be/g33-tYIs7zU?feature=shared\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REquests\n",
    "- https://requests.readthedocs.io/en/latest/user/quickstart/\n",
    "- https://youtu.be/FagmjKdOIDs?feature=shared Web Scraping Course \n",
    "- https://youtu.be/Xi1F2ZMAZ7Q?feature=shared Requests Library in Python\n",
    "- https://pypi.org/project/requests/00fMyoSLKek\n",
    "- https://requests.readthedocs.io/en/latest/\n",
    "- https://requests.readthedocs.io/en/latest/user/quickstart/\n",
    "- https://realpython.com/python-requests/ ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# pip install requests\n",
    "r = requests.get('https://api.github.com/user', auth=('user', 'pass'))\n",
    "r.status_code\n",
    "# 200\n",
    "r.headers['content-type']\n",
    "# 'application/json; charset=utf8'\n",
    "r.encoding\n",
    "# 'utf-8'\n",
    "r.text\n",
    "# '{\"type\":\"User\"...'\n",
    "r.json()\n",
    "# {'private_gists': 419, 'total_private_repos': 77, ...}\n",
    "\n",
    "\n",
    "\n",
    "x = requests.get('https://w3schools.com/python/demopage.htm')\n",
    "\n",
    "print(x.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tutorialspoint.com/requests/requests_quick_guide.htm\n",
    "---\n",
    "\n",
    "# **Python `requests` Library: An In-Depth Guide**\n",
    "\n",
    "## **1. Introduction to the `requests` Library**\n",
    "\n",
    "### **1.1 What is the `requests` Library?**\n",
    "The `requests` library is a popular Python package used for making HTTP requests in a simple and human-friendly way. It abstracts the complexities of handling HTTP requests and responses, allowing developers to focus on their application logic.\n",
    "\n",
    "### **1.2 Why Use `requests`?**\n",
    "- **Ease of Use:** Simplifies the process of making HTTP requests.\n",
    "- **Human Readable:** Provides a clean, human-readable syntax for working with HTTP.\n",
    "- **Robustness:** Handles many common tasks such as authentication, sessions, and more.\n",
    "- **Compatibility:** Works with Python 2.7 and 3.5+.\n",
    "\n",
    "### **1.3 Installing `requests`**\n",
    "To install the `requests` library, use pip:\n",
    "\n",
    "```bash\n",
    "pip install requests\n",
    "```\n",
    "\n",
    "## **2. Basic Syntax and Usage**\n",
    "\n",
    "### **2.1 Making a Simple GET Request**\n",
    "- **Definition**: The `GET` method is used to retrieve data from a specified resource.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "response = requests.get('https://jsonplaceholder.typicode.com/posts')\n",
    "print(response.status_code)  # Output: 200\n",
    "print(response.text)  # Prints the content of the response\n",
    "```\n",
    "\n",
    "### **2.2 Making a Simple POST Request**\n",
    "- **Definition**: The `POST` method is used to send data to the server to create/update a resource.\n",
    "\n",
    "```python\n",
    "url = 'https://jsonplaceholder.typicode.com/posts'\n",
    "data = {\n",
    "    'title': 'foo',\n",
    "    'body': 'bar',\n",
    "    'userId': 1\n",
    "}\n",
    "response = requests.post(url, json=data)\n",
    "print(response.status_code)  # Output: 201\n",
    "print(response.json())  # Prints the JSON response\n",
    "```\n",
    "\n",
    "### **2.3 Common Methods**\n",
    "- **`requests.get()`**: Sends a GET request.\n",
    "- **`requests.post()`**: Sends a POST request.\n",
    "- **`requests.put()`**: Sends a PUT request.\n",
    "- **`requests.delete()`**: Sends a DELETE request.\n",
    "- **`requests.head()`**: Sends a HEAD request.\n",
    "- **`requests.options()`**: Sends an OPTIONS request.\n",
    "\n",
    "### **2.4 Handling Query Parameters**\n",
    "- **Definition**: Query parameters are appended to the URL to pass data in a GET request.\n",
    "\n",
    "```python\n",
    "url = 'https://jsonplaceholder.typicode.com/posts'\n",
    "params = {'userId': 1}\n",
    "response = requests.get(url, params=params)\n",
    "print(response.url)  # Output: https://jsonplaceholder.typicode.com/posts?userId=1\n",
    "```\n",
    "\n",
    "### **2.5 Handling Response Content**\n",
    "- **`response.text`**: Returns the content of the response in Unicode.\n",
    "- **`response.content`**: Returns the content of the response in bytes.\n",
    "- **`response.json()`**: Returns the JSON content of the response, if any.\n",
    "- **`response.status_code`**: Returns the HTTP status code of the response.\n",
    "- **`response.headers`**: Returns the headers of the response.\n",
    "\n",
    "```python\n",
    "response = requests.get('https://jsonplaceholder.typicode.com/posts/1')\n",
    "print(response.text)  # Unicode content\n",
    "print(response.content)  # Byte content\n",
    "print(response.json())  # JSON content\n",
    "print(response.status_code)  # Status code\n",
    "print(response.headers)  # Headers\n",
    "```\n",
    "\n",
    "## **3. Advanced Usage**\n",
    "\n",
    "### **3.1 Sending Headers with Requests**\n",
    "- **Definition**: Custom headers can be sent along with a request to provide additional information.\n",
    "\n",
    "```python\n",
    "url = 'https://jsonplaceholder.typicode.com/posts'\n",
    "headers = {'Authorization': 'Bearer your_token'}\n",
    "response = requests.get(url, headers=headers)\n",
    "print(response.status_code)  # Output: 200\n",
    "```\n",
    "\n",
    "### **3.2 Sending Data in a POST Request**\n",
    "- **Form Data**: Sent as `application/x-www-form-urlencoded`.\n",
    "\n",
    "```python\n",
    "url = 'https://jsonplaceholder.typicode.com/posts'\n",
    "data = {'title': 'foo', 'body': 'bar', 'userId': 1}\n",
    "response = requests.post(url, data=data)\n",
    "print(response.json())\n",
    "```\n",
    "\n",
    "- **JSON Data**: Sent as `application/json`.\n",
    "\n",
    "```python\n",
    "url = 'https://jsonplaceholder.typicode.com/posts'\n",
    "json_data = {'title': 'foo', 'body': 'bar', 'userId': 1}\n",
    "response = requests.post(url, json=json_data)\n",
    "print(response.json())\n",
    "```\n",
    "\n",
    "### **3.3 Handling Timeout**\n",
    "- **Definition**: A timeout exception is raised if the server does not respond within the specified time.\n",
    "\n",
    "```python\n",
    "try:\n",
    "    response = requests.get('https://httpbin.org/delay/10', timeout=5)\n",
    "except requests.Timeout:\n",
    "    print('The request timed out')\n",
    "```\n",
    "\n",
    "### **3.4 Handling Errors and Exceptions**\n",
    "- **Definition**: Use exception handling to manage different types of errors that can occur.\n",
    "\n",
    "```python\n",
    "try:\n",
    "    response = requests.get('https://jsonplaceholder.typicode.com/invalid')\n",
    "    response.raise_for_status()  # Raises an HTTPError if the status is 4xx, 5xx\n",
    "except requests.HTTPError as err:\n",
    "    print(f'HTTP error occurred: {err}')\n",
    "except requests.RequestException as err:\n",
    "    print(f'Other error occurred: {err}')\n",
    "else:\n",
    "    print('Success!')\n",
    "```\n",
    "\n",
    "### **3.5 Session Objects**\n",
    "- **Definition**: Session objects allow you to persist certain parameters across requests, like headers, cookies, and more.\n",
    "\n",
    "```python\n",
    "session = requests.Session()\n",
    "session.headers.update({'Authorization': 'Bearer your_token'})\n",
    "\n",
    "response = session.get('https://jsonplaceholder.typicode.com/posts')\n",
    "print(response.status_code)\n",
    "```\n",
    "\n",
    "### **3.6 Uploading Files**\n",
    "- **Definition**: Files can be uploaded using the `files` parameter.\n",
    "\n",
    "```python\n",
    "url = 'https://httpbin.org/post'\n",
    "files = {'file': open('test.txt', 'rb')}\n",
    "response = requests.post(url, files=files)\n",
    "print(response.json())\n",
    "```\n",
    "\n",
    "### **3.7 Handling Cookies**\n",
    "- **Definition**: Cookies can be sent with requests or retrieved from responses.\n",
    "\n",
    "```python\n",
    "# Sending cookies\n",
    "url = 'https://httpbin.org/cookies'\n",
    "cookies = {'mycookie': 'chocolatechip'}\n",
    "response = requests.get(url, cookies=cookies)\n",
    "print(response.json())\n",
    "\n",
    "# Retrieving cookies\n",
    "response = requests.get('https://httpbin.org/cookies/set/sessioncookie/123456789')\n",
    "print(response.cookies['sessioncookie'])  # Output: 123456789\n",
    "```\n",
    "\n",
    "### **3.8 Handling Authentication**\n",
    "- **Definition**: Basic authentication and other forms of authentication can be managed using the `auth` parameter.\n",
    "\n",
    "```python\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "url = 'https://httpbin.org/basic-auth/user/pass'\n",
    "response = requests.get(url, auth=HTTPBasicAuth('user', 'pass'))\n",
    "print(response.json())  # Output: {'authenticated': True, 'user': 'user'}\n",
    "```\n",
    "\n",
    "## **4. Real-World Use Cases**\n",
    "\n",
    "### **4.1 Web Scraping**\n",
    "- **Scenario**: You need to scrape data from a website and process the content.\n",
    "\n",
    "```python\n",
    "url = 'https://example.com'\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    content = response.text\n",
    "    # Process the content, e.g., parse with BeautifulSoup\n",
    "else:\n",
    "    print('Failed to retrieve the webpage')\n",
    "```\n",
    "\n",
    "### **4.2 API Interaction**\n",
    "- **Scenario**: Interacting with a RESTful API to create, read, update, and delete resources.\n",
    "\n",
    "```python\n",
    "# Create a new resource\n",
    "url = 'https://jsonplaceholder.typicode.com/posts'\n",
    "json_data = {'title': 'foo', 'body': 'bar', 'userId': 1}\n",
    "response = requests.post(url, json=json_data)\n",
    "print(response.json())  # Output: The created resource\n",
    "\n",
    "# Retrieve a resource\n",
    "response = requests.get(url + '/1')\n",
    "print(response.json())  # Output: The resource with ID 1\n",
    "\n",
    "# Update a resource\n",
    "response = requests.put(url + '/1', json={'title': 'baz'})\n",
    "print(response.json())  # Output: The updated resource\n",
    "\n",
    "# Delete a resource\n",
    "response = requests.delete(url + '/1')\n",
    "print(response.status_code)  # Output: 200\n",
    "```\n",
    "\n",
    "### **4.3 Webhooks and Event-Driven Requests**\n",
    "- **Scenario**: Sending requests based on events, such as notifying a server when a specific action occurs.\n",
    "\n",
    "```python\n",
    "import json\n",
    "\n",
    "def notify_event(event_data):\n",
    "    url = 'https://example.com/webhook'\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    response = requests.post(url, data=json.dumps(event_data), headers=headers)\n",
    "    return response.status_code\n",
    "\n",
    "event_data = {'event': 'user_signup', 'user_id': 12345}\n",
    "status_code = notify_event(event_data)\n",
    "print(f'Notification sent with status code: {status_code}')\n",
    "```\n",
    "\n",
    "### **4.4 Downloading Files**\n",
    "- **Scenario**: Downloading files from the internet and saving them locally.\n",
    "\n",
    "```python\n",
    "url = 'https://example.com/file.zip'\n",
    "response = requests.get(url, stream=True)\n",
    "\n",
    "with open('file.zip', 'wb') as f:\n",
    "    for chunk in response.iter_content(chunk_size=8192):\n",
    "        f.write(chunk)\n",
    "print('File downloaded successfully')\n",
    "```\n",
    "\n",
    "## **5. Performance Considerations**\n",
    "\n",
    "### **5.1 Efficient Use of Sessions**\n",
    "- **Scenario**: Re\n",
    "\n",
    "use session objects to maintain TCP connection pooling and reduce overhead.\n",
    "\n",
    "```python\n",
    "session = requests.Session()\n",
    "for i in range(10):\n",
    "    response = session.get('https://example.com/data')\n",
    "    print(response.status_code)\n",
    "session.close()  # Close the session when done\n",
    "```\n",
    "\n",
    "### **5.2 Streaming Large Requests**\n",
    "- **Scenario**: Stream large files or content without loading it entirely into memory.\n",
    "\n",
    "```python\n",
    "url = 'https://example.com/largefile'\n",
    "response = requests.get(url, stream=True)\n",
    "\n",
    "with open('largefile.zip', 'wb') as f:\n",
    "    for chunk in response.iter_content(chunk_size=8192):\n",
    "        f.write(chunk)\n",
    "print('Large file downloaded successfully')\n",
    "```\n",
    "\n",
    "## **6. Advanced Topics**\n",
    "\n",
    "### **6.1 Custom Transport Adapters**\n",
    "- **Definition**: Transport adapters allow customization of the way requests are sent, such as using a custom SSL version or configuring retries.\n",
    "\n",
    "```python\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "session = requests.Session()\n",
    "retries = Retry(total=5, backoff_factor=0.1)\n",
    "adapter = HTTPAdapter(max_retries=retries)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "response = session.get('https://example.com')\n",
    "print(response.status_code)\n",
    "```\n",
    "\n",
    "### **6.2 Using Proxies**\n",
    "- **Definition**: Requests can be sent through proxies for anonymity or to bypass restrictions.\n",
    "\n",
    "```python\n",
    "proxies = {\n",
    "    'http': 'http://10.10.1.10:3128',\n",
    "    'https': 'https://10.10.1.10:1080',\n",
    "}\n",
    "\n",
    "response = requests.get('https://example.com', proxies=proxies)\n",
    "print(response.status_code)\n",
    "```\n",
    "\n",
    "### **6.3 Custom Authentication Schemes**\n",
    "- **Scenario**: Implementing a custom authentication scheme with `requests`.\n",
    "\n",
    "```python\n",
    "from requests.auth import AuthBase\n",
    "\n",
    "class CustomAuth(AuthBase):\n",
    "    def __call__(self, r):\n",
    "        r.headers['Custom-Auth'] = 'my-auth-token'\n",
    "        return r\n",
    "\n",
    "url = 'https://example.com/protected'\n",
    "response = requests.get(url, auth=CustomAuth())\n",
    "print(response.status_code)\n",
    "```\n",
    "\n",
    "## **7. Conclusion**\n",
    "The `requests` library is a powerful tool for interacting with the web and APIs in Python. It provides a simple, human-readable API that abstracts the complexities of HTTP. By mastering the concepts and examples outlined in this guide, you can efficiently make web requests, handle responses, manage sessions, and more in your Python applications.\n",
    "\n",
    "--- \n",
    "\n",
    "Feel free to adapt and expand upon this guide based on your needs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://pypi.org/project/beautifulsoup4/\n",
    "- https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Here's an in-depth guide to using the BeautifulSoup library in Python, covering everything from beginner to advanced concepts. This guide includes definitions, syntax explanations, use cases, real-world scenarios, and examples.\n",
    "\n",
    "---\n",
    "\n",
    "# **Python BeautifulSoup: An In-Depth Guide**\n",
    "\n",
    "## **1. Introduction to BeautifulSoup**\n",
    "\n",
    "### **1.1 What is BeautifulSoup?**\n",
    "- **Definition**: BeautifulSoup is a Python library used for parsing HTML and XML documents. It creates parse trees that are helpful for extracting data from HTML or XML documents in a hierarchical and readable way.\n",
    "\n",
    "### **1.2 Why Use BeautifulSoup?**\n",
    "- **Ease of Parsing**: Simplifies the process of extracting data from web pages.\n",
    "- **Flexible**: Can parse a variety of HTML/XML formats.\n",
    "- **Integration**: Works well with other libraries like `requests` to retrieve and parse content from the web.\n",
    "\n",
    "### **1.3 Installing BeautifulSoup**\n",
    "BeautifulSoup is often used with a parser like `lxml` or `html.parser`.\n",
    "\n",
    "```bash\n",
    "pip install beautifulsoup4 lxml\n",
    "```\n",
    "\n",
    "## **2. Basic Syntax and Usage**\n",
    "\n",
    "### **2.1 Creating a BeautifulSoup Object**\n",
    "- **Definition**: To work with an HTML or XML document, you first need to create a `BeautifulSoup` object.\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_doc = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "<p class=\"story\">...</p>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_doc, 'lxml')\n",
    "print(soup.prettify())\n",
    "```\n",
    "\n",
    "### **2.2 Navigating the Parse Tree**\n",
    "- **Tags**: Tags are the building blocks of HTML and XML documents.\n",
    "\n",
    "```python\n",
    "print(soup.title)  # Output: <title>The Dormouse's story</title>\n",
    "print(soup.title.name)  # Output: title\n",
    "print(soup.title.string)  # Output: The Dormouse's story\n",
    "```\n",
    "\n",
    "- **Attributes**: Attributes are properties of HTML tags.\n",
    "\n",
    "```python\n",
    "print(soup.p['class'])  # Output: ['title']\n",
    "print(soup.a['href'])  # Output: http://example.com/elsie\n",
    "```\n",
    "\n",
    "### **2.3 Searching the Tree**\n",
    "\n",
    "#### **2.3.1 Finding by Tag Name**\n",
    "- **Definition**: You can search for specific tags by their name.\n",
    "\n",
    "```python\n",
    "print(soup.find('a'))  # Output: First <a> tag\n",
    "print(soup.find_all('a'))  # Output: List of all <a> tags\n",
    "```\n",
    "\n",
    "#### **2.3.2 Finding by Attributes**\n",
    "- **Definition**: You can search for tags by their attributes, such as class, id, etc.\n",
    "\n",
    "```python\n",
    "print(soup.find('a', class_='sister'))  # Output: <a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>\n",
    "print(soup.find_all('a', id='link2'))  # Output: [<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a>]\n",
    "```\n",
    "\n",
    "#### **2.3.3 Using CSS Selectors**\n",
    "- **Definition**: BeautifulSoup allows you to search using CSS selectors.\n",
    "\n",
    "```python\n",
    "print(soup.select('p.story'))  # Output: List of all <p> tags with class=\"story\"\n",
    "print(soup.select('a#link1'))  # Output: List of <a> tags with id=\"link1\"\n",
    "```\n",
    "\n",
    "### **2.4 Modifying the Parse Tree**\n",
    "- **Definition**: You can modify the contents of a BeautifulSoup object.\n",
    "\n",
    "```python\n",
    "tag = soup.find('p', class_='story')\n",
    "tag.string = \"New story content.\"\n",
    "print(tag)  # Output: <p class=\"story\">New story content.</p>\n",
    "```\n",
    "\n",
    "### **2.5 Decomposing Tags**\n",
    "- **Definition**: Decomposing removes a tag and its content from the tree.\n",
    "\n",
    "```python\n",
    "tag = soup.find('p', class_='story')\n",
    "tag.decompose()\n",
    "print(soup.prettify())  # The <p> tag with class=\"story\" is removed\n",
    "```\n",
    "\n",
    "## **3. Advanced Usage**\n",
    "\n",
    "### **3.1 Navigating the Tree with Next/Previous Sibling**\n",
    "- **Definition**: You can navigate between siblings in the parse tree.\n",
    "\n",
    "```python\n",
    "sibling = soup.find('a', id='link1')\n",
    "print(sibling.next_sibling)  # Output: ',\\n'\n",
    "print(sibling.next_sibling.next_sibling)  # Output: <a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a>\n",
    "```\n",
    "\n",
    "### **3.2 Parent and Children**\n",
    "- **Definition**: Navigate through parent and child relationships in the tree.\n",
    "\n",
    "```python\n",
    "parent = soup.find('a', id='link1').parent\n",
    "print(parent)  # Output: <p class=\"story\">...</p>\n",
    "\n",
    "children = soup.find('p', class_='story').children\n",
    "for child in children:\n",
    "    print(child)  # Outputs each child element within the <p> tag\n",
    "```\n",
    "\n",
    "### **3.3 Stripping Tags**\n",
    "- **Definition**: You can extract text from tags, stripping out the tags themselves.\n",
    "\n",
    "```python\n",
    "text = soup.get_text()\n",
    "print(text)  # Outputs all text from the document without any tags\n",
    "```\n",
    "\n",
    "### **3.4 Handling Encodings**\n",
    "- **Definition**: Handle different encodings of HTML documents.\n",
    "\n",
    "```python\n",
    "soup = BeautifulSoup(html_doc, 'lxml', from_encoding='utf-8')\n",
    "```\n",
    "\n",
    "### **3.5 Working with XML**\n",
    "- **Definition**: BeautifulSoup can also parse XML documents.\n",
    "\n",
    "```python\n",
    "xml_doc = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<data>\n",
    "    <country name=\"Liechtenstein\">\n",
    "        <rank>1</rank>\n",
    "        <year>2008</year>\n",
    "        <gdppc>141100</gdppc>\n",
    "        <neighbor name=\"Austria\" direction=\"E\"/>\n",
    "        <neighbor name=\"Switzerland\" direction=\"W\"/>\n",
    "    </country>\n",
    "    <country name=\"Singapore\">\n",
    "        <rank>4</rank>\n",
    "        <year>2011</year>\n",
    "        <gdppc>59900</gdppc>\n",
    "        <neighbor name=\"Malaysia\" direction=\"N\"/>\n",
    "    </country>\n",
    "</data>\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(xml_doc, 'xml')\n",
    "print(soup.prettify())\n",
    "print(soup.find('country', name='Singapore'))  # Find specific XML tag\n",
    "```\n",
    "\n",
    "### **3.6 Handling Badly Formatted HTML**\n",
    "- **Definition**: BeautifulSoup is tolerant of bad HTML and can often parse it correctly.\n",
    "\n",
    "```python\n",
    "bad_html = \"<html><head><title>Test</title></head><body><h1>Test Header<h1><p>Test paragraph</p>\"\n",
    "soup = BeautifulSoup(bad_html, 'lxml')\n",
    "print(soup.prettify())  # BeautifulSoup corrects the badly formatted HTML\n",
    "```\n",
    "\n",
    "### **3.7 Extracting Data from Tables**\n",
    "- **Scenario**: Extracting and processing data from an HTML table.\n",
    "\n",
    "```python\n",
    "html_doc = \"\"\"\n",
    "<table>\n",
    "    <tr><th>Item</th><th>Price</th></tr>\n",
    "    <tr><td>Apple</td><td>$1</td></tr>\n",
    "    <tr><td>Banana</td><td>$2</td></tr>\n",
    "</table>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_doc, 'lxml')\n",
    "table = soup.find('table')\n",
    "rows = table.find_all('tr')\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    cols = [ele.text.strip() for ele in cols]\n",
    "    print(cols)  # Output: ['Apple', '$1'], ['Banana', '$2']\n",
    "```\n",
    "\n",
    "## **4. Real-World Use Cases**\n",
    "\n",
    "### **4.1 Web Scraping with BeautifulSoup and Requests**\n",
    "- **Scenario**: Scraping product information from an e-commerce site.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://example.com/products'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "products = soup.find_all('div', class_='product')\n",
    "for product in products:\n",
    "    name = product.find('h2').text\n",
    "    price = product.find('span', class_='price').text\n",
    "    print(f'Product: {name}, Price: {price}')\n",
    "```\n",
    "\n",
    "### **4.2 Automating Data Extraction**\n",
    "- **Scenario**: Extracting and saving data from a website to a CSV file.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "url = 'https://example.com/data'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "rows = soup.find('table').find_all('tr')\n",
    "with open('data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "\n",
    "    for row in rows:\n",
    "        cols = [ele.text.strip() for ele in row.find_all('td')]\n",
    "        writer.writerow(cols)\n",
    "```\n",
    "\n",
    "### **4.3 Parsing and Analyzing XML Data**\n",
    "- **Scenario**: Parsing XML data from an API and analyzing it.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://example.com/api/data.xml'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'xml')\n",
    "\n",
    "for country in soup.find_all('country'):\n",
    "    name = country['name']\n",
    "    rank = country.find('rank').text\n",
    "    print(f'Country: {name}, Rank: {rank}')\n",
    "```\n",
    "\n",
    "## **5. Performance Considerations**\n",
    "\n",
    "### **5.1 Efficient Parsing**\n",
    "- **Scenario**: Use the most appropriate parser for your use case.\n",
    "\n",
    "```python\n",
    "# Use 'lxml' for speed or 'html.parser' for a built-in Python parser.\n",
    "soup = BeautifulSoup(html_doc, 'lxml')\n",
    "```\n",
    "\n",
    "### **5.2 Avoiding Full Parse Trees**\n",
    "- **Scenario**: Avoid loading the entire document if you only need parts of it.\n",
    "\n",
    "```python\n",
    "soup = BeautifulSoup(html_doc, 'lxml')\n",
    "tag = soup.find('tag')\n",
    "```\n",
    "\n",
    "### **5.3 Caching Results**\n",
    "- **Scenario**: Cache parsed data for repeated use.\n",
    "\n",
    "```python\n",
    "# Use a library like `requests-cache` to cache requests, reducing the need to re-fetch and re-parse data.\n",
    "```\n",
    "\n",
    "## **6. Advanced Topics**\n",
    "\n",
    "### **6.1 Custom Parsers**\n",
    "- **Scenario**: Implement a custom parser if you have specific needs.\n",
    "\n",
    "```python\n",
    "# BeautifulSoup allows the creation of custom parsers, but this is an advanced topic that usually involves subclassing.\n",
    "```\n",
    "\n",
    "### **6.2 Handling JavaScript-rendered Content**\n",
    "- **Scenario**: Use `requests-html` or `Selenium` to handle JavaScript-rendered content before passing it to BeautifulSoup.\n",
    "\n",
    "```python\n",
    "from requests_html import HTMLSession\n",
    "\n",
    "session = HTMLSession()\n",
    "response = session.get('https://example.com')\n",
    "response.html.render()  # Render JavaScript\n",
    "soup = BeautifulSoup(response.html.html, 'lxml')\n",
    "```\n",
    "\n",
    "## **7. Conclusion**\n",
    "BeautifulSoup is a powerful tool for web scraping and data parsing in Python. With its intuitive syntax and robust features, it simplifies the process of navigating, searching, and modifying HTML or XML documents. By understanding the concepts and examples outlined in this guide, you can effectively use BeautifulSoup in various web scraping projects, from simple to complex tasks.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "pip install beautifulsoup4\n",
    "# pip install requests\n",
    "r = requests.get('https://www.tutorialsfreak.com/')\n",
    "print(r)\n",
    "r.content\n",
    "r.url\n",
    "r.status_code\n",
    "# parsing the html in web page \n",
    "# and using bs4 to extract data from html pages\n",
    "soup = bs(r.content, \"html.parser\")\n",
    "print(soup.prettify())\n",
    "soup.title\n",
    "soup.title.name\n",
    "soup.p\n",
    "soup.a\n",
    "soup.h1\n",
    "\n",
    "# kinds of objects bs in ws\n",
    "tags\n",
    "navigablrstring\n",
    "beautifulsoup\n",
    "comments\n",
    "\n",
    "# tags\n",
    "tags = soup.html\n",
    "type(tags)\n",
    "tag = soup.p\n",
    "tag\n",
    "tag = soup.h1\n",
    "tag = soup.a\n",
    "\n",
    "# navgablestring to find in string format\n",
    " \n",
    "tag =soup.p.string\n",
    "tag\n",
    "tag =soup.a.string\n",
    "tag\n",
    "\n",
    "\n",
    "# beautifulsoup obj\n",
    "soup.body\n",
    "soup.head\n",
    "soup.find(\"h1\")\n",
    "soup.find_all(\"h1\")\n",
    "soup.find(\"a\")\n",
    "\n",
    "#  comments \n",
    "com = soup.p.string\n",
    "\n",
    "or\n",
    "\n",
    "\n",
    "# finding elements in web page using python\n",
    "finding elements by class\n",
    "finding elements by id\n",
    "\n",
    "\n",
    "class_data = soup.find(\"div\",class_=\"app-container\")\n",
    "class_data.find_all(\"p\")\n",
    "class_data.find_all(\"a\")\n",
    "\n",
    "id_data = soup.find(\"div\",id=\"app-fronted\")\n",
    "\n",
    "\n",
    "# Extracting text from the tags in web page uing python\n",
    "\n",
    "lines = class_data.find_all(\"p\")\n",
    "for l in lines:\n",
    "    print(l.text)\n",
    "\n",
    "s1 = soup.find(\"p\",class_=\"card learn-card-outer mb-4\")\n",
    "s1\n",
    "s1.text\n",
    "\n",
    "\n",
    "# extract links\n",
    "for i in soup.find_all(\"a\"):\n",
    "    print(i.get(\"href\"))\n",
    "\n",
    "\n",
    "# extract image\n",
    "img = soup.find_all(\"img\")\n",
    "\n",
    "# or \n",
    "for i in img:\n",
    "    print(i.get(\"src\"))\n",
    "    print(type(i.get(\"src\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/HritikShukla02/amazon-scrapy/tree/main scrapy\n",
    "- https://quotes.toscrape.com/ https://youtu.be/OeWLWCWpty8?si=C8rreHNaUl9uEMIr\n",
    "- https://youtu.be/OkNQF7em6Jo?si=e5-Rv0x7a7nioLqd  Python Data Analysis Tutorial: Build Web Scraping Project  *****\n",
    "- Web scraping images using Python and Beautiful Soup and Selenium  https://youtu.be/aBK5igxppEE?si=enWm0k_I-dZHjPO4\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API CALL \n",
    "\n",
    "https://pokeapi.co/#google_vignette\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to connect to an API using Python\n",
    "import requests\n",
    "\n",
    "base_url = \"https://pokeapi.co/api/v2/\"\n",
    "\n",
    "def get_pokemon_info(name):\n",
    "    url = f\"{base_url}/pokemon/{name}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        pokemon_data = response.json()\n",
    "        return pokemon_data\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data {response.status_code}\")\n",
    "\n",
    "pokemon_name = \"pikachu\"\n",
    "pokemon_info = get_pokemon_info(pokemon_name)\n",
    "\n",
    "if pokemon_info:\n",
    "    print(f\"Name: {pokemon_info[\"name\"].capitalize()}\")\n",
    "    print(f\"Id: {pokemon_info[\"id\"]}\")\n",
    "    print(f\"Height: {pokemon_info[\"height\"]}\")\n",
    "    print(f\"Weight: {pokemon_info[\"weight\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra topics\n",
    "- https://youtu.be/row-SdNdHFE?feature=shared Best Practice to Make HTTP Request in FastAPI Application\n",
    "- How to Ignore SSL Certificate in Python Requests Library | ScrapingAnt\n",
    "https://youtu.be/V4ilCyjgyXA?feature=shared\n",
    "\n",
    "- https://scrapingant.com/blog/requests-ignore-ssl\n",
    "\n",
    "- Generate HTTP Requests with Python https://www.youtube.com/watch?v=9u_AfAjzJqI     https://github.com/LeonardoE95/yt-en/tree/main/src/2024-08-14-programming-python-http-requests\n",
    "\n",
    "\n",
    "- Python Requests Throwing SSLError: Causes and Solutions  https://youtu.be/LDDA_zu7DxI?feature=shared\n",
    "\n",
    "- Asyncio in Python - Full Tutorial https://youtu.be/Qb9s3UiMSTA?feature=shared  https://docs.python.org/3/library/asyncio.html\n",
    "- Handle Cookies in Python Requests https://youtu.be/o0UDBlo5tOM?feature=shared\n",
    "- How FastAPI Handles Requests Behind the Scenes  https://youtu.be/tGD3653BrZ8?feature=shared\n",
    "- Python API Tutorial For Beginners: A Code Along API Request Project  https://youtu.be/7vrayxFYY2w?feature=shared\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
