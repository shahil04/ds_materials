{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "statistics.mode(data)\n",
    "np.median(data)\n",
    "np.mean(data)\n",
    "np.percentile(data, [25, 50, 75, 95, 96, 97, 98, 99, 100])\n",
    "\n",
    "#outliers\n",
    "sns.boxplot(data)\n",
    "\n",
    "#measures of dispersion\n",
    "data\n",
    "np.var(data)\n",
    "np.std(data)\n",
    "statistics.variance(data) #sample variance (n-1)\n",
    "statistics.pvariance(data) #population variance\n",
    "\n",
    "import math\n",
    "math.sqrt(statistics.variance(data))\n",
    "\n",
    "#how to know the relationship between two features\n",
    "#correlation and covariance\n",
    "\n",
    "df\n",
    "corr = df[['total_bill', 'tip', 'size']].corr()\n",
    "corr\n",
    "df.corr(numeric_only = True)\n",
    "#if there is one unit increase in total bill then there is 67 percent chance that tip will increase in tip\n",
    "\n",
    "sns.heatmap(corr, annot = True)\n",
    "\n",
    "df.cov(numeric_only = True)\n",
    "sns.pairplot(df)\n",
    "sns.histplot(data, kde = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data create\n",
    "data1 = np.random.normal(0.5, 0.2, 1000)\n",
    "sns.distplot(data1)\n",
    "\n",
    "population = np.random.randint(10, 20, 50)\n",
    "\n",
    "sample1 = np.random.choice(population, 20)\n",
    "sample2 = np.random.choice(population, 20)\n",
    "sample3 = np.random.choice(population, 20)\n",
    "sample4 = np.random.choice(population, 20)\n",
    "\n",
    "\n",
    "mean_of_samples = []\n",
    "all_samples = [sample1, sample2, sample3, sample4]\n",
    "\n",
    "for sample in all_samples:\n",
    "    mean_of_samples.append(np.mean(sample))\n",
    "\n",
    "\n",
    "print(mean_of_samples)\n",
    "\n",
    "#central limit theorem >> for any distribution the sample mean will be approximated to normal distribution given sample size >= 30 and no of samples should be sufficiently large\n",
    "population = np.random.binomial(10, 0.5, 10000)\n",
    "sns.distplot(population)\n",
    "\n",
    "\n",
    "sample_size = 3000\n",
    "\n",
    "mean_of_samples = []\n",
    "\n",
    "for i in range(1, 10000):\n",
    "    sample = np.random.choice(population, size = sample_size)\n",
    "    mean_of_samples.append(np.mean(sample))\n",
    "# \n",
    "sns.distplot(mean_of_samples, kde = True, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hence mean=median=mode\n",
    "\n",
    "import scipy.stats as stats\n",
    "z_critical = stats.norm.ppf(q = 0.975)\n",
    "z_critical\n",
    "#zcritical for give alpha, ztable\n",
    "\n",
    "#t critical for give alpha, ttable\n",
    "stats.t.ppf(df = 24, q = 0.97)\n",
    "\n",
    "#margin of error\n",
    "margin_of_error = z_critical * np.std(mean_of_samples) / np.sqrt(20)\n",
    "\n",
    "margin_of_error\n",
    "\n",
    "mean_of_samples\n",
    "\n",
    "#confidence intervl\n",
    "np.mean(mean_of_samples)-margin_of_error, np.mean(mean_of_samples)+margin_of_error\n",
    "\n",
    "#Ztest\n",
    "population = np.random.randn(1000)\n",
    "population\n",
    "\n",
    "sns.distplot(population)\n",
    "\n",
    "np.mean(population), np.std(population)\n",
    "\n",
    "null_mean = 0.05 #claim\n",
    "#if pvalue <= 0.05 -- reject null hypothesis\n",
    "#if p value > 0.05 -- fail to reject null hypothesis\n",
    "\n",
    "\n",
    "from statsmodels.stats.weightstats import ztest\n",
    "\n",
    "zscore, pvalue = ztest(population, value = null_mean, alternative = 'larger') #here alternative means, mean is greater than null mean\n",
    "\n",
    "zscore\n",
    "pvalue\n",
    "\n",
    "if pvalue <= 0.05:\n",
    "    print(\"Reject the H0\")\n",
    "else:\n",
    "    print(\"fail to reject the H0\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A school calculated iq scores of 50 students, the average iq  turned out to be 100. the mean of population iq is 90 \n",
    "#and the std deviation 16. \\\n",
    "#state whether the claim by school \n",
    "#that IQ increases if student study more than avg of school with 5% of significance level\n",
    "\n",
    "#null hypothesis : mean_iq = 90\n",
    "#Alternate: mean_iq > 90\n",
    "\n",
    "\n",
    "#zcritical approach\n",
    "import scipy.stats as stats\n",
    "\n",
    "sample_mean = 100\n",
    "population_mean = 90\n",
    "population_std = 16\n",
    "sample_size = 50\n",
    "alpha = 0.05\n",
    "\n",
    "zscore = (sample_mean - population_mean)/(population_std/np.sqrt(sample_size))\n",
    "\n",
    "zscore\n",
    "#stats module in scipy>>zcritical\n",
    "#percent point function>>inverse of cdf\n",
    "zcritical = stats.norm.ppf(1-alpha)\n",
    "zcritical\n",
    "if zscore >= zcritical:\n",
    "    print(\"Reject the h0\")\n",
    "else:\n",
    "    print(\"fail to reject the h0\")\n",
    "\n",
    "#pvalue approach using scipy\n",
    "\n",
    "pvalue = 1-stats.norm.cdf(zscore)\n",
    "pvalue\n",
    "\n",
    "if pvalue <= alpha:\n",
    "    print(\"Reject the H0\")\n",
    "else:\n",
    "    print(\"Fail to reject H0\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### next topic #class imbalance\n",
    "\n",
    " >> one class has higeher percentage\n",
    " \n",
    "#upsampling, downsampling, smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(1) #for reproducibility\n",
    "\n",
    "no_samples = 1000\n",
    "class_0_ratio = 0.9\n",
    "no_class_0 = int(no_samples * class_0_ratio)\n",
    "no_class_1 = 100\n",
    "\n",
    "len(np.random.normal(0, 1, no_class_0))\n",
    "\n",
    "\n",
    "# create target\n",
    "class_0 = {'feature1': np.random.normal(0, 1, no_class_0),\n",
    "'feature2': np.random.normal(0, 1, no_class_0),\n",
    "'target': [0]*no_class_0}\n",
    "\n",
    "class_0 = pd.DataFrame(class_0)\n",
    "class_1 = pd.DataFrame({'feature1': np.random.normal(3, 1, no_class_1),\n",
    "'feature2': np.random.normal(3, 1, no_class_1),\n",
    "'target': [1]*no_class_1})\n",
    "\n",
    "df = pd.concat([class_0, class_1]).reset_index(drop = True)\n",
    "\n",
    "df.target.value_counts(normalize = True)   # show in percentage use normalize true\n",
    "\n",
    "df_minority = df[df.target == 1]\n",
    "df_majority = df[df.target == 0]\n",
    "\n",
    "df_minority\n",
    "\n",
    "\n",
    "#oversampling>>upsampling >>increasing the minority to majority no\n",
    "from sklearn.utils import resample\n",
    "df_minority_upsampled = resample(df_minority, replace=True, n_samples = len(df_majority), random_state =1)\n",
    "df_minority_upsampled.shape\n",
    "\n",
    "df_minority_upsampled.duplicated().sum()\n",
    "\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "df_upsampled.shape\n",
    "\n",
    "\n",
    "df_upsampled.target.value_counts()\n",
    "\n",
    "\n",
    "#downsmapling\n",
    "df_minority \n",
    "df_majority_downsampled = resample(df_majority, replace=False, n_samples = len(df_minority), random_state =1)\n",
    "\n",
    "df_downsampled = pd.concat([df_minority, df_majority_downsampled])\n",
    "\n",
    "\n",
    "#SMOTE>> synthetic minority oversampling technique\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples = 1000, n_redundant = 0, n_features=2, n_clusters_per_class = 1, weights = [0.90], random_state = 1)\n",
    "len(y[y==0])\n",
    "len(y[y==1])\n",
    "\n",
    "df1 = pd.DataFrame(X, columns = ['f1', 'f2'])\n",
    "df2 = pd.DataFrame(y, columns = ['target'])\n",
    "df_final = pd.concat([df1, df2], axis = 1)\n",
    "\n",
    "df_final.target.value_counts()\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(df_final['f1'], df_final['f2'], c = df_final['target'])\n",
    "\n",
    "# !pip install imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(df_final[['f1', 'f2']], df_final['target'])\n",
    "df1 = pd.DataFrame(X, columns = ['f1', 'f2'])\n",
    "df2 = pd.DataFrame(y, columns = ['target'])\n",
    "oversample = pd.concat([df1, df2], axis = 1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(oversample['f1'], oversample['f2'], c = oversample['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process in machine learning\n",
    "# 1. data collection\n",
    "    \n",
    "# 2. data preprocessing\n",
    "# 3. model selection\n",
    "# 4. model training\n",
    "# 5. model evaluation\n",
    "# 6. model deployment\n",
    "\n",
    "# after job  is done, we need to save the model so that we can use it later repitation\n",
    "\n",
    "# 7. model maintenance\n",
    "# 8. model improvement\n",
    "# 9. model explanation\n",
    "# 10. model monitoring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Stats \n",
    "2. Maths (linear algebra, matrix, slop/gradient descent)\n",
    "3. ML ,dL,AI, DS,\n",
    "4. EDA  \n",
    "    a. CRISP-DM\n",
    "    b. Data collection\n",
    "    c. Data Preparation\n",
    "        i. Missing values\n",
    "        ii. Duplicate values\n",
    "        iii. Outliers\n",
    "            a)Dropping\n",
    "            b)Capping\n",
    "            c)Replace with mean and median\n",
    "            d)Scaling Transforming (data normalizations)\n",
    "\n",
    "        iv.Feature engineering\n",
    "            1.Select relevent feature(filter,embedd,wrapper)\n",
    "            2.Create new feature\n",
    "            3.Modify existing feature\n",
    "            4.Feature scaling(normalization, standarizations) or transformation\n",
    "\n",
    "        v. Data Encoding\n",
    "            1.OHE\n",
    "            2.Label\n",
    "            3.Target Guided ordinal encoding\n",
    "        \n",
    "D)Data Analysis\n",
    "    a)Uni variant \n",
    "        i.Count,histogram,mean,median,types,pie,\n",
    "    b)Bi-variant\n",
    "        i.Count of 2features,bar,\n",
    "    c)Multi-variant\n",
    "        i.Bar, box, corr, heatmap, (outlier,iqr)\n",
    "d)S\n",
    "E)Model selecttion\n",
    "F)Model building\n",
    "G)Model evaluations\n",
    "H)\n",
    "\n",
    "\n",
    "\n",
    "1.ML\n",
    "2.Types of ml\n",
    "i.SL -> \n",
    "a.Classification\n",
    "b.Regression \n",
    "ii.USL\n",
    "1.cluster\n",
    "iii.SSl\n",
    "iv.RL\n",
    "3.Termonologies\n",
    "i.Overfitting\n",
    "ii.Underfitting\n",
    "iii.Bia\n",
    "iv.Variance\n",
    "v.Bias-variance trade-off\n",
    "vi.Label ,unlabel data\n",
    "vii.Cost funtion\n",
    "viii.Confusion matrix\n",
    "ix.Sampling (upsampling,downsampling, SMOTE)\n",
    "x.Data interpolation\n",
    "\n",
    "1.ML Supervise learning\n",
    "a)Regression\n",
    "i.Simple linear regression (1 x, y)\n",
    "ii.Multi LR (n X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Stats \n",
    "# 2. Maths (linear algebra, matrix, slop/gradient descent)\n",
    "# 3. ML ,dL,AI, DS,\n",
    "# 4. EDA  \n",
    "#     a. CRISP-DM\n",
    "#     b. Data collection\n",
    "#     c. Data Preparation\n",
    "#         i. Missing values\n",
    "#         ii. Duplicate values\n",
    "#         iii. Outliers\n",
    "#             a)Dropping\n",
    "#             b)Capping\n",
    "#             c)Replace with mean and median\n",
    "#             d)Scaling Transforming (data normalizations)\n",
    "\n",
    "#         iv.Feature engineering\n",
    "#             1.Select relevent feature(filter,embedd,wrapper)\n",
    "#             2.Create new feature\n",
    "#             3.Modify existing feature\n",
    "#             4.Feature scaling(normalization, standarizations) or transformation\n",
    "\n",
    "#         v. Data Encoding\n",
    "#             1.OHE\n",
    "#             2.Label\n",
    "#             3.Target Guided ordinal encoding\n",
    "        \n",
    "# D)Data Analysis\n",
    "#     a)Uni variant \n",
    "#         i.Count,histogram,mean,median,types,pie,\n",
    "#     b)Bi-variant\n",
    "#         i.Count of 2features,bar,\n",
    "#     c)Multi-variant\n",
    "#         i.Bar, box, corr, heatmap, (outlier,iqr)\n",
    "# d)S\n",
    "# E)Model selecttion\n",
    "# F)Model building\n",
    "# G)Model evaluations\n",
    "# H)\n",
    "\n",
    "\n",
    "\n",
    "# 1.ML\n",
    "# 2.Types of ml\n",
    "# i.SL -> \n",
    "# a.Classification\n",
    "# b.Regression \n",
    "# ii.USL\n",
    "# 1.cluster\n",
    "# iii.SSl\n",
    "# iv.RL\n",
    "# 3.Termonologies\n",
    "# i.Overfitting\n",
    "# ii.Underfitting\n",
    "# iii.Bia\n",
    "# iv.Variance\n",
    "# v.Bias-variance trade-off\n",
    "# vi.Label ,unlabel data\n",
    "# vii.Cost funtion\n",
    "# viii.Confusion matrix\n",
    "# ix.Sampling (upsampling,downsampling, SMOTE)\n",
    "# x.Data interpolation\n",
    "\n",
    "# 1.ML Supervise learning\n",
    "# a)Regression\n",
    "# i.Simple linear regression (1 x, y)\n",
    "# ii.Multi LR (n X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra \n",
    "C:\\Users\\hp\\Documents\\ds\\ds_materials\\stats_notes\\ps_stats\\ml2\\20_21_april_class_2_ml\\class imbalance and redressal methodology.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Car_sales.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic info \n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## handling missing values\n",
    "df.isna().sum()  # show the null values\n",
    "df.dtypes\n",
    "\n",
    "\n",
    "df[\"Fuel_capacity\"].mean()\n",
    "# replace the null with mean or median or anyvalue\n",
    "df[\"Fuel_capacity\"].fillna(df[\"Fuel_capacity\"].mean())\n",
    "\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df.isna().sum()\n",
    "\n",
    "df.duplicated().sum() # check duplicate\n",
    "\n",
    "# drop duplicate\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OUTLIERS CHECK AND TREAT\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns   \n",
    "\n",
    "df.columns\n",
    "\n",
    "\n",
    "\n",
    "#To check outliers >> distplot, boxplot\n",
    "\n",
    "plt.figure(figsize = (12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['Price_in_thousands'], kde = True)\n",
    "plt.title(\"Dist plot\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(data = df, x  = 'Price_in_thousands')\n",
    "plt.title(\"Box plot\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the outlier\n",
    "#capping the outlier\n",
    "#replace with mean and median\n",
    "#Scaling and transformation\n",
    "\n",
    "Q1 = df['Price_in_thousands'].quantile(0.25)\n",
    "Q3 = df['Price_in_thousands'].quantile(0.75)\n",
    "print(Q1)\n",
    "print(Q3)\n",
    "\n",
    "IQR = Q3-Q1\n",
    "print(IQR)\n",
    "\n",
    "lower_fence = Q1-1.5*IQR\n",
    "upper_fence = Q3+1.5*IQR\n",
    "\n",
    "\n",
    "#dropping the outlier\n",
    "df_filtered = df[(df.Price_in_thousands >= lower_fence) & (df.Price_in_thousands <= upper_fence)] \n",
    "\n",
    "\n",
    "#To check outliers >> distplot, boxplot\n",
    "\n",
    "plt.figure(figsize = (12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df_filtered['Price_in_thousands'], kde = True)\n",
    "plt.title(\"Dist plot\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(data = df_filtered, x  = 'Price_in_thousands')\n",
    "plt.title(\"Box plot\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#five point summary\n",
    "np.quantile(salary, [0, 0.25, .50, .75, 1])\n",
    "\n",
    "#To check outliers >> distplot, boxplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data interpolation  optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#data interpolation >>  is a process of estimating data in a range, getting unknown values from known values\n",
    "1. Linear Interpolation\n",
    "2. cubic interpolation\n",
    "3. Polynomial interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([1, 3, 5, 7, 9])\n",
    "\n",
    "plt.scatter(x, y)\n",
    "\n",
    "\n",
    "#linear interpolation\n",
    "\n",
    "x_new = np.linspace(1, 5, 10)\n",
    "x_new\n",
    "\n",
    "y_interp = np.interp(x_new, x, y)\n",
    "plt.scatter(x_new, y_interp)\n",
    "\n",
    "# ========================\n",
    "#cubic interpolation\n",
    "\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([1, 8, 27, 64, 125])\n",
    "plt.scatter(x, y)\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "f = interp1d(x, y, kind = 'cubic')\n",
    "\n",
    "x_new = np.linspace(1, 5, 10)\n",
    "y_interp = f(x_new)\n",
    "\n",
    "y_interp\n",
    "\n",
    "plt.scatter(x_new, y_interp)\n",
    "# =================================\n",
    "#polynomial interpolation\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([1, 4, 9, 1, 25])\n",
    "\n",
    "\n",
    "plt.scatter(x, y)\n",
    "\n",
    "p = np.polyfit(x, y, 2)\n",
    "x_new = np.linspace(1, 5, 10)\n",
    "y_interp = np.polyval(p, x_new)\n",
    "\n",
    "plt.scatter(x_new, y_interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
