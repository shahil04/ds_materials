{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Pandas\n",
    "\n",
    "Pandas is a Python library used for data manipulation and analysis. Pandas provides a convenient way to analyze and clean data.\n",
    "\n",
    "The Pandas library introduces two new data structures to Python - Series and DataFrame, both of which are built on top of NumPy.\n",
    "\n",
    "### What is Pandas Used for?\n",
    "\n",
    "Pandas is a powerful library generally used for:\n",
    "\n",
    "- Data Cleaning\n",
    "- Data Transformation\n",
    "- Data Analysis\n",
    "- Machine Learning\n",
    "- Data Visualization\n",
    "\n",
    "### Why Use Pandas?\n",
    "\n",
    "Some of the reasons why we should use Pandas are as follows:\n",
    "\n",
    "1. **Handle Large Data Efficiently**\n",
    "\n",
    "   Pandas is designed for handling large datasets. It provides powerful tools that simplify tasks like data filtering, transforming, and merging.\n",
    "\n",
    "   It also provides built-in functions to work with formats like CSV, JSON, TXT, Excel, and SQL databases.\n",
    "\n",
    "2. **Tabular Data Representation**\n",
    "\n",
    "   Pandas DataFrames, the primary data structure of Pandas, handle data in tabular format. This allows easy indexing, selecting, replacing, and slicing of data.\n",
    "\n",
    "3. **Data Cleaning and Preprocessing**\n",
    "\n",
    "   Data cleaning and preprocessing are essential steps in the data analysis pipeline, and Pandas provides powerful tools to facilitate these tasks. It has methods for handling missing values, removing duplicates, handling outliers, data normalization, etc.\n",
    "\n",
    "4. **Time Series Functionality**\n",
    "\n",
    "   Pandas contains an extensive set of tools for working with dates, times, and time-indexed data as it was initially developed for financial modeling.\n",
    "\n",
    "5. **Free and Open-Source**\n",
    "\n",
    "   Pandas follows the same principles as Python, allowing you to use and distribute Pandas for free, even for commercial use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  1. Introduction to Pandas\n",
    "\n",
    "* What is Pandas?\n",
    "* Why use it in data science?\n",
    "\n",
    "## ðŸ§ª 2. Installing and Importing Pandas\n",
    "\n",
    "* `pip install pandas`\n",
    "* `import pandas as pd`\n",
    "\n",
    "## ðŸ“Š 3. Pandas Data Structures\n",
    "\n",
    "* Series\n",
    "* DataFrame\n",
    "\n",
    "## ðŸ“ 4. Reading and Writing Files\n",
    "\n",
    "* CSV, Excel, JSON\n",
    "* `pd.read_csv`, `to_csv`, etc.\n",
    "\n",
    "## ðŸ” 5. Exploring Data\n",
    "\n",
    "* `head()`, `tail()`, `info()`, `describe()`\n",
    "\n",
    "## ðŸ”Ž 6. Indexing and Selecting Data\n",
    "\n",
    "* `loc`, `iloc`, `at`, `iat`\n",
    "* Conditional filtering\n",
    "\n",
    "## ðŸ§¹ 7. Data Cleaning\n",
    "\n",
    "* Handling missing values\n",
    "* Changing data types\n",
    "* Renaming columns\n",
    "\n",
    "## ðŸ” 8. Applying Functions\n",
    "\n",
    "* `apply()`, `map()`, `lambda` with Pandas\n",
    "\n",
    "## ðŸ“ˆ 9. GroupBy and Aggregations\n",
    "\n",
    "* `groupby()`, `agg()`, `pivot_table()`\n",
    "\n",
    "## ðŸ”— 10. Merging and Joining\n",
    "\n",
    "* `merge()`, `concat()`, `join()`\n",
    "\n",
    "## ðŸ•³ 11. Handling Missing Data\n",
    "\n",
    "* `isnull()`, `dropna()`, `fillna()`\n",
    "\n",
    "## ðŸ’¼ 12. Real-World Business Example\n",
    "\n",
    "* Sales dataset or Superstore analysis\n",
    "\n",
    "## ðŸ§  13. Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Pandas\n",
    "\n",
    "To install pandas, you need Python and PIP installed on your system. If you have Python and PIP installed already, you can install pandas by entering the following command in the terminal:\n",
    "\n",
    "```bash\n",
    "pip install pandas\n",
    "```\n",
    "\n",
    "If the installation completes without any errors, Pandas is now successfully installed on your system. You can start using it in your Python projects by importing the Pandas library.\n",
    "\n",
    "### Import Pandas in Python\n",
    "\n",
    "We can import Pandas in Python using the import statement:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "pandas.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pandas>>data manipulation and data wrangling\n",
    "# create Series, DataFrame ,indexing, columns, data types, assign, create new columns\n",
    "\n",
    "# data cleaning\n",
    "# drop columns,  drop rows, fill missing values, handle outliers, remove duplicates\n",
    "\n",
    "\n",
    "# data transformation\n",
    "# pivot, melt, groupby, sort, sortby, rank, quantile, shift,\n",
    "# data merging\n",
    "# join, merge, concat, append\n",
    "# data analysis\n",
    "# summary statistics, descriptive statistics, correlation, regression, time series analysis\n",
    "# data visualization\n",
    "# plot, scatter plot, bar plot, histogram, box plot, violin plot, heatmap\n",
    "# data export\n",
    "# to_csv, to_excel, to_json, to_pickle, to_sql\n",
    "\n",
    "\n",
    "\n",
    "# A Pandas Series is a one-dimensional labeled array-like object that can hold data of any type.\n",
    "### Labels\n",
    "\n",
    "# The labels in the Pandas Series are \n",
    "# index numbers by default. Like in DataFrame and array, the index number in Series starts from 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series and dataframe (Indexing & Slicing)\n",
    "##### Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    534\n",
       "3     34\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creaate the Series using list\n",
    "import pandas as pd\n",
    "li = [34,3,534,34,23]\n",
    "se = pd.Series(li)\n",
    "se[2]\n",
    "se[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Series and specify labels\n",
    "se2 = pd.Series([12,32,243,45] , index=['a','b','c','d'])\n",
    "se2[\"c\"]\n",
    "# reasign \n",
    "se2[\"c\"] = 24\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Name'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'Name'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m df\n\u001b[32m      6\u001b[39m pd.DataFrame({\u001b[33m\"\u001b[39m\u001b[33mx\u001b[39m\u001b[33m\"\u001b[39m:[\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m,\u001b[32m3\u001b[39m],\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m:[\u001b[32m4\u001b[39m,\u001b[32m5\u001b[39m,\u001b[32m6\u001b[39m],\u001b[33m\"\u001b[39m\u001b[33mz\u001b[39m\u001b[33m\"\u001b[39m:[\u001b[32m7\u001b[39m,\u001b[32m8\u001b[39m,\u001b[32m9\u001b[39m]})\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m s = pd.Series(\u001b[38;5;28mlist\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mName\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[32m2\u001b[39m:\u001b[32m5\u001b[39m]), index = [\u001b[33m'\u001b[39m\u001b[33ma\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mb\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mc\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      9\u001b[39m s\n\u001b[32m     10\u001b[39m s1 = pd.Series(\u001b[38;5;28mlist\u001b[39m(df[\u001b[33m'\u001b[39m\u001b[33mName\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m5\u001b[39m:\u001b[32m8\u001b[39m]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\.conda\\envs\\aienv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'Name'"
     ]
    }
   ],
   "source": [
    "\n",
    "#  data frame\n",
    "pd.DataFrame([[1,2,3],[4,5,6],[7,8,9]])\n",
    "\n",
    "df = pd.DataFrame([[1,2,3],[4,5,6],[7,8,9]], index=['a','b','c'], columns=['x','y','z'])\n",
    "df\n",
    "pd.DataFrame({\"x\":[1,2,3],\"y\":[4,5,6],\"z\":[7,8,9]})\n",
    "\n",
    "s = pd.Series(list(df['Name'][2:5]), index = ['a', 'b', 'c'])\n",
    "s\n",
    "s1 = pd.Series(list(df['Name'][5:8]))\n",
    "s1\n",
    "s+s1 #series or dataframe doesnt work with +\n",
    "s.append(s1)\n",
    "type(df)\n",
    "\n",
    "#data structure>> series, dataframe\n",
    "#series> 1 dimensional in nature\n",
    "#dataframe> 2 dimensional in nature , multiple series constitute to form a dataframe\n",
    "\n",
    "df.dtypes\n",
    "df.shape\n",
    "df\n",
    "pd.Series([2, 3, 4], index = [100, \"ajay\", 2])\n",
    "d = pd.DataFrame(pd.Series([2, 3, 4], index = [100, \"ajay\", 2]))\n",
    "d[1] = \"Anuj\"\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Data from Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version https://git-lfs.github.com/spec/v1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oid sha256:567be8c7e70a068367cdedd0728bf4f98b6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>size 19104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          version https://git-lfs.github.com/spec/v1\n",
       "0  oid sha256:567be8c7e70a068367cdedd0728bf4f98b6...\n",
       "1                                         size 19104"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# load Diffrent Files\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Diffrent Files\n",
    "df = pd.read_csv(\"services.csv\")\n",
    "ex = pd.read_excel(\"basic_data_excel.xlsx\")\n",
    "ex\n",
    "df\n",
    "df3 = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
    "df3\n",
    "df3.dtypes\n",
    "df3.shape\n",
    "df3.info()\n",
    "df3[['Sex']]\n",
    "\n",
    "df3.columns\n",
    "df4 = df3[['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
    "       'Parch', 'Ticket', 'Fare']]\n",
    "df4.to_csv(\"test.csv\", index = False)\n",
    "\n",
    "# pip install lxml\n",
    "import lxml\n",
    "url_df = pd.read_html(\"https://www.basketball-reference.com/leagues/NBA_2015_totals.html\")\n",
    "type(url_df)\n",
    "len(url_df)\n",
    "df4 = url_df[0]\n",
    "df4\n",
    "df4.head()\n",
    "df4.shape\n",
    "df4.info()\n",
    "df4.to_csv(\"players.csv\", index=False)\n",
    "\n",
    "json =\"https://api.github.com/repos/pandas-dev/pandas/issues\"\n",
    "\n",
    "for i in range(len(df)):\n",
    "    print(df[i]['user']['node_id'])\n",
    "\n",
    "df = pd.DataFrame(df, columns = ['user', 'timeline_url'])\n",
    "\n",
    "# df.to_csv('json_info.csv')\n",
    "#Solving the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'PENELOPE', 'GUINESS', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(2, 'NICK', 'WAHLBERG', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(3, 'ED', 'CHASE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(4, 'JENNIFER', 'DAVIS', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(5, 'JOHNNY', 'LOLLOBRIGIDA', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(6, 'BETTE', 'NICHOLSON', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(7, 'GRACE', 'MOSTEL', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(8, 'MATTHEW', 'JOHANSSON', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(9, 'JOE', 'SWANK', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(10, 'CHRISTIAN', 'GABLE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(11, 'ZERO', 'CAGE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(12, 'KARL', 'BERRY', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(13, 'UMA', 'WOOD', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(14, 'VIVIEN', 'BERGEN', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(15, 'CUBA', 'OLIVIER', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(16, 'FRED', 'COSTNER', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(17, 'HELEN', 'VOIGHT', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(18, 'DAN', 'TORN', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(19, 'BOB', 'FAWCETT', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(20, 'LUCILLE', 'TRACY', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(21, 'KIRSTEN', 'PALTROW', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(22, 'ELVIS', 'MARX', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(23, 'SANDRA', 'KILMER', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(24, 'CAMERON', 'STREEP', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(25, 'KEVIN', 'BLOOM', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(26, 'RIP', 'CRAWFORD', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(27, 'JULIA', 'MCQUEEN', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(28, 'WOODY', 'HOFFMAN', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(29, 'ALEC', 'WAYNE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(30, 'SANDRA', 'PECK', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(31, 'SISSY', 'SOBIESKI', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(32, 'TIM', 'HACKMAN', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(33, 'MILLA', 'PECK', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(34, 'AUDREY', 'OLIVIER', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(35, 'JUDY', 'DEAN', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(36, 'BURT', 'DUKAKIS', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(37, 'VAL', 'BOLGER', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(38, 'TOM', 'MCKELLEN', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(39, 'GOLDIE', 'BRODY', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(40, 'JOHNNY', 'CAGE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(41, 'JODIE', 'DEGENERES', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(42, 'TOM', 'MIRANDA', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(43, 'KIRK', 'JOVOVICH', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(44, 'NICK', 'STALLONE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(45, 'REESE', 'KILMER', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(46, 'PARKER', 'GOLDBERG', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(47, 'JULIA', 'BARRYMORE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(48, 'FRANCES', 'DAY-LEWIS', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(49, 'ANNE', 'CRONYN', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(50, 'NATALIE', 'HOPKINS', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(51, 'GARY', 'PHOENIX', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(52, 'CARMEN', 'HUNT', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(53, 'MENA', 'TEMPLE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(54, 'PENELOPE', 'PINKETT', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(55, 'FAY', 'KILMER', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(56, 'DAN', 'HARRIS', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(57, 'JUDE', 'CRUISE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(58, 'CHRISTIAN', 'AKROYD', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(59, 'DUSTIN', 'TAUTOU', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(60, 'HENRY', 'BERRY', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(61, 'CHRISTIAN', 'NEESON', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(62, 'JAYNE', 'NEESON', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(63, 'CAMERON', 'WRAY', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(64, 'RAY', 'JOHANSSON', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(65, 'ANGELA', 'HUDSON', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(66, 'MARY', 'TANDY', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(67, 'JESSICA', 'BAILEY', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(68, 'RIP', 'WINSLET', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(69, 'KENNETH', 'PALTROW', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(70, 'MICHELLE', 'MCCONAUGHEY', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(71, 'ADAM', 'GRANT', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(72, 'SEAN', 'WILLIAMS', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(73, 'GARY', 'PENN', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(74, 'MILLA', 'KEITEL', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(75, 'BURT', 'POSEY', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(76, 'ANGELINA', 'ASTAIRE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(77, 'CARY', 'MCCONAUGHEY', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(78, 'GROUCHO', 'SINATRA', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(79, 'MAE', 'HOFFMAN', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(80, 'RALPH', 'CRUZ', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(81, 'SCARLETT', 'DAMON', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(82, 'WOODY', 'JOLIE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(83, 'BEN', 'WILLIS', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(84, 'JAMES', 'PITT', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(85, 'MINNIE', 'ZELLWEGER', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(86, 'GREG', 'CHAPLIN', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(87, 'SPENCER', 'PECK', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(88, 'KENNETH', 'PESCI', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(89, 'CHARLIZE', 'DENCH', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(90, 'SEAN', 'GUINESS', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(91, 'CHRISTOPHER', 'BERRY', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(92, 'KIRSTEN', 'AKROYD', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(93, 'ELLEN', 'PRESLEY', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(94, 'KENNETH', 'TORN', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(95, 'DARYL', 'WAHLBERG', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(96, 'GENE', 'WILLIS', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(97, 'MEG', 'HAWKE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(98, 'CHRIS', 'BRIDGES', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(99, 'JIM', 'MOSTEL', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(100, 'SPENCER', 'DEPP', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(101, 'SUSAN', 'DAVIS', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(102, 'WALTER', 'TORN', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(103, 'MATTHEW', 'LEIGH', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(104, 'PENELOPE', 'CRONYN', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(105, 'SIDNEY', 'CROWE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(106, 'GROUCHO', 'DUNST', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(107, 'GINA', 'DEGENERES', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(108, 'WARREN', 'NOLTE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(109, 'SYLVESTER', 'DERN', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(110, 'SUSAN', 'DAVIS', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(111, 'CAMERON', 'ZELLWEGER', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(112, 'RUSSELL', 'BACALL', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(113, 'MORGAN', 'HOPKINS', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(114, 'MORGAN', 'MCDORMAND', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(115, 'HARRISON', 'BALE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(116, 'DAN', 'STREEP', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(117, 'RENEE', 'TRACY', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(118, 'CUBA', 'ALLEN', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(119, 'WARREN', 'JACKMAN', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(120, 'PENELOPE', 'MONROE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(121, 'LIZA', 'BERGMAN', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(122, 'SALMA', 'NOLTE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(123, 'JULIANNE', 'DENCH', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(124, 'SCARLETT', 'BENING', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(125, 'ALBERT', 'NOLTE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(126, 'FRANCES', 'TOMEI', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(127, 'KEVIN', 'GARLAND', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(128, 'CATE', 'MCQUEEN', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(129, 'DARYL', 'CRAWFORD', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(130, 'GRETA', 'KEITEL', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(131, 'JANE', 'JACKMAN', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(132, 'ADAM', 'HOPPER', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(133, 'RICHARD', 'PENN', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(134, 'GENE', 'HOPKINS', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(135, 'RITA', 'REYNOLDS', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(136, 'ED', 'MANSFIELD', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(137, 'MORGAN', 'WILLIAMS', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(138, 'LUCILLE', 'DEE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(139, 'EWAN', 'GOODING', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(140, 'WHOOPI', 'HURT', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(141, 'CATE', 'HARRIS', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(142, 'JADA', 'RYDER', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(143, 'RIVER', 'DEAN', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(144, 'ANGELA', 'WITHERSPOON', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(145, 'KIM', 'ALLEN', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(146, 'ALBERT', 'JOHANSSON', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(147, 'FAY', 'WINSLET', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(148, 'EMILY', 'DEE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(149, 'RUSSELL', 'TEMPLE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(150, 'JAYNE', 'NOLTE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(151, 'GEOFFREY', 'HESTON', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(152, 'BEN', 'HARRIS', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(153, 'MINNIE', 'KILMER', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(154, 'MERYL', 'GIBSON', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(155, 'IAN', 'TANDY', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(156, 'FAY', 'WOOD', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(157, 'GRETA', 'MALDEN', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(158, 'VIVIEN', 'BASINGER', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(159, 'LAURA', 'BRODY', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(160, 'CHRIS', 'DEPP', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(161, 'HARVEY', 'HOPE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(162, 'OPRAH', 'KILMER', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(163, 'CHRISTOPHER', 'WEST', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(164, 'HUMPHREY', 'WILLIS', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(165, 'AL', 'GARLAND', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(166, 'NICK', 'DEGENERES', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(167, 'LAURENCE', 'BULLOCK', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(168, 'WILL', 'WILSON', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(169, 'KENNETH', 'HOFFMAN', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(170, 'MENA', 'HOPPER', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(171, 'OLYMPIA', 'PFEIFFER', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(172, 'GROUCHO', 'WILLIAMS', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(173, 'ALAN', 'DREYFUSS', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(174, 'MICHAEL', 'BENING', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(175, 'WILLIAM', 'HACKMAN', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(176, 'JON', 'CHASE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(177, 'GENE', 'MCKELLEN', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(178, 'LISA', 'MONROE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(179, 'ED', 'GUINESS', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(180, 'JEFF', 'SILVERSTONE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(181, 'MATTHEW', 'CARREY', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(182, 'DEBBIE', 'AKROYD', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(183, 'RUSSELL', 'CLOSE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(184, 'HUMPHREY', 'GARLAND', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(185, 'MICHAEL', 'BOLGER', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(186, 'JULIA', 'ZELLWEGER', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(187, 'RENEE', 'BALL', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(188, 'ROCK', 'DUKAKIS', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(189, 'CUBA', 'BIRCH', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(190, 'AUDREY', 'BAILEY', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(191, 'GREGORY', 'GOODING', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(192, 'JOHN', 'SUVARI', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(193, 'BURT', 'TEMPLE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(194, 'MERYL', 'ALLEN', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(195, 'JAYNE', 'SILVERSTONE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(196, 'BELA', 'WALKEN', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(197, 'REESE', 'WEST', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(198, 'MARY', 'KEITEL', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(199, 'JULIA', 'FAWCETT', datetime.datetime(2006, 2, 15, 4, 34, 33))\n",
      "(200, 'THORA', 'TEMPLE', datetime.datetime(2006, 2, 15, 4, 34, 33))\n"
     ]
    }
   ],
   "source": [
    "# mysql connector\n",
    "import mysql.connector  \n",
    "#Create the connection object   \n",
    "myconn = mysql.connector.connect(host = \"127.0.0.1\", user = \"root\",passwd = \"root\" ,database = \"mavenmovies\")  \n",
    "  \n",
    "#creating the cursor object  \n",
    "cur = myconn.cursor()   \n",
    "try:  \n",
    "    #Reading the Employee data      \n",
    "    cur.execute(\"select * from Actor\")  \n",
    "  \n",
    "    #fetching the rows from the cursor object  \n",
    "    result = cur.fetchall()  \n",
    "    #printing the result  \n",
    "  \n",
    "    for x in result:  \n",
    "        print(x);  \n",
    "except:  \n",
    "    myconn.rollback()  \n",
    "  \n",
    "myconn.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2d slicing \n",
    "df2[[\"a\",\"c\"]][1:]   #but not using slicing in 2d\n",
    "\n",
    "# loc \n",
    "# df.loc[rows , columns]\n",
    "df2.loc[\"y\"]\n",
    "\n",
    "# df.iloc[rows , columns]\n",
    "df2.iloc[1: ,1:3]  # index base slicing \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================== \n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
    "df\n",
    "df.columns\n",
    "df.head()\n",
    "df.tail()\n",
    "df.dtypes\n",
    "df.size\n",
    "df.sample(1)\n",
    "df.columns\n",
    "list(df.columns)\n",
    "df.info()\n",
    "df.dtypes\n",
    "df.shape\n",
    "df.describe() #numerical data\n",
    "\n",
    "type(df['id'])\n",
    "\n",
    "# df.numeric.describe()\n",
    "df.describe()\n",
    "df[['PassengerId', 'Survived', 'Pclass']]\n",
    "df.describe(include = 'object')\n",
    "df.describe(include = 'all')\n",
    "df.astype('object').describe()\n",
    "\n",
    "df.dtypes == 'object'\n",
    "df.dtypes\n",
    "df.dtypes[df.dtypes == 'object'].index\n",
    "df[df.dtypes[df.dtypes == 'object'].index]\n",
    "df[df.columns[df.dtypes == 'object']]\n",
    "\n",
    "df[df.dtypes[df.dtypes != 'object'].index].describe()\n",
    "df[df.columns[df.dtypes != 'object']].describe()\n",
    "df[df.dtypes[df.dtypes == 'object'].index].describe()\n",
    "\n",
    "\n",
    "# df.numeric.describe()\n",
    "df.describe()\n",
    "df\n",
    "# Slicing : indexing  by label or position\n",
    "df[10:100:5]\n",
    "pd.Categorical(df['Pclass'])\n",
    "pd.Categorical(df['Cabin'])\n",
    "df['Cabin'].unique()\n",
    "df['Cabin'].nunique()\n",
    "df['Cabin'].value_counts()\n",
    "df\n",
    "df.head()\n",
    "# deep copy shallow copy\n",
    "df3= df\n",
    "df2 = df.copy()   # copy the dataframe\n",
    "\n",
    "# Slicing name basis\n",
    "df4.loc[0:4]   # row  slicing\n",
    "df4.loc[0:4,\"Name\":\"Fare\"] # rows and  columns slice\n",
    "df4.loc[0:4,[\"Name\",\"Fare\"]]\n",
    "\n",
    "# df.iloc[rows , columns]\n",
    "df2.iloc[1: ,1:3]  # index base slicing \n",
    "\n",
    "# Slicing index\n",
    "df4.iloc[0:4]   # row  slicing\n",
    "df4.iloc[0:4,2:6]\n",
    "df4.iloc[0:4,[2,5]]\n",
    "\n",
    "# create new cols and add into df4\n",
    "df4[\"new col\"] =0\n",
    "\n",
    "#Access df rows #implicit index>internal/integer index and explicit index/named>>define\n",
    "df[0:100]\n",
    "df.iloc[0:2] #start from 0 and go to 1\n",
    "df.loc[0:2] #give me the rows whose name is 0, 1, 2\n",
    "\n",
    "#loc will go with named indexes, iloc will go with inbuilt index\n",
    "df.iloc[0:2, ['Name', 'Sex', 'Age']] #it will throw an error, why?\n",
    "df.loc[0:2, ['Name', 'Sex', 'Age']]\n",
    "\n",
    "df\n",
    "df.iloc[0:2, 3:6]\n",
    "list(df['Name'][2:5])\n",
    "\n",
    "\n",
    "df1[['name']].dropna(axis = 1) #for one column\n",
    "df1.fillna(\"missing_value_here>lalalalalala\", inplace = True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace value example\n",
    "df.loc[7, 'Duration'] = 45\n",
    "\n",
    "# If the value is higher than 120, set it to 120:\n",
    "\n",
    "for x in df.index:\n",
    "  if df.loc[x, \"Duration\"] > 120:\n",
    "    df.loc[x, \"Duration\"] = 120\n",
    "# Delete rows where \"Duration\" is higher than 120:\n",
    "\n",
    "for x in df.index:\n",
    "  if df.loc[x, \"Duration\"] > 120:\n",
    "    df.drop(x, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "df4.duplicated()\n",
    "df4.duplicated().sum()\n",
    "df4.drop_duplicates()\n",
    "\n",
    "df4.isna()\n",
    "df4.isna().sum()\n",
    "df4.drop(columns=[\"Unnamed: 0\",\"Cabin\"] ,axis=1) \n",
    "df4.drop(columns=[\"Unnamed: 0\",\"Cabin\"] ,axis=1,inplace=True)\n",
    "df4.isnull().sum()\n",
    "\n",
    "df.drop(1, inplace=True)\n",
    "df\n",
    "df.set_index('Name', inplace = True) #time series data>>you will be making date time column as index\n",
    "df\n",
    "df.reset_index(inplace = True)\n",
    "df\n",
    "\n",
    "# use aggrigate funtions\n",
    "df4[\"Age\"].mean()   # for replace value find mean\n",
    "\n",
    "#  fill null age value # replace missing values with 0\n",
    "df4[\"Age\"].fillna(df4[\"Age\"].mean(),  inplace=True)\n",
    "\n",
    "df4.dropna(inplace=True)\n",
    "\n",
    "# change the datatypes\n",
    "df4[\"SibSp\"].astype(\"float32\")\n",
    "# data types change\n",
    "df4[\"SibSp\"] = df4[\"SibSp\"].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# remove duplicates based on column 'A'\n",
    "df.drop_duplicates(subset=['A'], keep='first', inplace=True)\n",
    "\n",
    "# rename columns\n",
    "df.rename(columns={'A': 'Age', 'B': 'Name', 'C': 'Salary'}, inplace=True)\n",
    "\n",
    "# remove rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#  How to remove columns containing only NaN values?\n",
    "# check which columns contain only NaN values\n",
    "columns_with_nan = df.columns[df.isnull().all()]\n",
    "\n",
    "# drop the columns containing only NaN values\n",
    "df = df.drop(columns=columns_with_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Questions form dataset\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#Q. How many passengers are less than 5 years old\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m df2\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m\"\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m df2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n\u001b[0;32m      7\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m5\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": [
    "# Questions form dataset\n",
    "\n",
    "#Q. How many passengers are less than 5 years old\n",
    "df2.drop([\"Unnamed: 0\"], inplace=True,axis=1)\n",
    "# unique value count  of age\n",
    "\n",
    "df2['Age'].value_counts()\n",
    "\n",
    "df['Age'] < 5\n",
    "df[df['Age'] < 5]\n",
    "\n",
    "len(df[df['Age'] < 5])\n",
    "\n",
    "#no of passenger >18\n",
    "\n",
    "len(df[df['Age'] > 18])\n",
    "\n",
    "#how many passengers are less than 18 years old\n",
    "\n",
    "len(df) - len(df[df['Age'] > 18])\n",
    "\n",
    "len(df[df['Age'] <= 18]) #missing value in age column\n",
    "\n",
    "#Q. How many passengers have paid less than avg fare\n",
    "\n",
    "df['Fare'].mean()\n",
    "df[df['Fare']<df['Fare'].mean()]\n",
    "len(df[df['Fare'] > df['Fare'].mean()])\n",
    "#How many passengers paid 0 fare\n",
    "list(df[df['Fare'] == 0].Name)\n",
    "\n",
    "#Qhow many passengers are male and female\n",
    "len(df[df['Sex'] == \"male\"])\n",
    "len(df[df['Sex'] == \"female\"])\n",
    "df['Sex'].value_counts(normalize = True)\n",
    "\n",
    "#Q how many passengers of class 1\n",
    "df[df['Pclass'] == 1]\n",
    "\n",
    "#How many passengers survived\n",
    "df[df['Survived'] == 1]\n",
    "df['Survived'].value_counts(normalize = True)\n",
    "\n",
    "#How many females paid more than avg fare\n",
    "df['Sex'] == 'female'\n",
    "df['Fare'].mean()\n",
    "df[(df['Sex'] == 'female') & (df['Fare'] > df['Fare'].mean())]\n",
    "\n",
    "#Q how many passengers are male or who paid greater than avg fare >>or\n",
    "#Qhow many male passenger paid more than avg >>and\n",
    "df[(df['Sex'] == 'male') | (df['Fare'] > df['Fare'].mean())]\n",
    "\n",
    "np.mean(df['Fare'])\n",
    "df['Fare'].mean()\n",
    "max(df['Fare'])\n",
    "min(df['Fare'])\n",
    "\n",
    "#who are the passengers who paid maximum fare\n",
    "df[df['Fare'] == max(df['Fare'])]['Name']\n",
    "\n",
    "# Q. How many passenger have parch greater than 3\n",
    "# Q. How many passenger who survived paid the maximum fare\n",
    "# Q. How many passengers who didnt survived was from class 1\n",
    "# Q. How many passengers are children(<5 years old)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 2. Handle Error Values\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mpd\u001b[49m.DataFrame({\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mName\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33mAlice\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mBob\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mCharlie\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mScore\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m85\u001b[39m, \u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m92\u001b[39m]\n\u001b[32m      5\u001b[39m })\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Replace 'error' with NaN and convert column to numeric\u001b[39;00m\n\u001b[32m      8\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mScore\u001b[39m\u001b[33m'\u001b[39m] = pd.to_numeric(df[\u001b[33m'\u001b[39m\u001b[33mScore\u001b[39m\u001b[33m'\u001b[39m], errors=\u001b[33m'\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# 2. Handle Error Values\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Score': [85, 'error', 92]\n",
    "})\n",
    "\n",
    "# Replace 'error' with NaN and convert column to numeric\n",
    "df['Score'] = pd.to_numeric(df['Score'], errors='coerce')\n",
    "\n",
    "\n",
    "# When errors='coerce' is specified in these functions, it dictates how the function handles values that cannot be successfully converted to the target data type. Instead of raising an error and stopping the execution (which is the default behavior, errors='raise'), errors='coerce' will convert any unparseable or invalid values to NaN (Not a Number) for numeric types or NaT (Not a Time) for datetime/timedelta types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df.rename(columns={'A': 'Age'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>447.016393</td>\n",
       "      <td>2.531876</td>\n",
       "      <td>30.626179</td>\n",
       "      <td>0.553734</td>\n",
       "      <td>0.329690</td>\n",
       "      <td>22.117887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>444.368421</td>\n",
       "      <td>1.950292</td>\n",
       "      <td>28.343690</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.464912</td>\n",
       "      <td>48.395408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          PassengerId    Pclass        Age     SibSp     Parch       Fare\n",
       "Survived                                                                 \n",
       "0          447.016393  2.531876  30.626179  0.553734  0.329690  22.117887\n",
       "1          444.368421  1.950292  28.343690  0.473684  0.464912  48.395408"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
    "df\n",
    "df1=df.copy()\n",
    "\n",
    "# group by for single columns\n",
    "df.groupby([\"customer segment\"])[\"Fare\"].sum()\n",
    "\n",
    "df.groupby('Survived').mean(numeric_only=True) #for recent pandas version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by for columns\n",
    "df.groupby([\"Sex\",\"customer segment\"])[\"Fare\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # group by for columns and use mupltiple aggregate functions\n",
    "df.groupby([\"customer segment\"])[\"Fare\"].aggregate([\"mean\",\"count\",\"min\",\"max\",\"sum\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>245412</td>\n",
       "      <td>1390</td>\n",
       "      <td>Braund, Mr. Owen HarrisAllen, Mr. William Henr...</td>\n",
       "      <td>malemalemalemalemalemalemalefemalemalefemalema...</td>\n",
       "      <td>12985.50</td>\n",
       "      <td>304</td>\n",
       "      <td>181</td>\n",
       "      <td>A/5 2117137345033087717463349909A/5. 215134708...</td>\n",
       "      <td>12142.7199</td>\n",
       "      <td>E46C23 C25 C27B30C83F G73E31A5D26C110B58 B60D2...</td>\n",
       "      <td>SSQSSSSSQSSSCSSCSCSSSSSCSQCSSSCCSCSSCSSSSSCSSS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>151974</td>\n",
       "      <td>667</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>femalefemalefemalefemalefemalefemalefemalefema...</td>\n",
       "      <td>8219.67</td>\n",
       "      <td>162</td>\n",
       "      <td>159</td>\n",
       "      <td>PC 17599STON/O2. 3101282113803347742237736PP 9...</td>\n",
       "      <td>16551.2294</td>\n",
       "      <td>C85C123G6C103D56A6B78D33C52B28F33C23 C25 C27D1...</td>\n",
       "      <td>CSSSCSSSSCSQSSQCQCCCQQCSSSSCSSSSSSQSSSCSSSQSCS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          PassengerId  Pclass  \\\n",
       "Survived                        \n",
       "0              245412    1390   \n",
       "1              151974     667   \n",
       "\n",
       "                                                       Name  \\\n",
       "Survived                                                      \n",
       "0         Braund, Mr. Owen HarrisAllen, Mr. William Henr...   \n",
       "1         Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "\n",
       "                                                        Sex       Age  SibSp  \\\n",
       "Survived                                                                       \n",
       "0         malemalemalemalemalemalemalefemalemalefemalema...  12985.50    304   \n",
       "1         femalefemalefemalefemalefemalefemalefemalefema...   8219.67    162   \n",
       "\n",
       "          Parch                                             Ticket  \\\n",
       "Survived                                                             \n",
       "0           181  A/5 2117137345033087717463349909A/5. 215134708...   \n",
       "1           159  PC 17599STON/O2. 3101282113803347742237736PP 9...   \n",
       "\n",
       "                Fare                                              Cabin  \\\n",
       "Survived                                                                  \n",
       "0         12142.7199  E46C23 C25 C27B30C83F G73E31A5D26C110B58 B60D2...   \n",
       "1         16551.2294  C85C123G6C103D56A6B78D33C52B28F33C23 C25 C27D1...   \n",
       "\n",
       "                                                   Embarked  \n",
       "Survived                                                     \n",
       "0         SSQSSSSSQSSSCSSCSCSSSSSCSQCSSSCCSCSSCSSSSSCSSS...  \n",
       "1         CSSSCSSSSCSQSSQCQCCCQQCSSSSCSSSSSSQSSSCSSSQSCS...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Survived').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Survived').describe()\n",
    "df.groupby('Survived').aggregate([min, 'max', 'mean', 'median', 'count', 'var'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# # Sorting by column \"Population\"\n",
    "df.sort_values(by=['Fare'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_976\\1239507556.py:3: FutureWarning: The provided callable <function min at 0x000001EAB3CB9BC0> is currently using SeriesGroupBy.min. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"min\" instead.\n",
      "  result = df.groupby('Survived')[numeric_columns].aggregate([np.min, 'max', 'mean', 'median', 'count', 'var'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"6\" halign=\"left\">PassengerId</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Survived</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Parch</th>\n",
       "      <th colspan=\"6\" halign=\"left\">Fare</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>count</th>\n",
       "      <th>var</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>...</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>count</th>\n",
       "      <th>var</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>count</th>\n",
       "      <th>var</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>891</td>\n",
       "      <td>447.016393</td>\n",
       "      <td>455.0</td>\n",
       "      <td>549</td>\n",
       "      <td>67933.454110</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329690</td>\n",
       "      <td>0.0</td>\n",
       "      <td>549</td>\n",
       "      <td>0.677602</td>\n",
       "      <td>0.0</td>\n",
       "      <td>263.0000</td>\n",
       "      <td>22.117887</td>\n",
       "      <td>10.5</td>\n",
       "      <td>549</td>\n",
       "      <td>985.219509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>890</td>\n",
       "      <td>444.368421</td>\n",
       "      <td>439.5</td>\n",
       "      <td>342</td>\n",
       "      <td>63684.984102</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.464912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>342</td>\n",
       "      <td>0.595539</td>\n",
       "      <td>0.0</td>\n",
       "      <td>512.3292</td>\n",
       "      <td>48.395408</td>\n",
       "      <td>26.0</td>\n",
       "      <td>342</td>\n",
       "      <td>4435.160158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         PassengerId                                             Survived      \\\n",
       "                 min  max        mean median count           var      min max   \n",
       "Survived                                                                        \n",
       "0                  1  891  447.016393  455.0   549  67933.454110        0   0   \n",
       "1                  2  890  444.368421  439.5   342  63684.984102        1   1   \n",
       "\n",
       "                      ...     Parch                        Fare            \\\n",
       "         mean median  ...      mean median count       var  min       max   \n",
       "Survived              ...                                                   \n",
       "0         0.0    0.0  ...  0.329690    0.0   549  0.677602  0.0  263.0000   \n",
       "1         1.0    1.0  ...  0.464912    0.0   342  0.595539  0.0  512.3292   \n",
       "\n",
       "                                               \n",
       "               mean median count          var  \n",
       "Survived                                       \n",
       "0         22.117887   10.5   549   985.219509  \n",
       "1         48.395408   26.0   342  4435.160158  \n",
       "\n",
       "[2 rows x 42 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "result = df.groupby('Survived')[numeric_columns].aggregate([np.min, 'max', 'mean', 'median', 'count', 'var'])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q what is the average fare paid by people who survived?\n",
    "#groupby >> https://www.w3resource.com/python-exercises/pandas/groupby/index.php\n",
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
    "df\n",
    "df1=df.copy()\n",
    "df.groupby('Survived').mean(numeric_only=True) #for recent pandas version\n",
    "\n",
    "df.groupby('Survived').mean()\n",
    "df.groupby('Survived').min()\n",
    "df.groupby('Survived').sum()\n",
    "df.groupby('Survived').mean(numeric_only=True)\n",
    "df.groupby('Survived').mean()\n",
    "df.groupby('Survived').describe()\n",
    "df.groupby('Survived').aggregate([min, 'max', 'mean', 'median', 'count', np.std, 'var'])\n",
    "\n",
    "#groupby with two columns\n",
    "#Q. Total people for each sex, pclass\n",
    "df1.groupby(['Sex', 'Pclass'])['Survived'].sum()\n",
    "\n",
    "#To convert the result to dataframe\n",
    "df.groupby(['Sex', 'Pclass'])['Survived'].sum().to_frame()\n",
    "\n",
    "df.groupby(['Sex', 'Pclass'])['Survived'].sum().unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hstack,unstack,to_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.groupby('Pclass').sum(numeric_only = True)\n",
    "df1\n",
    "df1.T\n",
    "df1.transpose()\n",
    "\n",
    "df_1 = df[['Name', 'Sex', 'Age']][0:5]\n",
    "df_1\n",
    "df_2 =  df[['Name', 'Sex', 'Age']][5:10]\n",
    "df_2.reset_index(drop = True, inplace = True)\n",
    "result = pd.concat([df_1, df_2], axis = 0)\n",
    "result\n",
    "pd.concat([df_1, df_2], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat\n",
    "#apply\n",
    "df['len_name'] = df['Name'].apply(len)\n",
    "\n",
    "df\n",
    "# convert dollor in rupee\n",
    "def convert(x):\n",
    "    return x*90\n",
    "\n",
    "df['Fare_1'] = df['Fare'].apply(convert)\n",
    "\n",
    "#  ============================\n",
    "def create_flag(x):\n",
    "    if x < 10:\n",
    "        return \"cheap\"\n",
    "    elif x >= 10 and x <20:\n",
    "          return \"medium\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "\n",
    "df[\"flag_fare\"] = df['Fare'].apply(create_flag)\n",
    "\n",
    "df\n",
    "\n",
    "df1.set_index('c', inplace = True)\n",
    "df.sort_values(by = \"Fare\")\n",
    "\n",
    "# # Sorting by column \"Population\"\n",
    "df.sort_values(by=['Population'], ascending=False)\n",
    "# Sorting by columns \"Country\" and then \"Continent\"\n",
    "df.sort_values(by=['Country', 'Continent'])\n",
    "# Sorting by columns \"Country\" in descending\n",
    "# order and then \"Continent\" in ascending order\n",
    "df.sort_values(by=['Country', 'Continent'],\n",
    "               ascending=[False, True])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grouping, aggregation, merging, and **joining**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "titanic.head()\n",
    "\n",
    "## ðŸ”¹ Part 1: Grouping and Aggregation\n",
    "\n",
    "### âœ… Use Case: Find average age of passengers grouped by gender and class\n",
    "\n",
    "```python\n",
    "# Group by sex and class, then find average age\n",
    "grouped = titanic.groupby(['sex', 'class'])['age'].mean().reset_index()\n",
    "print(grouped)\n",
    "```\n",
    "\n",
    "### âœ… Use Case: Count passengers in each class\n",
    "\n",
    "```python\n",
    "# Count of passengers in each class\n",
    "class_counts = titanic['class'].value_counts()\n",
    "print(class_counts)\n",
    "```\n",
    "\n",
    "### âœ… Use Case: Survival rate by gender\n",
    "\n",
    "```python\n",
    "# Mean of survived (1 = survived, 0 = did not survive)\n",
    "survival_rate = titanic.groupby('sex')['survived'].mean().reset_index()\n",
    "print(survival_rate)\n",
    "```\n",
    "\n",
    "### âœ… Use Case: Multiple Aggregations\n",
    "\n",
    "```python\n",
    "# Aggregate age with multiple functions\n",
    "agg_stats = titanic.groupby('class')['age'].agg(['mean', 'min', 'max', 'count']).reset_index()\n",
    "print(agg_stats)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Part 2: Merging and Joining DataFrames\n",
    "\n",
    "Letâ€™s create two example DataFrames from the Titanic dataset for merging and joining.\n",
    "\n",
    "### âœ… Step 1: Create two dataframes\n",
    "\n",
    "```python\n",
    "# Selecting relevant columns\n",
    "df1 = titanic[['survived', 'sex', 'age']].iloc[:10]\n",
    "df2 = titanic[['age', 'fare']].iloc[:10]\n",
    "1\n",
    "# Add an ID column for joining\n",
    "df = df1.reset_index().rename(columns={'index': 'id'})\n",
    "df2 = df2.reset_index().rename(columns={'index': 'id'})\n",
    "```\n",
    "\n",
    "### âœ… Merge: Inner Join on `id`\n",
    "\n",
    "```python\n",
    "merged_inner = pd.merge(df1, df2, on='id', how='inner')\n",
    "print(merged_inner)\n",
    "```\n",
    "\n",
    "### âœ… Merge: Left Join\n",
    "\n",
    "```python\n",
    "merged_left = pd.merge(df1, df2, on='id', how='left')\n",
    "print(merged_left)\n",
    "```\n",
    "\n",
    "### âœ… Join: Using `set_index()` and `join()`\n",
    "\n",
    "```python\n",
    "df1_indexed = df1.set_index('id')\n",
    "df2_indexed = df2.set_index('id')\n",
    "\n",
    "joined_df = df1_indexed.join(df2_indexed, how='inner')\n",
    "print(joined_df)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Summary\n",
    "\n",
    "| Task                  | Function                              |\n",
    "| --------------------- | ------------------------------------- |\n",
    "| Grouping by column(s) | `groupby()`                           |\n",
    "| Aggregation           | `agg()`, `mean()`, `sum()`, `count()` |\n",
    "| Merge two dataframes  | `pd.merge()`                          |\n",
    "| Join on index         | `df1.join(df2)`                       |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like this in a **teaching format with assignments** or a **Jupyter Notebook**?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitions and Use Cases:\n",
    "# ------------------------------------------------------------\n",
    "# concat: Combines DataFrames either vertically (row-wise) or horizontally (column-wise).\n",
    "#   Use case: Combine data from multiple CSVs or append new data.\n",
    "# merge: Combines DataFrames based on common columns (similar to SQL joins).\n",
    "#   Use case: Merge customer info with transaction data.\n",
    "# join: Joins DataFrames using their index.\n",
    "#   Use case: Combine metadata indexed by unique IDs.\n",
    "# pivot: Reshapes data by turning unique values in a column into new columns.\n",
    "#   Use case: Create summary tables like Excel pivot tables.\n",
    "# melt (unpivot): Converts wide-format data into long-format.\n",
    "#   Use case: Prepare data for analysis/visualization by tidying it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### --- Part 0: Basic df1 and df2 for Join, Merge, Concat, Pivot, Melt --- ###\n",
    "\n",
    "df1 = pd.DataFrame({\n",
    "    'ID': [1, 2, 3],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Score': [85, 90, 95]\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'ID': [2, 3, 4],\n",
    "    'Subject': ['Math', 'English', 'Science'],\n",
    "    'Grade': ['A', 'B', 'A']\n",
    "})\n",
    "\n",
    "print(\"\\ndf1:\")\n",
    "print(df1)\n",
    "print(\"\\ndf2:\")\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Concat Example --- ###\n",
    "\n",
    "concat_rows = pd.concat([df1, df1], axis=0, ignore_index=True)\n",
    "concat_cols = pd.concat([df1, df2], axis=1)\n",
    "print(\"\\nConcat Rows:\")\n",
    "print(concat_rows)\n",
    "print(\"\\nConcat Columns:\")\n",
    "print(concat_cols)\n",
    "\n",
    "### --- Merge Example --- ###\n",
    "\n",
    "merged_inner = pd.merge(df1, df2, on='ID', how='inner')\n",
    "merged_outer = pd.merge(df1, df2, on='ID', how='outer')\n",
    "print(\"\\nInner Merge:\")\n",
    "print(merged_inner)\n",
    "print(\"\\nOuter Merge:\")\n",
    "print(merged_outer)\n",
    "\n",
    "### --- Join Example --- ###\n",
    "\n",
    "df1_join = df1.set_index('ID')\n",
    "df2_join = df2.set_index('ID')\n",
    "joined_df = df1_join.join(df2_join, how='outer')\n",
    "print(\"\\nJoin on Index:\")\n",
    "print(joined_df)\n",
    "\n",
    "### --- Pivot Example --- ###\n",
    "pivot_data = pd.DataFrame({\n",
    "    'ID': [1, 1, 2, 2],\n",
    "    'Subject': ['Math', 'Science', 'Math', 'Science'],\n",
    "    'Score': [88, 92, 80, 85]\n",
    "})\n",
    "pivot_table = pivot_data.pivot(index='ID', columns='Subject', values='Score')\n",
    "print(\"\\nPivot Table:\")\n",
    "print(pivot_table)\n",
    "\n",
    "### --- Melt (Unpivot) Example --- ###\n",
    "\n",
    "melted = pd.melt(pivot_table.reset_index(), id_vars=['ID'], value_vars=['Math', 'Science'], var_name='Subject', value_name='Score')\n",
    "print(\"\\nMelted Data:\")\n",
    "print(melted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Let's now cover **concatenation** using Pandas and the **Titanic dataset**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Part 3: Concatenation in Pandas\n",
    "\n",
    "### âœ… What is Concatenation?\n",
    "\n",
    "**Concatenation** means **combining multiple DataFrames either vertically (row-wise)** or **horizontally (column-wise)**.\n",
    "\n",
    "The function used is:\n",
    "\n",
    "```python\n",
    "pd.concat([df1, df2], axis=0 or 1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### â–¶ï¸ Example 1: **Row-wise Concatenation** (Vertical)\n",
    "\n",
    "Use Case: Combine first 5 rows and next 5 rows of Titanic dataset.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# Create two subsets\n",
    "df_top = titanic.iloc[:5]\n",
    "df_bottom = titanic.iloc[5:10]\n",
    "\n",
    "# Concatenate row-wise (axis=0)\n",
    "df_vertical = pd.concat([df_top, df_bottom], axis=0)\n",
    "print(df_vertical)\n",
    "```\n",
    "\n",
    "ðŸ“Œ Default is `axis=0` which stacks rows.\n",
    "âœ… The column names must match for proper stacking.\n",
    "\n",
    "---\n",
    "\n",
    "### â–¶ï¸ Example 2: **Column-wise Concatenation** (Horizontal)\n",
    "\n",
    "Use Case: Combine two DataFrames side by side.\n",
    "\n",
    "```python\n",
    "# Select 5 rows of different columns\n",
    "df1 = titanic[['survived', 'sex']].iloc[:5]\n",
    "df2 = titanic[['age', 'fare']].iloc[:5]\n",
    "\n",
    "# Concatenate column-wise (axis=1)\n",
    "df_horizontal = pd.concat([df1, df2], axis=1)\n",
    "print(df_horizontal)\n",
    "```\n",
    "\n",
    "ðŸ“Œ `axis=1` joins DataFrames **side-by-side**, like adding new features.\n",
    "\n",
    "---\n",
    "\n",
    "### â–¶ï¸ Example 3: Concatenation with `ignore_index=True`\n",
    "\n",
    "Use Case: Reset the index after concatenation.\n",
    "\n",
    "```python\n",
    "df_combined = pd.concat([df_top, df_bottom], axis=0, ignore_index=True)\n",
    "print(df_combined)\n",
    "```\n",
    "\n",
    "ðŸ“Œ `ignore_index=True` resets the row indices in the new DataFrame.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Summary Table\n",
    "\n",
    "| Type        | Axis                | Description     |\n",
    "| ----------- | ------------------- | --------------- |\n",
    "| Vertical    | `axis=0`            | Appends rows    |\n",
    "| Horizontal  | `axis=1`            | Appends columns |\n",
    "| Reset index | `ignore_index=True` | Renumbers rows  |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like **assignments** or **use-case-based practice questions** using `concat`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date time\n",
    "df = pd.DataFrame({\"date\": ['2024-02-08', '2024-02-09', '2024-02-10']})\n",
    "df\n",
    "df.dtypes\n",
    "df['updated_date'] = pd.to_datetime(df['date'])\n",
    "df\n",
    "df.dtypes\n",
    "df['month'] = df['updated_date'].dt.month\n",
    "df\n",
    "df['year'] = df['updated_date'].dt.year\n",
    "df\n",
    "df['day'] = df['updated_date'].dt.day\n",
    "df\n",
    "\n",
    "\n",
    "# Convert to date:datatype\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dataframe is dictionary\n",
    "df1 = {\"key1\": [2, 3, 4, 5],\n",
    "    \"key2\": [4, 5, 6, 7],\n",
    "    \"key3\": [2, 3, 4, 5]}\n",
    "\n",
    "df1\n",
    "df1 = pd.DataFrame(df1)\n",
    "df1\n",
    "#make dataframe is dictionary\n",
    "df2= {\"key1\": (2, 3, 4, 5),\n",
    "    \"key2\": (4, 5, 6, 7),\n",
    "    \"key3\": (2, 3, 4, 5)}\n",
    "df2\n",
    "df2 = pd.DataFrame(df2)\n",
    "df2\n",
    "\n",
    "pd.merge(df1, df2, how = 'left')\n",
    "\n",
    "#merge\n",
    "pd.merge(df1, df2, how = 'right')\n",
    "pd.merge(df1, df2, how = 'left', left_on = 'key2', right_on = 'key4')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# This is useful for categorizing passengers\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# by age groups, which can be helpful for further analysis (e.g., checking survival rates by age group).\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mAge_Group\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mpd\u001b[49m.cut(df[\u001b[33m'\u001b[39m\u001b[33mAge\u001b[39m\u001b[33m'\u001b[39m], \n\u001b[32m      4\u001b[39m                          bins=[\u001b[32m0\u001b[39m, \u001b[32m12\u001b[39m, \u001b[32m18\u001b[39m, \u001b[32m60\u001b[39m, \u001b[32m80\u001b[39m], \n\u001b[32m      5\u001b[39m                          labels=[\u001b[33m'\u001b[39m\u001b[33mChild\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTeen\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAdult\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSenior\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# pd.cut() function, which segments the data into bins (ranges) and labels those\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# bins with categorical names (e.g., 'Child', 'Teen', 'Adult', 'Senior') based on the values in the Age column.\u001b[39;00m\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m \n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Further Analysis:\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# You can now analyze data based on these groups.\u001b[39;00m\n\u001b[32m     12\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mAge_Group\u001b[39m\u001b[33m'\u001b[39m].value_counts()\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# This is useful for categorizing passengers\n",
    "# by age groups, which can be helpful for further analysis (e.g., checking survival rates by age group).\n",
    "df['Age_Group'] = pd.cut(df['Age'], \n",
    "                         bins=[0, 12, 18, 60, 80], \n",
    "                         labels=['Child', 'Teen', 'Adult', 'Senior'])\n",
    "# pd.cut() function, which segments the data into bins (ranges) and labels those\n",
    "# bins with categorical names (e.g., 'Child', 'Teen', 'Adult', 'Senior') based on the values in the Age column.\n",
    "\n",
    "\n",
    "# Further Analysis:\n",
    "# You can now analyze data based on these groups.\n",
    "df['Age_Group'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Part 4: Convert Pandas DataFrame to JSON\n",
    "\n",
    "Pandas makes it easy to convert a DataFrame to JSON format using the `.to_json()` method.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Syntax\n",
    "\n",
    "```python\n",
    "df.to_json(path_or_buf=None, orient=None, lines=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### â–¶ï¸ Example 1: Convert Titanic DataFrame to JSON string\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Load dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# Convert first 5 rows to JSON\n",
    "json_data = titanic.head().to_json()\n",
    "print(json_data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### â–¶ï¸ Example 2: Save DataFrame as JSON file\n",
    "\n",
    "```python\n",
    "# Save first 5 rows to a JSON file\n",
    "titanic.head().to_json(\"titanic_sample.json\", orient=\"records\", lines=True)\n",
    "```\n",
    "\n",
    "ðŸ“‚ This will create a JSON file with each record on a new line (useful for large datasets).\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ” Different `orient` options in `.to_json()`\n",
    "\n",
    "| Orient      | Description                    | Output Format                       |\n",
    "| ----------- | ------------------------------ | ----------------------------------- |\n",
    "| `'split'`   | Dict with index, columns, data | `{index, columns, data}`            |\n",
    "| `'records'` | List like row-wise dicts       | `[{\"col1\":val1, \"col2\":val2}, ...]` |\n",
    "| `'index'`   | Dict of dicts (index as key)   | `{index: {col:val}}`                |\n",
    "| `'columns'` | Dict of columns                | `{col: {index:val}}`                |\n",
    "| `'values'`  | Just the data as list of lists | `[[...], [...]]`                    |\n",
    "| `'table'`   | JSON Table Schema              | Complex (used in API)               |\n",
    "\n",
    "---\n",
    "\n",
    "### â–¶ï¸ Example 3: Use different `orient`\n",
    "\n",
    "```python\n",
    "# Records orientation\n",
    "json_records = titanic.head().to_json(orient='records')\n",
    "print(json_records)\n",
    "\n",
    "# Split orientation\n",
    "json_split = titanic.head().to_json(orient='split')\n",
    "print(json_split)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Summary\n",
    "\n",
    "| Task                   | Code                                    |\n",
    "| ---------------------- | --------------------------------------- |\n",
    "| Convert to JSON string | `df.to_json()`                          |\n",
    "| Save to JSON file      | `df.to_json(\"file.json\")`               |\n",
    "| Pretty JSON            | Use `json.dumps()` with `indent=4`      |\n",
    "| Control format         | Use `orient='records'`, `'split'`, etc. |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a **sample assignment** or **real-world use case** involving JSON export (e.g., preparing data for a REST API)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Let's now learn how to **load JSON data into a Pandas DataFrame** â€” the reverse of exporting.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Part 5: Convert JSON to Pandas DataFrame\n",
    "\n",
    "### âœ… Common Use Case\n",
    "\n",
    "You receive a **JSON file** (e.g., from a web API or data export) and need to **analyze it using Pandas**.\n",
    "\n",
    "---\n",
    "\n",
    "### â–¶ï¸ Example 1: Read JSON String\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Sample JSON string (records orientation)\n",
    "json_str = '''\n",
    "[\n",
    "    {\"name\": \"Alice\", \"age\": 25, \"city\": \"New York\"},\n",
    "    {\"name\": \"Bob\", \"age\": 30, \"city\": \"Paris\"},\n",
    "    {\"name\": \"Charlie\", \"age\": 28, \"city\": \"London\"}\n",
    "]\n",
    "'''\n",
    "\n",
    "# Convert JSON string to DataFrame\n",
    "df = pd.read_json(json_str)\n",
    "print(df)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### â–¶ï¸ Example 2: Read JSON File (from disk)\n",
    "\n",
    "```python\n",
    "# Load JSON file into DataFrame\n",
    "df_json = pd.read_json('female_survivors.json', lines=True)\n",
    "print(df_json.head())\n",
    "```\n",
    "\n",
    "ðŸ”¹ `lines=True` is **required** if each row is a separate JSON object (NDJSON format).\n",
    "\n",
    "---\n",
    "\n",
    "### â–¶ï¸ Example 3: Read Nested JSON (Using `json_normalize`)\n",
    "\n",
    "```python\n",
    "from pandas import json_normalize\n",
    "\n",
    "# Sample nested JSON\n",
    "nested_json = {\n",
    "    \"department\": \"IT\",\n",
    "    \"employees\": [\n",
    "        {\"name\": \"Alice\", \"role\": \"Developer\"},\n",
    "        {\"name\": \"Bob\", \"role\": \"Manager\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Normalize nested list\n",
    "df_nested = json_normalize(nested_json, 'employees', meta='department')\n",
    "print(df_nested)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Summary Table\n",
    "\n",
    "| Task                          | Function                   | Notes                      |\n",
    "| ----------------------------- | -------------------------- | -------------------------- |\n",
    "| JSON string to DataFrame      | `pd.read_json()`           | From string or file        |\n",
    "| NDJSON to DataFrame           | `pd.read_json(lines=True)` | Each line is a JSON object |\n",
    "| Nested JSON to flat DataFrame | `json_normalize()`         | Extract nested fields      |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Assignments: JSON â†’ Pandas\n",
    "\n",
    "### ðŸŽ¯ **Assignment 1: Basic File Import**\n",
    "\n",
    "| Task    | Instructions                                              |\n",
    "| ------- | --------------------------------------------------------- |\n",
    "| File    | Use `children_passengers.json` (from previous assignment) |\n",
    "| Load    | Use `pd.read_json()` with `lines=True`                    |\n",
    "| Display | Show first 5 rows                                         |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ **Assignment 2: Normalize Nested JSON**\n",
    "\n",
    "| Task        | Instructions                                                                                                                     |\n",
    "| ----------- | -------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| Create JSON | Create a Python dictionary like:<br>`{\"team\": \"Data\", \"members\": [{\"name\": \"A\", \"skill\": \"ML\"}, {\"name\": \"B\", \"skill\": \"SQL\"}]}` |\n",
    "| Convert     | Use `json_normalize()` to extract `members` with `team` as meta                                                                  |\n",
    "| Display     | Print the DataFrame                                                                                                              |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a **complete Jupyter Notebook** combining all parts (Export + Import JSON using Titanic data)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hereâ€™s a detailed explanation of each of the provided data manipulation techniques in Pandas:\n",
    "\n",
    "### 1. **Pivot the DataFrame**\n",
    "\n",
    "The `pivot_table()` function reshapes the data by summarizing it. It allows you to group by one or more columns and calculate aggregate values (e.g., mean, sum).\n",
    "\n",
    "#### Syntax:\n",
    "```python\n",
    "pivot_table = df.pivot_table(values='Fare', index='Pclass', columns='Sex', aggfunc='mean')\n",
    "```\n",
    "\n",
    "- **`values='Fare'`**: The column we want to aggregate.\n",
    "- **`index='Pclass'`**: Rows will be grouped by the `Pclass` (Passenger Class) column.\n",
    "- **`columns='Sex'`**: The unique values in the `Sex` column will become the columns in the resulting table (i.e., Male, Female).\n",
    "- **`aggfunc='mean'`**: The aggregation function, in this case, calculates the **mean** fare for each combination of `Pclass` and `Sex`.\n",
    "\n",
    "#### Example Output:\n",
    "| Sex    | Female | Male |\n",
    "|--------|--------|------|\n",
    "| Pclass |        |      |\n",
    "| 1      | 100.0  | 80.0 |\n",
    "| 2      | 40.0   | 20.0 |\n",
    "| 3      | 10.0   | 5.0  |\n",
    "\n",
    "This gives the **mean fare** for each combination of **Passenger Class (`Pclass`)** and **Sex**.\n",
    "\n",
    "### 2. **Melt the DataFrame (Unpivot)**\n",
    "\n",
    "The `pd.melt()` function reshapes the DataFrame from wide format to long format (unpivot). It is useful when you want to convert multiple columns into rows.\n",
    "\n",
    "#### Syntax:\n",
    "```python\n",
    "df_melted = pd.melt(df, id_vars=['Pclass'], value_vars=['Age', 'Fare'])\n",
    "```\n",
    "\n",
    "- **`id_vars=['Pclass']`**: These columns are kept intact.\n",
    "- **`value_vars=['Age', 'Fare']`**: These columns will be \"melted\" into one single column of values, creating a long-form DataFrame.\n",
    "\n",
    "#### Example Output:\n",
    "\n",
    "| Pclass | variable | value |\n",
    "|--------|----------|-------|\n",
    "| 1      | Age      | 22    |\n",
    "| 1      | Fare     | 71.0  |\n",
    "| 2      | Age      | 26    |\n",
    "| 2      | Fare     | 12.0  |\n",
    "\n",
    "The new DataFrame is in a **long** format, with each row representing a single observation for `Age` or `Fare`.\n",
    "\n",
    "### 3. **Group by 'Pclass' and Calculate Mean Fare**\n",
    "\n",
    "The `groupby()` function groups the DataFrame by one or more columns and allows you to apply aggregation functions like `mean`, `sum`, etc.\n",
    "\n",
    "#### Syntax:\n",
    "```python\n",
    "df_grouped = df.groupby('Pclass')['Fare'].mean()\n",
    "```\n",
    "\n",
    "- **`groupby('Pclass')`**: Groups the data by `Pclass` (Passenger Class).\n",
    "- **`['Fare']`**: Selects the `Fare` column.\n",
    "- **`.mean()`**: Calculates the **mean fare** for each group.\n",
    "\n",
    "#### Example Output:\n",
    "\n",
    "| Pclass | Fare  |\n",
    "|--------|-------|\n",
    "| 1      | 84.0  |\n",
    "| 2      | 20.0  |\n",
    "| 3      | 13.0  |\n",
    "\n",
    "This shows the **mean fare** paid by passengers in each class (`Pclass`).\n",
    "\n",
    "### 4. **Sort by 'Age' Column**\n",
    "\n",
    "The `sort_values()` function sorts the DataFrame by one or more columns.\n",
    "\n",
    "#### Syntax:\n",
    "```python\n",
    "df_sorted = df.sort_values(by='Age', ascending=False)\n",
    "```\n",
    "\n",
    "- **`by='Age'`**: Specifies the column by which to sort the DataFrame (here, `Age`).\n",
    "- **`ascending=False`**: Sorts the data in **descending** order, so the oldest passengers appear first.\n",
    "\n",
    "#### Example Output:\n",
    "\n",
    "| Name  | Age | Fare |\n",
    "|-------|-----|------|\n",
    "| John  | 80  | 100  |\n",
    "| Jane  | 60  | 50   |\n",
    "| Mark  | 45  | 20   |\n",
    "\n",
    "This sorts the DataFrame by **Age** in descending order, so the oldest passengers are listed first.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "- **Pivot Table**: Reshapes the DataFrame by summarizing values based on rows and columns.\n",
    "- **Melt**: Unpivots the DataFrame, converting wide format into long format.\n",
    "- **GroupBy**: Groups the data by one or more columns and applies aggregation functions (like `mean`).\n",
    "- **Sort**: Sorts the DataFrame by specified columns in either ascending or descending order.\n",
    "\n",
    "These operations are fundamental for transforming and analyzing data in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Data Transformation\n",
    "# Pivot the DataFrame\n",
    "pivot_table = df.pivot_table(values='Fare', index='Pclass', columns='Sex', aggfunc='mean')\n",
    "\n",
    "# Melt the DataFrame (unpivot)\n",
    "df_melted = pd.melt(df, id_vars=['Pclass'], value_vars=['Age', 'Fare'])\n",
    "\n",
    "# Group by 'Pclass' and calculate mean fare\n",
    "df_grouped = df.groupby('Pclass')['Fare'].mean()\n",
    "\n",
    "# Sort by 'Age' column\n",
    "df_sorted = df.sort_values(by='Age', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "d = pd.Series([1, 2, 8, 4, 5, 6])\n",
    "d.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pandas.pydata.org/docs/user_guide/visualization.html\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
