{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn13OlQLzEft"
      },
      "source": [
        "### Introduction to Pandas\n",
        "\n",
        "Pandas is a Python library used for data manipulation and analysis. Pandas provides a convenient way to analyze and clean data.\n",
        "\n",
        "The Pandas library introduces two new data structures to Python - Series and DataFrame, both of which are built on top of NumPy.\n",
        "\n",
        "### What is Pandas Used for?\n",
        "\n",
        "Pandas is a powerful library generally used for:\n",
        "\n",
        "- Data Cleaning\n",
        "- Data Transformation\n",
        "- Data Analysis\n",
        "- Machine Learning\n",
        "- Data Visualization\n",
        "\n",
        "### Why Use Pandas?\n",
        "\n",
        "Some of the reasons why we should use Pandas are as follows:\n",
        "\n",
        "1. **Handle Large Data Efficiently**\n",
        "\n",
        "   Pandas is designed for handling large datasets. It provides powerful tools that simplify tasks like data filtering, transforming, and merging.\n",
        "\n",
        "   It also provides built-in functions to work with formats like CSV, JSON, TXT, Excel, and SQL databases.\n",
        "\n",
        "2. **Tabular Data Representation**\n",
        "\n",
        "   Pandas DataFrames, the primary data structure of Pandas, handle data in tabular format. This allows easy indexing, selecting, replacing, and slicing of data.\n",
        "\n",
        "3. **Data Cleaning and Preprocessing**\n",
        "\n",
        "   Data cleaning and preprocessing are essential steps in the data analysis pipeline, and Pandas provides powerful tools to facilitate these tasks. It has methods for handling missing values, removing duplicates, handling outliers, data normalization, etc.\n",
        "\n",
        "4. **Time Series Functionality**\n",
        "\n",
        "   Pandas contains an extensive set of tools for working with dates, times, and time-indexed data as it was initially developed for financial modeling.\n",
        "\n",
        "5. **Free and Open-Source**\n",
        "\n",
        "   Pandas follows the same principles as Python, allowing you to use and distribute Pandas for free, even for commercial use.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw2wKp9nzEf6"
      },
      "source": [
        "## ðŸ§  1. Introduction to Pandas\n",
        "\n",
        "* What is Pandas?\n",
        "* Why use it in data science?\n",
        "\n",
        "## ðŸ§ª 2. Installing and Importing Pandas\n",
        "\n",
        "* `pip install pandas`\n",
        "* `import pandas as pd`\n",
        "\n",
        "## ðŸ“Š 3. Pandas Data Structures\n",
        "\n",
        "* Series\n",
        "* DataFrame\n",
        "\n",
        "## ðŸ“ 4. Reading and Writing Files\n",
        "\n",
        "* CSV, Excel, JSON\n",
        "* `pd.read_csv`, `to_csv`, etc.\n",
        "\n",
        "## ðŸ” 5. Exploring Data\n",
        "\n",
        "* `head()`, `tail()`, `info()`, `describe()`\n",
        "\n",
        "## ðŸ”Ž 6. Indexing and Selecting Data\n",
        "\n",
        "* `loc`, `iloc`, `at`, `iat`\n",
        "* Conditional filtering\n",
        "\n",
        "## ðŸ§¹ 7. Data Cleaning\n",
        "\n",
        "* Handling missing values\n",
        "* Changing data types\n",
        "* Renaming columns\n",
        "\n",
        "## ðŸ” 8. Applying Functions\n",
        "\n",
        "* `apply()`, `map()`, `lambda` with Pandas\n",
        "\n",
        "## ðŸ“ˆ 9. GroupBy and Aggregations\n",
        "\n",
        "* `groupby()`, `agg()`, `pivot_table()`\n",
        "\n",
        "## ðŸ”— 10. Merging and Joining\n",
        "\n",
        "* `merge()`, `concat()`, `join()`\n",
        "\n",
        "## ðŸ•³ 11. Handling Missing Data\n",
        "\n",
        "* `isnull()`, `dropna()`, `fillna()`\n",
        "\n",
        "## ðŸ’¼ 12. Real-World Business Example\n",
        "\n",
        "* Sales dataset or Superstore analysis\n",
        "\n",
        "## ðŸ§  13. Assignments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DORXCpXezEf8"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX5TvKTkzEf9"
      },
      "source": [
        "### Install Pandas\n",
        "\n",
        "To install pandas, you need Python and PIP installed on your system. If you have Python and PIP installed already, you can install pandas by entering the following command in the terminal:\n",
        "\n",
        "```bash\n",
        "pip install pandas\n",
        "```\n",
        "\n",
        "If the installation completes without any errors, Pandas is now successfully installed on your system. You can start using it in your Python projects by importing the Pandas library.\n",
        "\n",
        "### Import Pandas in Python\n",
        "\n",
        "We can import Pandas in Python using the import statement:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3WHDJgGzEf-"
      },
      "outputs": [],
      "source": [
        "# pip install pandas\n",
        "# pip install pandas=1.9.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "hveEwgCfzEgB",
        "outputId": "0c5ce512-d24a-41ba-90ba-b0c7004dc259"
      },
      "outputs": [],
      "source": [
        "import pandas\n",
        "pandas.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBy0ejTOzEgF"
      },
      "outputs": [],
      "source": [
        "#pandas>>data manipulation and data wrangling\n",
        "\n",
        "# create Series, DataFrame ,indexing, columns, data types, assign, create new columns\n",
        "\n",
        "# 1.data Load --> different types\n",
        "# 2.data basic Informations\n",
        "# 3. null values check\n",
        "# 3.1 null values remove\n",
        "# 4. data type check and correct it\n",
        "# 5. duplicates value check and remove\n",
        "# 6. remove columns/ or\n",
        "# 7. add columns after calculations\n",
        "# 8. save the clean data\n",
        "# 9. sovle the business questions /data analysis\n",
        "# 10. visualize the data\n",
        "\n",
        "# data cleaning\n",
        "# drop columns,  drop rows, fill missing values, handle outliers, remove duplicates\n",
        "\n",
        "\n",
        "# data transformation\n",
        "# pivot, melt, groupby, sort, sortby, rank, quantile, shift,\n",
        "# data merging\n",
        "# join, merge, concat, append\n",
        "# data analysis\n",
        "# summary statistics, descriptive statistics, correlation, regression, time series analysis\n",
        "# data visualization\n",
        "# plot, scatter plot, bar plot, histogram, box plot, violin plot, heatmap\n",
        "# data export\n",
        "# to_csv, to_excel, to_json, to_pickle, to_sql\n",
        "\n",
        "\n",
        "\n",
        "# A Pandas Series is a one-dimensional labeled array-like object that can hold data of any type.\n",
        "### Labels\n",
        "\n",
        "# The labels in the Pandas Series are\n",
        "# index numbers by default. Like in DataFrame and array, the index number in Series starts from 0.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "appSfEwf1ODV"
      },
      "source": [
        "### Series\n",
        "The pandas Series is a fundamental data structure in the Python pandas library. It is a **one-dimensional** labeled array capable of holding data of any type (integer, string, float, etc.), similar to a single column in a spreadsheet or database table. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPosYOIS1jT1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "a = np.array([1,2,3,4,5])\n",
        "print(a)\n",
        "a.ndim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTSrbnLg2KSX"
      },
      "outputs": [],
      "source": [
        "type(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMoxCpeU1MlW"
      },
      "outputs": [],
      "source": [
        "# series is a 1D array\n",
        "import pandas as pd\n",
        "s = pd.Series([1,2,3,4,5])\n",
        "print(s)\n",
        "s.ndim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Jz7rwM32IWA"
      },
      "outputs": [],
      "source": [
        "type(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnZksCFwzEgI"
      },
      "source": [
        "### Series and dataframe (Indexing & Slicing)\n",
        "##### Series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNI7rl3vzEgJ"
      },
      "outputs": [],
      "source": [
        "# creaate the Series using list\n",
        "import pandas as pd\n",
        "li = [34,3,534,34,23]\n",
        "se = pd.Series(li)\n",
        "# indexing\n",
        "se[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlGwMp9p22Ov"
      },
      "outputs": [],
      "source": [
        "# assign new value\n",
        "se[2] = 45\n",
        "se"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNQOmCpX2u3s"
      },
      "outputs": [],
      "source": [
        "se[2:4]\n",
        "# slicing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0fhzJnW3IA8"
      },
      "outputs": [],
      "source": [
        "se[2:4] =4,5\n",
        "se"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfPJZxALzEgK"
      },
      "outputs": [],
      "source": [
        "# Create a Series and specify labels\n",
        "se2 = pd.Series([12,32,243,45] , index=['a','b','c','d'],)\n",
        "se2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71HNeIsp5F2e"
      },
      "outputs": [],
      "source": [
        "se2['b']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dK6X-DFu5yPG"
      },
      "outputs": [],
      "source": [
        "se2[2]\n",
        "# default indexing always use if you add your custom indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QklnaK1V6PkT"
      },
      "outputs": [],
      "source": [
        "se2[\"d\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# METHODS\n",
        "se2.pop('b')\n",
        "# delete the value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "se2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "se"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9C8YQcDN6zuu"
      },
      "outputs": [],
      "source": [
        "# change the index name using index\n",
        "se.index = ['a','b','c','d','e']\n",
        "se"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "l2 = ['a','x','c','d','e']\n",
        "se.index = l2\n",
        "se"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yY7tOAXw6GTx"
      },
      "outputs": [],
      "source": [
        "# change the index name using set_axis methods\n",
        "se.set_axis(['x','y','z','a','b'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3kxYQY_8GnY"
      },
      "outputs": [],
      "source": [
        "se"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aB1lyCF_7jRC"
      },
      "outputs": [],
      "source": [
        "se = se.set_axis(['x','y','z','a','b']) # reassign for permanent change\n",
        "se"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQniJ7Yp7jCb"
      },
      "outputs": [],
      "source": [
        "se.reset_index() # reset index with default indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkeksg19841R"
      },
      "outputs": [],
      "source": [
        "type(se.reset_index())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D3bEhpnzEgM"
      },
      "source": [
        "## DataFrame /2D/ tabular data\n",
        "A DataFrame is a 2-dimensional, labeled data structure with columns of potentially different types, organized like a spreadsheet or an SQL table. It is a fundamental tool in data analysis for storing, manipulating, and analyzing data, often using the pandas library in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvqE6cTG9iCE"
      },
      "outputs": [],
      "source": [
        "# create a dataframe\n",
        "df = pd.DataFrame()\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[[1,2,3],[4,5,6],[7,8,9]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# use np\n",
        "np.array([[1,2,3],[4,5,6],[7,8,9]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKTWIQoO91Mh"
      },
      "outputs": [],
      "source": [
        "#  dataframe create using array\n",
        "df1 = pd.DataFrame([[1,2,3],[4,5,6],[7,8,9]])\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wYtoKlB-RrM"
      },
      "outputs": [],
      "source": [
        "#  dataframe create using dictionay\n",
        "df2 = pd.DataFrame({\"x\":[1,2,3],\"y\":[4,5,6],\"z\":[7,8,9]})\n",
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnyjHNU6AAG9"
      },
      "outputs": [],
      "source": [
        "df2.set_axis([\"A\",'B','C'],axis=1)\n",
        "# axis =0 means change the index/row name\n",
        "# axis =1 means change the columns name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6vGc3MvAuAk"
      },
      "outputs": [],
      "source": [
        "df2 = df2.set_axis([\"A\",'B','C'])\n",
        "df2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWY7hsaBBLMV"
      },
      "source": [
        "### indexing  slicing in DataFrame\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVv17rYhA2ZP"
      },
      "outputs": [],
      "source": [
        "# indexing columns\n",
        "df2[\"x\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rIuYlMQCMyb"
      },
      "outputs": [],
      "source": [
        "# more then one columns\n",
        "df2[['x','y']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pwy8lc9jDGhv"
      },
      "source": [
        "###  .loc and .iloc \n",
        "In Pandas, .loc and .iloc are two fundamental methods used for selecting data from DataFrames, differing primarily in their indexing approach:\n",
        "* .loc (Label-based indexing):\n",
        "Accesses data by labels (names) of rows and columns.\n",
        "\n",
        "Syntax: df.loc[row_label, column_label]\n",
        "\n",
        "Slicing: When slicing, both the start and end labels are inclusive. For example, df.loc['start_row':'end_row'] will include both 'start\\_row' and 'end\\_row'.\n",
        "Usage: Ideal when you know the specific names of the rows and columns you want to access, or when working with non-integer or custom indices. Can also be used with Boolean arrays for conditional selection.\n",
        "\n",
        "* .iloc (Integer-position based indexing):\n",
        "Accesses data by the integer positions of rows and columns. These positions are 0-indexed, meaning the first row/column is at position 0, the second at position 1, and so on.\n",
        "\n",
        "Syntax: df.iloc[row_position, column_position]\n",
        "\n",
        "Slicing: When slicing, the start position is inclusive, but the end position is exclusive. For example, df.iloc[0:5] will include rows at positions 0, 1, 2, 3, and 4, but not 5.\n",
        "Usage: Useful when you need to select data based on its numerical position within the DataFrame, regardless of the actual labels. It can be faster for very large datasets due to its direct reliance on integer positions.\n",
        "Key Differences Summarized:\n",
        "Feature\n",
        ".loc\n",
        ".iloc\n",
        "Indexing\n",
        "Label-based (using row/column names)\n",
        "Integer-position based (0-indexed)\n",
        "Slicing\n",
        "Inclusive of both start and end labels\n",
        "Exclusive of the end position\n",
        "Flexibility\n",
        "Handles various index types (e.g., strings, dates)\n",
        "Primarily for numerical integer positions\n",
        "Speed\n",
        "Generally slower for large datasets\n",
        "Can be faster for large datasets due to direct position access\n",
        "Example:\n",
        "Python\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = {'col1': [1, 2, 3], 'col2': [4, 5, 6]}\n",
        "df = pd.DataFrame(data, index=['rowA', 'rowB', 'rowC'])\n",
        "\n",
        "# Using .loc\n",
        "print(df.loc['rowB', 'col1'])  # Accesses value at row 'rowB', column 'col1'\n",
        "print(df.loc['rowA':'rowC', 'col1']) # Slices rows from 'rowA' to 'rowC' (inclusive) for 'col1'\n",
        "\n",
        "# Using .iloc\n",
        "print(df.iloc[1, 0])  # Accesses value at row position 1, column position 0\n",
        "print(df.iloc[0:2, 0]) # Slices rows from position 0 to 1 (exclusive of 2) for c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FErn3A4rDGTr"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6e2YMDoB5Wz"
      },
      "outputs": [],
      "source": [
        "# loc and iloc\n",
        "df2\n",
        "# .loc (Label-based indexing):\n",
        "# Syntax: df.loc[row_label, column_label]\n",
        "# Slicing:df.loc['start_row':'end_row']\n",
        "\n",
        "# .iloc (Integer-position based indexing):\n",
        "# Syntax: df.iloc[row_position, column_position]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# return x column --> loc\n",
        "df2.loc[:,'x']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# return x and y column --> loc\n",
        "df2.loc[:,'x':'y']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# return x and y column\n",
        "# iloc\n",
        "df2.iloc[:,0:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get 2 rows from starting and 2 cols from starting  df2 = df2.set_axis([\"A\",'B','C'])\n",
        "df2.loc[:\"B\", 'x':\"y\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df2.loc['A','x':'z'] # only 1 row data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luqqUCLuDk5m"
      },
      "outputs": [],
      "source": [
        "# ge5 the value  4 from Table/df only\n",
        "df2.loc['A',\"y\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cK2MQelELdI"
      },
      "outputs": [],
      "source": [
        "# show 4,7\n",
        "df2.loc['A',\"y\":]\n",
        "# slicing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Attributes\n",
        "df2.ndim\n",
        "df2.shape\n",
        "df2.size\n",
        "df2.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxOw-zV0zEgN"
      },
      "outputs": [],
      "source": [
        "## Methods \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w--StBjJ4c5F"
      },
      "outputs": [],
      "source": [
        "s\n",
        "# 2.data basic Informations\n",
        "# 3. null values check\n",
        "# 3.1 null values remove\n",
        "# 4. data type check and correct it\n",
        "# 5. duplicates value check and remove\n",
        "# 6. remove columns/ or\n",
        "# 7. add columns after calculations\n",
        "# 8. save the clean data\n",
        "# 9. sovle the business questions /data analysis\n",
        "# 10. visualize the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OouDyDszEgP"
      },
      "source": [
        "#### Load the Data from Files\n",
        "\n",
        "csv, xls, website, database, cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### 1.data Load --> different type\n",
        "pd.read_csv('Netflix.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBAIQOM74t_L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# read the csv filef\n",
        "f1 = pd.read_csv(r'C:\\Users\\hp\\Documents\\ds_materials\\5.0_python_6.libraries\\python_libraries\\2.pandas\\extra\\historical_automobile_sales.csv')\n",
        "f1.head() # starting  5 rows "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "f1.head(10) # starting  10 rows "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j54wrnxv6zL8"
      },
      "outputs": [],
      "source": [
        "!pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install xlrd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-JdfEue5pQ3"
      },
      "outputs": [],
      "source": [
        "# read excel file\n",
        "f2 = pd.read_excel(r\"c:\\Users\\hp\\Documents\\ds_materials\\5.0_python_6.libraries\\python_libraries\\Sample - Superstore.xls\")\n",
        "f2.head(20) # pass a number for show the 20 rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGyP38u3zEgQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# load Diffrent Files from csv url \n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PpEvKwK7wdi"
      },
      "outputs": [],
      "source": [
        "# read html table data\n",
        "!pip install lxml\n",
        "import lxml\n",
        "url_df = pd.read_html(\"https://www.basketball-reference.com/leagues/NBA_2015_totals.html\")\n",
        "url_df\n",
        "type(url_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg9d9L_I8UOT"
      },
      "outputs": [],
      "source": [
        "len(url_df) # 2 table in url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdkEqxsE8YZ6"
      },
      "outputs": [],
      "source": [
        "# show\n",
        "url_df[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaXTHkGO-KZL"
      },
      "outputs": [],
      "source": [
        "# save\n",
        "url_df[0].to_csv(\"players.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUaC_y8w9WfE"
      },
      "outputs": [],
      "source": [
        "# read json file\n",
        "df = pd.read_json(\"https://api.github.com/repos/pandas-dev/pandas/issues\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeA7OUB8_1-Y"
      },
      "outputs": [],
      "source": [
        "# database connect use\n",
        "# 1.mysql-connector\n",
        "# 2.sqlalchemy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iM0CeuxdAKBu"
      },
      "outputs": [],
      "source": [
        "# mysql connector\n",
        "!pip install mysql-connector-python\n",
        "import mysql.connector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBuwkpLfzEgR"
      },
      "outputs": [],
      "source": [
        "#Create the connection object\n",
        "myconn = mysql.connector.connect(host = \"127.0.0.1\", user = \"root\",\n",
        "                                 passwd = \"root\" ,database = \"mavenmovies\")\n",
        "\n",
        "#creating the cursor object\n",
        "cur = myconn.cursor()\n",
        "try:\n",
        "    #Reading the Employee data\n",
        "    cur.execute(\"select * from Actor\")\n",
        "\n",
        "    #fetching the rows from the cursor object\n",
        "    result = cur.fetchall()\n",
        "    #printing the result\n",
        "\n",
        "    for x in result:\n",
        "        print(x)\n",
        "except:\n",
        "    myconn.rollback()\n",
        "\n",
        "myconn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJxD9Y1AzEga"
      },
      "source": [
        "### Data Explore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzzWASx21N9q"
      },
      "source": [
        "## Use Titanic dataset\n",
        "\n",
        "https://www.kaggle.com/competitions/titanic/data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz6GF-191qjz"
      },
      "source": [
        "\n",
        "# 1.data Load --> different types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "7lDaxYwiHTKO",
        "outputId": "70084618-7bda-4698-bc81-406c6e3be71d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "df.head() # show the starting 5 rows\n",
        "df.head(10) # show only 10 rows\n",
        "\n",
        "# bottom data\n",
        "df.tail() # show the last 5 rows\n",
        "df.tail(10) # show only 10 rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQBWSY6f1wgv"
      },
      "source": [
        "# 2.data basic Informations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68tpbkqhHx8D"
      },
      "outputs": [],
      "source": [
        "# shape, size ,ndim\n",
        "print(df.shape)\n",
        "print(df.size)\n",
        "print(df.ndim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vadvTJWbIA-0"
      },
      "outputs": [],
      "source": [
        "# show the only names of table\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkE59r_bINMx"
      },
      "outputs": [],
      "source": [
        "df.dtypes # show the data -->datatyps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcRyXcztJ32m"
      },
      "outputs": [],
      "source": [
        "# basic info about data\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F41C6wSbK8Y4"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcDP7hTFKVFe"
      },
      "outputs": [],
      "source": [
        "# statistical summary about data\n",
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3udjXiRHAg1j"
      },
      "outputs": [],
      "source": [
        "df.describe(include = 'object')\n",
        "df.describe(include = 'all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4l1F-bNOp9v"
      },
      "outputs": [],
      "source": [
        "df.sample()\n",
        "# return a random value from df\n",
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRD7gw1wPIus"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHGtTDnxO7_Y"
      },
      "outputs": [],
      "source": [
        "# indxing and slcing\n",
        "df[\"Name\"] # only show the name column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME5QWxrKPOSA"
      },
      "outputs": [],
      "source": [
        "# show the name and fare columns\n",
        "df[[\"Name\",\"Fare\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deahH1PWPkJ1"
      },
      "outputs": [],
      "source": [
        "# show the value from name  to fare\n",
        "df.loc[:,\"Name\":\"Fare\"]\n",
        "# show the starting 10 passanger details  from name to fare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3plB4r5AQGzm"
      },
      "outputs": [],
      "source": [
        "# show the data from row 10 to 20\n",
        "\n",
        "# show the data from row 30 to 35 and show only name and fare\n",
        "\n",
        "\n",
        "#Access df rows #implicit index>internal/integer index and explicit index/named>>define\n",
        "df[0:100]\n",
        "df.iloc[0:2] #start from 0 and go to 1\n",
        "df.loc[0:2] #give me the rows whose name is 0, 1, 2\n",
        "\n",
        "#loc will go with named indexes, iloc will go with inbuilt index\n",
        "df.iloc[0:2, ['Name', 'Sex', 'Age']] #it will throw an error, why?\n",
        "df.loc[0:2, ['Name', 'Sex', 'Age']]\n",
        "\n",
        "df\n",
        "df.iloc[0:2, 3:6]\n",
        "list(df['Name'][2:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mS0vJy_b12_U"
      },
      "source": [
        "# 3. null values check\n",
        "# 3.1 null values remove"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHZoSslKHDkx"
      },
      "outputs": [],
      "source": [
        "df[\"Age\"].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaY0VfPu16Eu"
      },
      "outputs": [],
      "source": [
        "#  isna , isnull ,fillna\n",
        "df.isna().sum() # check null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYgWhDrTG3No"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YUOS8LwHC9y"
      },
      "outputs": [],
      "source": [
        "# if null values is more then 50% we should drop the columns\n",
        "df.drop('Cabin',axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXxGncLBIHvG"
      },
      "outputs": [],
      "source": [
        "# for permanenet change\n",
        "# 1. reassign\n",
        "# df = df.drop('Cabin',axis=1)\n",
        "\n",
        "# 2. use inplace parameter\n",
        "df.drop('Cabin',axis=1,inplace=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M90MssNAI9KF"
      },
      "source": [
        "#### Replace the null value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-lQ9_CGI8OU"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzFJmjkYIHki"
      },
      "outputs": [],
      "source": [
        "df[\"Age\"].mean()  # find the mean\n",
        "df[\"Age\"].median()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgO_MFKOKATm"
      },
      "outputs": [],
      "source": [
        "# pip install matplotlib\n",
        "# pip install seaborn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# create boxplot plt use\n",
        "sns.boxplot(df[\"Age\"])\n",
        "plt.show()\n",
        "# check the outlier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtlIbzEVNENv"
      },
      "outputs": [],
      "source": [
        "# warning remove\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a68wshZbLMF-"
      },
      "outputs": [],
      "source": [
        "# filna\n",
        "df[\"Age\"].fillna(29)\n",
        "\n",
        "df[\"Age\"].fillna(df[\"Age\"].mean()) # only temperory change\n",
        "df[\"Age\"].fillna(df[\"Age\"].mean(), inplace=True)\n",
        "\n",
        "# df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-xgEIBHP8qM"
      },
      "outputs": [],
      "source": [
        "# categorical data replace\n",
        "# mode\n",
        "df[\"Embarked\"].mode()[0]\n",
        "\n",
        "df[\"Embarked\"] = df[\"Embarked\"].fillna(df[\"Embarked\"].mode()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIZHwcABNWR2"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxWpbSeYORwf"
      },
      "outputs": [],
      "source": [
        "# remove null values only in row wise\n",
        "df.dropna(axis=0,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLin82ULOfLq"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkEOe_-iPgXL"
      },
      "source": [
        "5. duplicates value check and remove"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18xXDY-SPCN6"
      },
      "outputs": [],
      "source": [
        "# check the duplicate value\n",
        "df.duplicated().sum()\n",
        "\n",
        "# remove the duplicate value\n",
        "df.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jatN8Q2IPcvF"
      },
      "source": [
        "4. data type check and correct it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEX_XJDOPtj5"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xGVWAr1PoCT"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XD7hfzhwQk3f"
      },
      "outputs": [],
      "source": [
        "# change the data types --> using as astype\n",
        "df[\"PassengerId\"] =  df[\"PassengerId\"].astype(\"object\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84jBM6zIRC6U"
      },
      "outputs": [],
      "source": [
        "# 7. add columns\n",
        "df['age'] =0\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAxwh3QSUQAP"
      },
      "outputs": [],
      "source": [
        "df[\"Fare\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "we1QEKaeT188"
      },
      "outputs": [],
      "source": [
        "# using\n",
        "df[\"Fare\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L81TH1yxTc8P"
      },
      "outputs": [],
      "source": [
        "d =df[\"Fare\"][3]\n",
        "print(d)\n",
        "if d>100:\n",
        "  print(\"premium\")\n",
        "elif d>50:\n",
        "  print(\"standard\")\n",
        "else:\n",
        "  print(\"economy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIyEhkjDUt-r"
      },
      "outputs": [],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RT8yo9NAUx6i"
      },
      "outputs": [],
      "source": [
        "df.head(65)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FaerjOrPUfLW"
      },
      "outputs": [],
      "source": [
        "# custom list for update / create the links\n",
        "# df.reset_index(inplace = True)\n",
        "df[\"type\"] = 0\n",
        "for  i in range(len(df)):\n",
        "  try:\n",
        "    print(df[\"Fare\"][i])\n",
        "    if df[\"Fare\"][i]>100:\n",
        "      df[\"type\"][i] =\"premium\"\n",
        "\n",
        "      print(\"premium\")\n",
        "    elif df[\"Fare\"][i]>50:\n",
        "      print(\"standard\")\n",
        "      df[\"type\"][i] =\"standard\"\n",
        "    else:\n",
        "      print(\"economy\")\n",
        "      df[\"type\"][i] =\"economy\"\n",
        "  except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igTGjHZAXBlE"
      },
      "outputs": [],
      "source": [
        "# apply -->condtions as a functions\n",
        "def fare_type(x):\n",
        "  if x>100:\n",
        "    return \"premium\"\n",
        "  elif x>50:\n",
        "    return \"standard\"\n",
        "  else:\n",
        "    return \"economy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwWdxW3vXMKq"
      },
      "outputs": [],
      "source": [
        "df[\"customer_type\"] = df[\"Fare\"].apply(fare_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7cXh_fsVyW9"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3reWTpvQ9AG"
      },
      "outputs": [],
      "source": [
        "# 6. remove columns\n",
        "df = df.drop(\"type\",axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yviK9jKX9gM"
      },
      "outputs": [],
      "source": [
        "# save the clean data\n",
        "df.to_csv(\"cleantitanic_data.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0h4ucInHZBZi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGKj8gf1YPNO"
      },
      "outputs": [],
      "source": [
        "# laod\n",
        "df = pd.read_csv(\"cleantitanic_data.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG2OHMvmlVRQ"
      },
      "source": [
        "# 9. sovle the business questions /data analysis\n",
        " 0. Boolean\n",
        " 1. group by\n",
        " 2. aggrigate\n",
        " 3. sorting\n",
        " 4. reset index  \n",
        " 5. Rename columns\n",
        " 6. handle Error value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UUCRDYTMBz5"
      },
      "source": [
        "## Boolean Indexing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9J_x2a9rMYCp"
      },
      "outputs": [],
      "source": [
        "# age\n",
        "# conditions\n",
        "df['Age'] >60\n",
        "# and it return true or false"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cr0zk0MsM1hz"
      },
      "outputs": [],
      "source": [
        "# show the passanger details of seniors 60\n",
        "# df[pass boolean conditions as index]\n",
        "# df[index]\n",
        "df[df['Age'] >60]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dt_8hpgYOA5X"
      },
      "outputs": [],
      "source": [
        "# show the only Age columns\n",
        "df[\"Age\"][df['Age'] >60]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PrwNnnhOiDo"
      },
      "outputs": [],
      "source": [
        "# show the name and age of passanger who is senior\n",
        "df[[\"Name\",\"Age\"]][df['Age'] >60]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VTkJTzjOsOL"
      },
      "outputs": [],
      "source": [
        "# Q.find the max fare\n",
        "df[\"Fare\"].max()\n",
        "# Q.show the details of the passanger who pay the maximum fare\n",
        "\n",
        "df[\"Fare\"] == df[\"Fare\"].max() # condition\n",
        "\n",
        "# indexing\n",
        "df[ df[\"Fare\"] == df[\"Fare\"].max() ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2286eI1RCVq"
      },
      "outputs": [],
      "source": [
        "# count the total number of teenager <18\n",
        "df[\"Age\"]<18 # conditions\n",
        "\n",
        "len(df[df[\"Age\"]<18])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBNoyNRZSXyZ"
      },
      "outputs": [],
      "source": [
        "# count the total number of female teenager\n",
        "# multiple conditions\n",
        "# boolean (0,1) and or not ,--> bitwise and or not(& ,| ,!)\n",
        "\n",
        "# 1st conditions\n",
        "df[\"Age\"]<18\n",
        "# 2nd conditions\n",
        "df[\"Sex\"] == \"female\"\n",
        "\n",
        "(df[\"Age\"]<18) & (df[\"Sex\"] == \"female\")\n",
        "\n",
        "df[(df[\"Age\"]<18) & (df[\"Sex\"] == \"female\")]\n",
        "\n",
        "len(df[(df[\"Age\"]<18) & (df[\"Sex\"] == \"female\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk2RZ7zCVIwS"
      },
      "outputs": [],
      "source": [
        "# find the number of passanger who pay more then average fare\n",
        "df[\"Fare\"].mean()\n",
        "df[\"Fare\"] > df[\"Fare\"].mean()\n",
        "len(df[df[\"Fare\"] > df[\"Fare\"].mean()])\n",
        "\n",
        "# find the passager who pay the max and servive\n",
        "len(df[df[\"Fare\"]== df[\"Fare\"].max()])\n",
        "df[df[\"Fare\"]== df[\"Fare\"].max()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3Ae2GDwYKVc"
      },
      "outputs": [],
      "source": [
        "# find the passager who pay the minimum and servive\n",
        "len(df[df[\"Fare\"]== df[\"Fare\"].min()])\n",
        "df[df[\"Fare\"]== df[\"Fare\"].min()]\n",
        "\n",
        "df[\"Fare\"]== df[\"Fare\"].min()\n",
        "df[\"Survived\"]==1\n",
        "df[(df[\"Fare\"]== df[\"Fare\"].min()) & (df[\"Survived\"]==1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJIclUuPYwZj"
      },
      "outputs": [],
      "source": [
        "# methods\n",
        "# unique()\n",
        "df[\"Embarked\"].unique()  # show the distinct values from df\n",
        "# nunique()\n",
        "df[\"Embarked\"].nunique() # show the distinct values count from df\n",
        "# value_counts()\n",
        "df[\"Embarked\"].value_counts() # show the distinct values and count from df\n",
        "df[\"Survived\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aMcY0mgLwSm"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Rename columns\n",
        "df.rename(columns={'A': 'Age'}, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfYew82jaAcM"
      },
      "source": [
        "#### 1.group by\n",
        "\n",
        " The groupby() method splits a DataFrame or Series into groups based on the unique values in one or more specified columns\n",
        "\n",
        " df.groupby('column name')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yj0swCEeaijV"
      },
      "outputs": [],
      "source": [
        "df.groupby(\"Survived\")[\"Fare\"].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhTQyW6nbQ4a"
      },
      "outputs": [],
      "source": [
        "# total fare pay by male and female\n",
        "# embarrked wise sales\n",
        "\n",
        "# multiple columns for groupings\n",
        "# count the number  survive or not survive in each gender\n",
        "df.groupby([\"Sex\",\"Survived\"]).count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVO9_qHBc23G"
      },
      "outputs": [],
      "source": [
        "df.groupby([\"Sex\",\"Survived\"])[\"PassengerId\"].count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "KfU87BRmcm1R",
        "outputId": "62c0194a-62af-4310-93f2-339d83de8c64"
      },
      "outputs": [],
      "source": [
        "# Aggregation: Calculating a summary statistic for each group (e.g., sum(), mean(), count(), min(), max()).\n",
        "df.groupby([\"Survived\"])[\"Fare\"].aggregate([\"mean\",'sum','count','min','max'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FRLOvUOeM78"
      },
      "outputs": [],
      "source": [
        "df.groupby([\"Survived\",\"Embarked\"])[\"Fare\"].count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYO7LmTamIGR"
      },
      "source": [
        "### reset_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9F0ou7IfMnv"
      },
      "outputs": [],
      "source": [
        "xyz = df.groupby([\"Survived\",\"Embarked\"])[\"Fare\"].count().reset_index()\n",
        "\n",
        "# change the columns name\n",
        "# df.rename(columns={'A': 'Age'}, inplace=True)\n",
        "xyz.rename(columns={'Fare': 'count'}, inplace=True)\n",
        "xyz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDJDNG-WhdaJ"
      },
      "source": [
        "## Sortings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDPUvhffhcMX"
      },
      "outputs": [],
      "source": [
        "xyz.sort_values(by = \"count\",ascending = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "0dIyyBIOhuit",
        "outputId": "bd7109f9-97fe-4ffe-8a5f-56709f67e85e"
      },
      "outputs": [],
      "source": [
        "# sort by index\n",
        "xyz.sort_index()\n",
        "xyz.sort_index(ascending = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtukYzWFiUqj"
      },
      "source": [
        "### handle Error value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBOiW6M_iYZq"
      },
      "outputs": [],
      "source": [
        "# 2. Handle Error Values\n",
        "df1 = pd.DataFrame({\n",
        "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
        "    'Score': [85, 'error', 92]\n",
        "})\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gExq21wvii_U"
      },
      "outputs": [],
      "source": [
        "pd.to_numeric(df1['Score'], errors='coerce')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VvQq8DliH7h"
      },
      "outputs": [],
      "source": [
        "# Replace 'error' with NaN and convert column to numeric\n",
        "df1['Score'] = pd.to_numeric(df1['Score'], errors='coerce')\n",
        "df1\n",
        "# When errors='coerce' is specified in these functions, it dictates how the function handles values that cannot be successfully converted to the target data type. Instead of raising an error and stopping the execution (which is the default behavior, errors='raise'), errors='coerce' will convert any unparseable or invalid values to NaN (Not a Number) for numeric types or NaT (Not a Time) for datetime/timedelta types.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6Y4-WhZkKkr"
      },
      "source": [
        "## ðŸ”¹ Business Scenario 1: Passenger Demographics\n",
        "\n",
        "**Goal:** Understand the customer base to tailor safety and comfort services.\n",
        "\n",
        "**Questions:**\n",
        "\n",
        "1. What is the **average age** of passengers on the Titanic?\n",
        "2. What **percentage of passengers** were **male vs female**?\n",
        "3. Which **port (Embarked)** contributed the **most passengers**?\n",
        "4. How many passengers were **children (age < 18)**, **adults (18â€“60)**, and **seniors (60+)**?\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Business Scenario 2: Survival Analysis\n",
        "\n",
        "**Goal:** Identify which groups were more likely to survive to improve future safety protocols.\n",
        "\n",
        "**Questions:**\n",
        "\n",
        "1. What was the **overall survival rate**?\n",
        "2. Which **gender** had a higher survival rate?\n",
        "3. How did survival rates differ by **passenger class (Pclass)**?\n",
        "4. Did **children** have better survival chances than adults?\n",
        "5. Which **port of embarkation (Embarked)** had the highest survival rate?\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Business Scenario 3: Revenue Insights\n",
        "\n",
        "**Goal:** Understand ticket pricing patterns and potential revenue drivers.\n",
        "\n",
        "**Questions:**\n",
        "\n",
        "1. What was the **average fare per passenger class**?\n",
        "2. Is there a correlation between **fare** and **survival**?\n",
        "3. Which **port** generated the **highest total fare revenue**?\n",
        "4. Identify **top 10 paying customers** (by fare).\n",
        "5. Did **family size** (SibSp + Parch) affect fare price or survival?\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Business Scenario 4: Customer Segmentation\n",
        "\n",
        "**Goal:** Segment passengers to target different customer profiles.\n",
        "\n",
        "**Questions:**\n",
        "\n",
        "1. Create a new column `FamilySize = SibSp + Parch + 1`.\n",
        "\n",
        "   * How many passengers traveled **alone** vs **with family**?\n",
        "2. Compare **survival rates** between solo travelers and those with family.\n",
        "3. Find the **average fare and survival rate** for each combination of (`Pclass`, `Sex`).\n",
        "4. Cluster passengers into groups based on **Age**, **Fare**, and **Pclass** (optional, for advanced users).\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Business Scenario 5: Predictive Indicators (Exploratory)\n",
        "\n",
        "**Goal:** Identify key variables influencing survival for a predictive model.\n",
        "\n",
        "**Questions:**\n",
        "\n",
        "1. Which features show the **strongest correlation** with survival?\n",
        "2. Create a pivot table showing **mean survival rate** by `Pclass` and `Sex`.\n",
        "3. Which combination of `Pclass` and `Embarked` had the **lowest survival rate**?\n",
        "4. If the company were to build a new ship, what **demographic group** should they ensure receives better safety measures?\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ’¡ Bonus: Data Cleaning & Preparation Tasks\n",
        "\n",
        "To mimic real-world work, try:\n",
        "\n",
        "1. Handle **missing values** in `Age`, `Cabin`, and `Embarked`.\n",
        "2. Create **bins** for age groups (Child, Teen, Adult, Senior).\n",
        "3. Extract **titles** (Mr, Mrs, Miss, etc.) from the `Name` column â€” analyze survival by title.\n",
        "4. Encode categorical columns (`Sex`, `Embarked`) for modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wizKV5HTZKHV"
      },
      "outputs": [],
      "source": [
        "# merge\n",
        "# join\n",
        "# concat /vstack/hstack\n",
        "# pivot\n",
        "# unpivot/melt\n",
        "# convert to json /json dataframe\n",
        "# cut"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlKjDIjzWNHg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOuy4N-EYI8j"
      },
      "source": [
        "### 1. Merge -->table join"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbBhLlEQXmeU",
        "outputId": "056737ad-934d-4b5d-c821-4d724f1c956c"
      },
      "outputs": [],
      "source": [
        "# student details table\n",
        "student = pd.DataFrame({\n",
        "    'ID': [1, 2, 3],\n",
        "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
        "    'Score': [85, 90, 95]\n",
        "})\n",
        "\n",
        "# subject details table\n",
        "sub = pd.DataFrame({\n",
        "    'ID': [2, 3, 4],\n",
        "    'Subject': ['Math', 'English', 'Science'],\n",
        "    'Grade': ['A', 'B', 'A'],\n",
        "    'Name': ['Alice', 'Bob', 'Charlie']\n",
        "})\n",
        "print(student)\n",
        "print(\"==============\")\n",
        "print(sub)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2Kbqtn5YVsx"
      },
      "outputs": [],
      "source": [
        "# merge  use with pandas univeral funcions\n",
        "# syntax\n",
        "# pd.merge(left, right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, suffixes=('_x', '_y'))\n",
        "# select 8 from t1 join t2 on t1.col1 = t2.col1\n",
        "pd .merge(student, sub, on='ID', how='inner',suffixes=(\"_std\",\"sub\"))\n",
        "#\n",
        "merged = pd.merge(student, sub, on='ID', how='left', suffixes=('_std', '_sub'))\n",
        "print(merged)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8zhRfEabUuS"
      },
      "outputs": [],
      "source": [
        "# 2.join use with variable  --> default using index as a common column\n",
        "student.join(sub,how='left',lsuffix='_std', rsuffix='_sub')\n",
        "# show the join as index values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGMLQIbYcg6I"
      },
      "outputs": [],
      "source": [
        "# Join: Using set_index() and join()\n",
        "df1 = student.set_index('ID')\n",
        "df2 = sub.set_index('ID')\n",
        "df2\n",
        "joined_df = df1.join(df2, how='left',lsuffix='_std', rsuffix='_sub')\n",
        "print(joined_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqvGG_Yqd0wK"
      },
      "source": [
        "### concatincaton // vstack/hstack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "RxTz1uXYd8ey",
        "outputId": "b576ec8a-97de-44ea-94cd-f278aa8e8fe2"
      },
      "outputs": [],
      "source": [
        "pd.concat([student, sub], axis=0, ignore_index=True)\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBr57_V1d8PF",
        "outputId": "1855f8d8-0d0b-4872-ad15-2eef91d6bcc0"
      },
      "outputs": [],
      "source": [
        "pd.concat([student, sub], axis=1)\n",
        "#\n",
        "# pd.concat([df1, df2], axis=1) (hstack)\n",
        "# pd.concat([df1, df2], axis=0) (vstack)\n",
        "\n",
        "# import numpy as np\n",
        "# np.stack([df1, df2], axis=1) ## shape must be\n",
        "# np.concatenate([df1, df2], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US36wmxGhmhV"
      },
      "source": [
        "### Pivot table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "collapsed": true,
        "id": "reHiy2MbhqzJ",
        "outputId": "fa07bc0d-5e42-4c17-bd7a-a350d95e130f"
      },
      "outputs": [],
      "source": [
        "# filter, row, column ,values\n",
        "# pivot table\n",
        "pivot_data = pd.DataFrame({\n",
        "    'ID': [1, 1, 2, 2],\n",
        "    'Subject': ['Math', 'Science', 'Math', 'Science'],\n",
        "    'Score': [88, 92, 80, 85]\n",
        "})\n",
        "pivot_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aVU_I8k-iMpX",
        "outputId": "e35a91c6-5172-4277-cc24-07116962bb57"
      },
      "outputs": [],
      "source": [
        "pivot_table= pivot_data.pivot(index='ID', columns='Subject', values='Score')\n",
        "# index =row, columns =column ,values =value\n",
        "print(pivot_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "collapsed": true,
        "id": "GtzaLUc_iwQu",
        "outputId": "94361af6-96c7-4ef9-f8f8-4f7e7b5c0c41"
      },
      "outputs": [],
      "source": [
        "# unpivot /melt\n",
        "d = pivot_table.reset_index()\n",
        "pd.melt(d, id_vars=['ID'], value_vars=['Math', 'Science'], var_name='Subject', value_name='Score')\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkbgaBYzf6vB"
      },
      "outputs": [],
      "source": [
        "# Definitions and Use Cases:\n",
        "# ------------------------------------------------------------\n",
        "# concat: Combines DataFrames either vertically (row-wise) or horizontally (column-wise).\n",
        "#   Use case: Combine data from multiple CSVs or append new data.\n",
        "# merge: Combines DataFrames based on common columns (similar to SQL joins).\n",
        "#   Use case: Merge customer info with transaction data.\n",
        "# join: Joins DataFrames using their index.\n",
        "#   Use case: Combine metadata indexed by unique IDs.\n",
        "# pivot: Reshapes data by turning unique values in a column into new columns.\n",
        "#   Use case: Create summary tables like Excel pivot tables.\n",
        "# melt (unpivot): Converts wide-format data into long-format.\n",
        "#   Use case: Prepare data for analysis/visualization by tidying it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3gDxSmakRhK"
      },
      "outputs": [],
      "source": [
        "# cut method\n",
        "# used for segmenting and sorting data values into discrete intervals,\n",
        "# or \"bins.\" It is particularly useful for transforming a continuous numerical variable into a categorical variable.\n",
        "# Create a sample DataFrame\n",
        "data = {'Score': [65, 75, 88, 92, 55, 70, 83, 95]}\n",
        "df_scores = pd.DataFrame(data)\n",
        "\n",
        "# Define bins and labels\n",
        "bins = [0, 60, 70, 80, 90, 100]\n",
        "labels = ['Fail', 'Pass', 'Good', 'Very Good', 'Excellent']\n",
        "\n",
        "# Use pd.cut to categorize scores\n",
        "df_scores['Category'] = pd.cut(df_scores['Score'], bins=bins, labels=labels, right=True)\n",
        "\n",
        "print(df_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fev99MqanrUJ"
      },
      "source": [
        "# DateTime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "jOj5twltmAMj",
        "outputId": "436b47a8-e6e8-4ab1-eb70-4dd0397e3c73"
      },
      "outputs": [],
      "source": [
        "# DateTime\n",
        "df = pd.DataFrame({\"date\": ['2024-02-08', '2024-02-09', '2024-02-10']})\n",
        "df.dtypes\n",
        "# change the data types\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df.dtypes\n",
        "df[\"date\"].dt.month\n",
        "\n",
        "#\n",
        "df[\"months\"] =  df[\"date\"].dt.month\n",
        "df[\"days\"] =  df[\"date\"].dt.day\n",
        "df[\"year\"] =  df[\"date\"].dt.year\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91WbAfLlpBBw"
      },
      "source": [
        "## dataframe to json\n",
        "\n",
        "json to dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YxosE5DpI8o",
        "outputId": "a9c50dcf-a1ca-447c-9443-1faa68b08e93"
      },
      "outputs": [],
      "source": [
        "# dataframe to json\n",
        "df.to_json(\"data.json\")\n",
        "#\n",
        "print(df.to_json())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37ADMMMCppud",
        "outputId": "28ac820b-5474-478c-c03d-b80b666777ed"
      },
      "outputs": [],
      "source": [
        "df.to_json(\"data.json\", orient=\"records\")\n",
        "\n",
        "print(df.to_json( orient=\"records\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "vDpoHAqKrq5W",
        "outputId": "b4c81aad-8e13-4d7c-b22e-7dfbca59c2ce"
      },
      "outputs": [],
      "source": [
        "# json to dataframe\n",
        "df1 = pd.read_json(\"data.json\")\n",
        "df1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wQ0_BOK16zn"
      },
      "source": [
        "\n",
        "\n",
        "# 10. visualize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e12dfc5"
      },
      "outputs": [],
      "source": [
        "# Visualizations in pandas\n",
        "df1[\"date\"].plot(kind='line')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRcBUAEvTA8U"
      },
      "source": [
        "### Extra Topics or parameter(Optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMlsi4v-TaHs"
      },
      "source": [
        "Why use pd.Categorical?\n",
        "\n",
        "Categorical data saves memory and makes grouping / sorting / modeling more efficient.\n",
        "\n",
        "1ï¸âƒ£ Memory efficiency\n",
        "\n",
        "If you have millions of rows and only 3 unique values (1, 2, 3), pandas internally stores them as integer codes instead of full numeric or string copies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yNb0Wi9zEgb"
      },
      "outputs": [],
      "source": [
        "# pd.Categorical() converts a Pandas Series (or list-like object) into a Categorical data type â€” a special kind of data structure\n",
        "#  in pandas that represents discrete fixed values (categories) instead of plain numeric or string data.\n",
        "pd.Categorical(df['Pclass'])\n",
        "\n",
        "# deep copy shallow copy\n",
        "df3= df\n",
        "df2 = df.copy()   # copy the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sB3UAoZzEgi"
      },
      "outputs": [],
      "source": [
        "df.groupby('Survived').mean(numeric_only=True) #for recent pandas version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iNEOm9LzEgm"
      },
      "outputs": [],
      "source": [
        "df.groupby('Survived').describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjSs8y0MzEgo"
      },
      "outputs": [],
      "source": [
        "# # Sorting by column \"Population\"\n",
        "df.sort_values(by=['Fare'], ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0yRJXfvzEgr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
        "result = df.groupby('Survived')[numeric_columns].aggregate([np.min, 'max', 'mean', 'median', 'count', 'var'])\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTeNLKt_zEgt"
      },
      "outputs": [],
      "source": [
        "#Q what is the average fare paid by people who survived?\n",
        "#groupby >> https://www.w3resource.com/python-exercises/pandas/groupby/index.php\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "df\n",
        "#To convert the result to dataframe\n",
        "df.groupby(['Sex', 'Pclass'])['Survived'].sum().to_frame()\n",
        "\n",
        "df.groupby(['Sex', 'Pclass'])['Survived'].sum().unstack()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNW_449uzEgt"
      },
      "source": [
        "## hstack,unstack,to_frame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98_eZUMjzEhG"
      },
      "outputs": [],
      "source": [
        "# # Sorting by column \"Population\"\n",
        "df.sort_values(by=['Population'], ascending=False)\n",
        "# Sorting by columns \"Country\" and then \"Continent\"\n",
        "df.sort_values(by=['Country', 'Continent'])\n",
        "# Sorting by columns \"Country\" in descending\n",
        "# order and then \"Continent\" in ascending order\n",
        "df.sort_values(by=['Country', 'Continent'],\n",
        "               ascending=[False, True])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRkmvNGJzEhK"
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Load Titanic dataset\n",
        "titanic = sns.load_dataset('titanic')\n",
        "titanic.head()\n",
        "\n",
        "## ðŸ”¹ Part 1: Grouping and Aggregation\n",
        "\n",
        "### âœ… Use Case: Find average age of passengers grouped by gender and class\n",
        "\n",
        "```python\n",
        "# Group by sex and class, then find average age\n",
        "grouped = titanic.groupby(['sex', 'class'])['age'].mean().reset_index()\n",
        "print(grouped)\n",
        "```\n",
        "\n",
        "### âœ… Use Case: Count passengers in each class\n",
        "\n",
        "```python\n",
        "# Count of passengers in each class\n",
        "class_counts = titanic['class'].value_counts()\n",
        "print(class_counts)\n",
        "```\n",
        "\n",
        "### âœ… Use Case: Survival rate by gender\n",
        "\n",
        "```python\n",
        "# Mean of survived (1 = survived, 0 = did not survive)\n",
        "survival_rate = titanic.groupby('sex')['survived'].mean().reset_index()\n",
        "print(survival_rate)\n",
        "```\n",
        "\n",
        "### âœ… Use Case: Multiple Aggregations\n",
        "\n",
        "```python\n",
        "# Aggregate age with multiple functions\n",
        "agg_stats = titanic.groupby('class')['age'].agg(['mean', 'min', 'max', 'count']).reset_index()\n",
        "print(agg_stats)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Part 2: Merging and Joining DataFrames\n",
        "\n",
        "Letâ€™s create two example DataFrames from the Titanic dataset for merging and joining.\n",
        "\n",
        "### âœ… Step 1: Create two dataframes\n",
        "\n",
        "```python\n",
        "# Selecting relevant columns\n",
        "df1 = titanic[['survived', 'sex', 'age']].iloc[:10]\n",
        "df2 = titanic[['age', 'fare']].iloc[:10]\n",
        "1\n",
        "# Add an ID column for joining\n",
        "df = df1.reset_index().rename(columns={'index': 'id'})\n",
        "df2 = df2.reset_index().rename(columns={'index': 'id'})\n",
        "```\n",
        "\n",
        "### âœ… Merge: Inner Join on `id`\n",
        "\n",
        "```python\n",
        "merged_inner = pd.merge(df1, df2, on='id', how='inner')\n",
        "print(merged_inner)\n",
        "```\n",
        "\n",
        "### âœ… Merge: Left Join\n",
        "\n",
        "```python\n",
        "merged_left = pd.merge(df1, df2, on='id', how='left')\n",
        "print(merged_left)\n",
        "```\n",
        "\n",
        "### âœ… Join: Using `set_index()` and `join()`\n",
        "\n",
        "```python\n",
        "df1_indexed = df1.set_index('id')\n",
        "df2_indexed = df2.set_index('id')\n",
        "\n",
        "joined_df = df1_indexed.join(df2_indexed, how='inner')\n",
        "print(joined_df)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  Summary\n",
        "\n",
        "| Task                  | Function                              |\n",
        "| --------------------- | ------------------------------------- |\n",
        "| Grouping by column(s) | `groupby()`                           |\n",
        "| Aggregation           | `agg()`, `mean()`, `sum()`, `count()` |\n",
        "| Merge two dataframes  | `pd.merge()`                          |\n",
        "| Join on index         | `df1.join(df2)`                       |\n",
        "\n",
        "---\n",
        "\n",
        "Would you like this in a **teaching format with assignments** or a **Jupyter Notebook**?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YngE-3XizEhO"
      },
      "source": [
        "Great! Let's now cover **concatenation** using Pandas and the **Titanic dataset**.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Part 3: Concatenation in Pandas\n",
        "\n",
        "### âœ… What is Concatenation?\n",
        "\n",
        "**Concatenation** means **combining multiple DataFrames either vertically (row-wise)** or **horizontally (column-wise)**.\n",
        "\n",
        "The function used is:\n",
        "\n",
        "```python\n",
        "pd.concat([df1, df2], axis=0 or 1)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### â–¶ï¸ Example 1: **Row-wise Concatenation** (Vertical)\n",
        "\n",
        "Use Case: Combine first 5 rows and next 5 rows of Titanic dataset.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Load Titanic dataset\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Create two subsets\n",
        "df_top = titanic.iloc[:5]\n",
        "df_bottom = titanic.iloc[5:10]\n",
        "\n",
        "# Concatenate row-wise (axis=0)\n",
        "df_vertical = pd.concat([df_top, df_bottom], axis=0)\n",
        "print(df_vertical)\n",
        "```\n",
        "\n",
        "ðŸ“Œ Default is `axis=0` which stacks rows.\n",
        "âœ… The column names must match for proper stacking.\n",
        "\n",
        "---\n",
        "\n",
        "### â–¶ï¸ Example 2: **Column-wise Concatenation** (Horizontal)\n",
        "\n",
        "Use Case: Combine two DataFrames side by side.\n",
        "\n",
        "```python\n",
        "# Select 5 rows of different columns\n",
        "df1 = titanic[['survived', 'sex']].iloc[:5]\n",
        "df2 = titanic[['age', 'fare']].iloc[:5]\n",
        "\n",
        "# Concatenate column-wise (axis=1)\n",
        "df_horizontal = pd.concat([df1, df2], axis=1)\n",
        "print(df_horizontal)\n",
        "```\n",
        "\n",
        "ðŸ“Œ `axis=1` joins DataFrames **side-by-side**, like adding new features.\n",
        "\n",
        "---\n",
        "\n",
        "### â–¶ï¸ Example 3: Concatenation with `ignore_index=True`\n",
        "\n",
        "Use Case: Reset the index after concatenation.\n",
        "\n",
        "```python\n",
        "df_combined = pd.concat([df_top, df_bottom], axis=0, ignore_index=True)\n",
        "print(df_combined)\n",
        "```\n",
        "\n",
        "ðŸ“Œ `ignore_index=True` resets the row indices in the new DataFrame.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§  Summary Table\n",
        "\n",
        "| Type        | Axis                | Description     |\n",
        "| ----------- | ------------------- | --------------- |\n",
        "| Vertical    | `axis=0`            | Appends rows    |\n",
        "| Horizontal  | `axis=1`            | Appends columns |\n",
        "| Reset index | `ignore_index=True` | Renumbers rows  |\n",
        "\n",
        "---\n",
        "\n",
        "Would you like **assignments** or **use-case-based practice questions** using `concat`?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhEffBngzEhP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcosa6-XzEhV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW_o5HhwzEhV"
      },
      "source": [
        "### ðŸ”¹ Part 4: Convert Pandas DataFrame to JSON\n",
        "\n",
        "Pandas makes it easy to convert a DataFrame to JSON format using the `.to_json()` method.\n",
        "\n",
        "---\n",
        "\n",
        "### âœ… Syntax\n",
        "\n",
        "```python\n",
        "df.to_json(path_or_buf=None, orient=None, lines=False)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### â–¶ï¸ Example 1: Convert Titanic DataFrame to JSON string\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Load dataset\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Convert first 5 rows to JSON\n",
        "json_data = titanic.head().to_json()\n",
        "print(json_data)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### â–¶ï¸ Example 2: Save DataFrame as JSON file\n",
        "\n",
        "```python\n",
        "# Save first 5 rows to a JSON file\n",
        "titanic.head().to_json(\"titanic_sample.json\", orient=\"records\", lines=True)\n",
        "```\n",
        "\n",
        "ðŸ“‚ This will create a JSON file with each record on a new line (useful for large datasets).\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ” Different `orient` options in `.to_json()`\n",
        "\n",
        "| Orient      | Description                    | Output Format                       |\n",
        "| ----------- | ------------------------------ | ----------------------------------- |\n",
        "| `'split'`   | Dict with index, columns, data | `{index, columns, data}`            |\n",
        "| `'records'` | List like row-wise dicts       | `[{\"col1\":val1, \"col2\":val2}, ...]` |\n",
        "| `'index'`   | Dict of dicts (index as key)   | `{index: {col:val}}`                |\n",
        "| `'columns'` | Dict of columns                | `{col: {index:val}}`                |\n",
        "| `'values'`  | Just the data as list of lists | `[[...], [...]]`                    |\n",
        "| `'table'`   | JSON Table Schema              | Complex (used in API)               |\n",
        "\n",
        "---\n",
        "\n",
        "### â–¶ï¸ Example 3: Use different `orient`\n",
        "\n",
        "```python\n",
        "# Records orientation\n",
        "json_records = titanic.head().to_json(orient='records')\n",
        "print(json_records)\n",
        "\n",
        "# Split orientation\n",
        "json_split = titanic.head().to_json(orient='split')\n",
        "print(json_split)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§  Summary\n",
        "\n",
        "| Task                   | Code                                    |\n",
        "| ---------------------- | --------------------------------------- |\n",
        "| Convert to JSON string | `df.to_json()`                          |\n",
        "| Save to JSON file      | `df.to_json(\"file.json\")`               |\n",
        "| Pretty JSON            | Use `json.dumps()` with `indent=4`      |\n",
        "| Control format         | Use `orient='records'`, `'split'`, etc. |\n",
        "\n",
        "---\n",
        "\n",
        "Would you like a **sample assignment** or **real-world use case** involving JSON export (e.g., preparing data for a REST API)?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PoZR3VBzEhW"
      },
      "source": [
        "Great! Let's now learn how to **load JSON data into a Pandas DataFrame** â€” the reverse of exporting.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Part 5: Convert JSON to Pandas DataFrame\n",
        "\n",
        "### âœ… Common Use Case\n",
        "\n",
        "You receive a **JSON file** (e.g., from a web API or data export) and need to **analyze it using Pandas**.\n",
        "\n",
        "---\n",
        "\n",
        "### â–¶ï¸ Example 1: Read JSON String\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Sample JSON string (records orientation)\n",
        "json_str = '''\n",
        "[\n",
        "    {\"name\": \"Alice\", \"age\": 25, \"city\": \"New York\"},\n",
        "    {\"name\": \"Bob\", \"age\": 30, \"city\": \"Paris\"},\n",
        "    {\"name\": \"Charlie\", \"age\": 28, \"city\": \"London\"}\n",
        "]\n",
        "'''\n",
        "\n",
        "# Convert JSON string to DataFrame\n",
        "df = pd.read_json(json_str)\n",
        "print(df)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### â–¶ï¸ Example 2: Read JSON File (from disk)\n",
        "\n",
        "```python\n",
        "# Load JSON file into DataFrame\n",
        "df_json = pd.read_json('female_survivors.json', lines=True)\n",
        "print(df_json.head())\n",
        "```\n",
        "\n",
        "ðŸ”¹ `lines=True` is **required** if each row is a separate JSON object (NDJSON format).\n",
        "\n",
        "---\n",
        "\n",
        "### â–¶ï¸ Example 3: Read Nested JSON (Using `json_normalize`)\n",
        "\n",
        "```python\n",
        "from pandas import json_normalize\n",
        "\n",
        "# Sample nested JSON\n",
        "nested_json = {\n",
        "    \"department\": \"IT\",\n",
        "    \"employees\": [\n",
        "        {\"name\": \"Alice\", \"role\": \"Developer\"},\n",
        "        {\"name\": \"Bob\", \"role\": \"Manager\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Normalize nested list\n",
        "df_nested = json_normalize(nested_json, 'employees', meta='department')\n",
        "print(df_nested)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§  Summary Table\n",
        "\n",
        "| Task                          | Function                   | Notes                      |\n",
        "| ----------------------------- | -------------------------- | -------------------------- |\n",
        "| JSON string to DataFrame      | `pd.read_json()`           | From string or file        |\n",
        "| NDJSON to DataFrame           | `pd.read_json(lines=True)` | Each line is a JSON object |\n",
        "| Nested JSON to flat DataFrame | `json_normalize()`         | Extract nested fields      |\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ Assignments: JSON â†’ Pandas\n",
        "\n",
        "### ðŸŽ¯ **Assignment 1: Basic File Import**\n",
        "\n",
        "| Task    | Instructions                                              |\n",
        "| ------- | --------------------------------------------------------- |\n",
        "| File    | Use `children_passengers.json` (from previous assignment) |\n",
        "| Load    | Use `pd.read_json()` with `lines=True`                    |\n",
        "| Display | Show first 5 rows                                         |\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸŽ¯ **Assignment 2: Normalize Nested JSON**\n",
        "\n",
        "| Task        | Instructions                                                                                                                     |\n",
        "| ----------- | -------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| Create JSON | Create a Python dictionary like:<br>`{\"team\": \"Data\", \"members\": [{\"name\": \"A\", \"skill\": \"ML\"}, {\"name\": \"B\", \"skill\": \"SQL\"}]}` |\n",
        "| Convert     | Use `json_normalize()` to extract `members` with `team` as meta                                                                  |\n",
        "| Display     | Print the DataFrame                                                                                                              |\n",
        "\n",
        "---\n",
        "\n",
        "Would you like a **complete Jupyter Notebook** combining all parts (Export + Import JSON using Titanic data)?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P15lH028zEhY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqLi4HrczEhZ"
      },
      "source": [
        "Hereâ€™s a detailed explanation of each of the provided data manipulation techniques in Pandas:\n",
        "\n",
        "### 1. **Pivot the DataFrame**\n",
        "\n",
        "The `pivot_table()` function reshapes the data by summarizing it. It allows you to group by one or more columns and calculate aggregate values (e.g., mean, sum).\n",
        "\n",
        "#### Syntax:\n",
        "```python\n",
        "pivot_table = df.pivot_table(values='Fare', index='Pclass', columns='Sex', aggfunc='mean')\n",
        "```\n",
        "\n",
        "- **`values='Fare'`**: The column we want to aggregate.\n",
        "- **`index='Pclass'`**: Rows will be grouped by the `Pclass` (Passenger Class) column.\n",
        "- **`columns='Sex'`**: The unique values in the `Sex` column will become the columns in the resulting table (i.e., Male, Female).\n",
        "- **`aggfunc='mean'`**: The aggregation function, in this case, calculates the **mean** fare for each combination of `Pclass` and `Sex`.\n",
        "\n",
        "#### Example Output:\n",
        "| Sex    | Female | Male |\n",
        "|--------|--------|------|\n",
        "| Pclass |        |      |\n",
        "| 1      | 100.0  | 80.0 |\n",
        "| 2      | 40.0   | 20.0 |\n",
        "| 3      | 10.0   | 5.0  |\n",
        "\n",
        "This gives the **mean fare** for each combination of **Passenger Class (`Pclass`)** and **Sex**.\n",
        "\n",
        "### 2. **Melt the DataFrame (Unpivot)**\n",
        "\n",
        "The `pd.melt()` function reshapes the DataFrame from wide format to long format (unpivot). It is useful when you want to convert multiple columns into rows.\n",
        "\n",
        "#### Syntax:\n",
        "```python\n",
        "df_melted = pd.melt(df, id_vars=['Pclass'], value_vars=['Age', 'Fare'])\n",
        "```\n",
        "\n",
        "- **`id_vars=['Pclass']`**: These columns are kept intact.\n",
        "- **`value_vars=['Age', 'Fare']`**: These columns will be \"melted\" into one single column of values, creating a long-form DataFrame.\n",
        "\n",
        "#### Example Output:\n",
        "\n",
        "| Pclass | variable | value |\n",
        "|--------|----------|-------|\n",
        "| 1      | Age      | 22    |\n",
        "| 1      | Fare     | 71.0  |\n",
        "| 2      | Age      | 26    |\n",
        "| 2      | Fare     | 12.0  |\n",
        "\n",
        "The new DataFrame is in a **long** format, with each row representing a single observation for `Age` or `Fare`.\n",
        "\n",
        "### 3. **Group by 'Pclass' and Calculate Mean Fare**\n",
        "\n",
        "The `groupby()` function groups the DataFrame by one or more columns and allows you to apply aggregation functions like `mean`, `sum`, etc.\n",
        "\n",
        "#### Syntax:\n",
        "```python\n",
        "df_grouped = df.groupby('Pclass')['Fare'].mean()\n",
        "```\n",
        "\n",
        "- **`groupby('Pclass')`**: Groups the data by `Pclass` (Passenger Class).\n",
        "- **`['Fare']`**: Selects the `Fare` column.\n",
        "- **`.mean()`**: Calculates the **mean fare** for each group.\n",
        "\n",
        "#### Example Output:\n",
        "\n",
        "| Pclass | Fare  |\n",
        "|--------|-------|\n",
        "| 1      | 84.0  |\n",
        "| 2      | 20.0  |\n",
        "| 3      | 13.0  |\n",
        "\n",
        "This shows the **mean fare** paid by passengers in each class (`Pclass`).\n",
        "\n",
        "### 4. **Sort by 'Age' Column**\n",
        "\n",
        "The `sort_values()` function sorts the DataFrame by one or more columns.\n",
        "\n",
        "#### Syntax:\n",
        "```python\n",
        "df_sorted = df.sort_values(by='Age', ascending=False)\n",
        "```\n",
        "\n",
        "- **`by='Age'`**: Specifies the column by which to sort the DataFrame (here, `Age`).\n",
        "- **`ascending=False`**: Sorts the data in **descending** order, so the oldest passengers appear first.\n",
        "\n",
        "#### Example Output:\n",
        "\n",
        "| Name  | Age | Fare |\n",
        "|-------|-----|------|\n",
        "| John  | 80  | 100  |\n",
        "| Jane  | 60  | 50   |\n",
        "| Mark  | 45  | 20   |\n",
        "\n",
        "This sorts the DataFrame by **Age** in descending order, so the oldest passengers are listed first.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary:\n",
        "- **Pivot Table**: Reshapes the DataFrame by summarizing values based on rows and columns.\n",
        "- **Melt**: Unpivots the DataFrame, converting wide format into long format.\n",
        "- **GroupBy**: Groups the data by one or more columns and applies aggregation functions (like `mean`).\n",
        "- **Sort**: Sorts the DataFrame by specified columns in either ascending or descending order.\n",
        "\n",
        "These operations are fundamental for transforming and analyzing data in Pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kjuqzj8nzEha"
      },
      "outputs": [],
      "source": [
        "# Step 7: Data Transformation\n",
        "# Pivot the DataFrame\n",
        "pivot_table = df.pivot_table(values='Fare', index='Pclass', columns='Sex', aggfunc='mean')\n",
        "\n",
        "# Melt the DataFrame (unpivot)\n",
        "df_melted = pd.melt(df, id_vars=['Pclass'], value_vars=['Age', 'Fare'])\n",
        "\n",
        "# Group by 'Pclass' and calculate mean fare\n",
        "df_grouped = df.groupby('Pclass')['Fare'].mean()\n",
        "\n",
        "# Sort by 'Age' column\n",
        "df_sorted = df.sort_values(by='Age', ascending=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DGieE_4zEhb"
      },
      "outputs": [],
      "source": [
        "# plotting\n",
        "d = pd.Series([1, 2, 8, 4, 5, 6])\n",
        "d.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4mXN8I1zEhc"
      },
      "source": [
        "https://pandas.pydata.org/docs/user_guide/visualization.html\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-kUusWpzEhc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
